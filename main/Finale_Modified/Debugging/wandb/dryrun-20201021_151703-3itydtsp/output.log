==============================================Features==============================================
Prediction = depth, Environment = unity
Available features :  ['x-0', 'y-1', 'z-2', 'u-3', 'v-4', 'd-5', 'eot-6', 'og-7', 'rad-8', 'f_sin-9', 'f_cos-10', 'g-11']
Selected features :  [5]
1. input_col =  [3, 4, 5]
2. input_startpos_col =  [3, 4, 5, 5]
3. gt_col =  [5, 5]
4. gt_startpos_col =  [0, 1, 2, 5]
5. gt_xyz_col =  [0, 1, 2, 5]
====================================================================================================
[#]Training : Trajectory Estimation
Mixed:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s]Mixed: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 45.92it/s]
===============================Dataset shape===============================
Mixed : (724,)
===========================================================================
Mixed:   0%|                                                                                                                  | 0/1 [00:00<?, ?it/s]Mixed: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.40it/s]
===============================Dataset shape===============================
Mixed : (1841,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 1754, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1754, 3]), initial position=torch.Size([128, 1, 4])
gt batch [0] : batch=torch.Size([128, 1754, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1755, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 1820, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1820, 3]), initial position=torch.Size([128, 1, 4])
gt batch [1] : batch=torch.Size([128, 1820, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1821, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 1997, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1997, 3]), initial position=torch.Size([128, 1, 4])
gt batch [2] : batch=torch.Size([128, 1997, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1998, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 1814, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1814, 3]), initial position=torch.Size([128, 1, 4])
gt batch [3] : batch=torch.Size([128, 1814, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1815, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 1810, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1810, 3]), initial position=torch.Size([128, 1, 4])
gt batch [4] : batch=torch.Size([128, 1810, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1811, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
{'flag': {'input_size': 2, 'output_size': 1, 'hidden_dim': 32, 'n_layers': 1, 'n_stack': 0, 'recurrent_stacked': [2, 32, 32, 32], 'fc_size': [64, 32, 16, 8, 4, 1]}, 'depth': {'input_size': 3, 'output_size': 1, 'hidden_dim': 32, 'n_layers': 1, 'n_stack': 0, 'recurrent_stacked': [3, 32, 32, 32], 'fc_size': [64, 32, 16, 8, 4, 1]}}
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - Depth #######
BiLSTM(
  (recurrent_blocks): ModuleList(
    (0): LSTM(3, 32, batch_first=True, bidirectional=True)
    (1): LSTM(64, 32, batch_first=True, bidirectional=True)
    (2): LSTM(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): LeakyReLU(negative_slope=0.01)
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.003
===> [Minibatch 1/5].........Train Loss : 52847.629, Val Loss : 54659.637
======> Trajectory Loss : 45004.297, Gravity Loss : 0.003, BelowGroundPenalize Loss : 7711.254, Depth Loss : 0.132
===> [Minibatch 2/5].........Train Loss : 58297.727, Val Loss : 49722.023
======> Trajectory Loss : 49571.133, Gravity Loss : 0.004, BelowGroundPenalize Loss : 8606.199, Depth Loss : 0.120
===> [Minibatch 3/5].........Train Loss : 44371.039, Val Loss : 44928.336
======> Trajectory Loss : 37682.812, Gravity Loss : 0.002, BelowGroundPenalize Loss : 6579.155, Depth Loss : 0.109
===> [Minibatch 4/5].........Train Loss : 45223.535, Val Loss : 39804.359
======> Trajectory Loss : 39153.027, Gravity Loss : 0.003, BelowGroundPenalize Loss : 5971.335, Depth Loss : 0.099
===> [Minibatch 5/5].........Train Loss : 31733.859, Val Loss : 34676.473
======> Trajectory Loss : 27019.791, Gravity Loss : 0.002, BelowGroundPenalize Loss : 4626.235, Depth Loss : 0.088
[#]Finish Epoch : 1/100000.........Train loss : 46494.758, Val loss : 44758.166
[+++]Saving the best model checkpoint : Prev loss 20000000000.000 > Curr loss 44758.166
[+++]Saving the best model checkpoint to :  ../../model_checkpoints/test/LatentDebuggingLSTM//LatentDebuggingLSTM_best.pth
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 2/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.003
===> [Minibatch 1/5].........Train Loss : 31846.492, Val Loss : 28636.453
======> Trajectory Loss : 27350.787, Gravity Loss : 0.002, BelowGroundPenalize Loss : 4419.463, Depth Loss : 0.076
===> [Minibatch 2/5].........Train Loss : 30297.457, Val Loss : 24594.391
======> Trajectory Loss : 26009.211, Gravity Loss : 0.002, BelowGroundPenalize Loss : 4219.179, Depth Loss : 0.069
===> [Minibatch 3/5].........Train Loss : 29101.943, Val Loss : 18787.227
======> Trajectory Loss : 24588.898, Gravity Loss : 0.002, BelowGroundPenalize Loss : 4453.084, Depth Loss : 0.060
===> [Minibatch 4/5].........Train Loss : 16028.585, Val Loss : 12227.247
======> Trajectory Loss : 13495.465, Gravity Loss : 0.001, BelowGroundPenalize Loss : 2487.261, Depth Loss : 0.046
===> [Minibatch 5/5].........Train Loss : 10682.759, Val Loss : 5871.401
======> Trajectory Loss : 9083.313, Gravity Loss : 0.000, BelowGroundPenalize Loss : 1568.802, Depth Loss : 0.031
[#]Finish Epoch : 2/100000.........Train loss : 23591.447, Val loss : 18023.344
[+++]Saving the best model checkpoint : Prev loss 44758.166 > Curr loss 18023.344
[+++]Saving the best model checkpoint to :  ../../model_checkpoints/test/LatentDebuggingLSTM//LatentDebuggingLSTM_best.pth
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 3/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.003
===> [Minibatch 1/5].........Train Loss : 6276.000, Val Loss : 2554.320
======> Trajectory Loss : 5355.597, Gravity Loss : 0.000, BelowGroundPenalize Loss : 904.551, Depth Loss : 0.016
===> [Minibatch 2/5].........Train Loss : 2488.842, Val Loss : 695.113
======> Trajectory Loss : 2165.902, Gravity Loss : 0.000, BelowGroundPenalize Loss : 315.267, Depth Loss : 0.008
===> [Minibatch 3/5].........Train Loss : 623.894, Val Loss : 554.743
======> Trajectory Loss : 529.438, Gravity Loss : 0.000, BelowGroundPenalize Loss : 90.936, Depth Loss : 0.004
===> [Minibatch 4/5].........Train Loss : 509.145, Val Loss : 447.559
======> Trajectory Loss : 426.820, Gravity Loss : 0.000, BelowGroundPenalize Loss : 79.127, Depth Loss : 0.003
===> [Minibatch 5/5].........Train Loss : 401.453, Val Loss : 351.865
======> Trajectory Loss : 340.411, Gravity Loss : 0.000, BelowGroundPenalize Loss : 58.071, Depth Loss : 0.003
[#]Finish Epoch : 3/100000.........Train loss : 2059.867, Val loss : 920.720
[+++]Saving the best model checkpoint : Prev loss 18023.344 > Curr loss 920.720
[+++]Saving the best model checkpoint to :  ../../model_checkpoints/test/LatentDebuggingLSTM//LatentDebuggingLSTM_best.pth
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 4/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.003
===> [Minibatch 1/5].........Train Loss : 301.989, Val Loss : 250.851
======> Trajectory Loss : 259.304, Gravity Loss : 0.000, BelowGroundPenalize Loss : 40.010, Depth Loss : 0.003
===> [Minibatch 2/5].........Train Loss : 229.666, Val Loss : 183.482
======> Trajectory Loss : 194.861, Gravity Loss : 0.000, BelowGroundPenalize Loss : 32.344, Depth Loss : 0.002
