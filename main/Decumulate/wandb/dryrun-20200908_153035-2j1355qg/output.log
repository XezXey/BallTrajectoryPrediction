[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise/val_set
Mixed:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.81it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.77it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
Mixed:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.10it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.07it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 1906, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1906, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 1907, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1907, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 1985, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1985, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 1986, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1986, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 1913, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1913, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 1914, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1914, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 2171, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2171, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 2172, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2172, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 2364, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2364, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 2365, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2365, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 1915, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1915, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 1916, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1916, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 1938, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1938, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 1939, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1939, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 2248, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2248, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 2249, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2249, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 1892, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1892, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 1893, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1893, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 1938, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1938, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 1939, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1939, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 2187, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2187, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 2188, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2188, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 2049, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2049, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 2050, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2050, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 2042, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2042, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 2043, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2043, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 2028, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2028, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 2029, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2029, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 2255, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2255, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 2256, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2256, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.001
===> [Minibatch 1/15].........torch.Size([128, 1836, 1]) torch.Size([128, 1836, 2]) torch.Size([128, 1, 4]) torch.Size([128, 1836, 1])
tensor([[   0],
        [ 215],
        [ 457],
        [ 609],
        [ 760],
        [1022],
        [1406],
        [1537]], device='cuda:0')
[tensor([[57.5693],
        [-0.3595],
        [-0.3573],
        [-0.3520],
        [-0.3521],
        [-0.3509],
        [-0.3512],
        [-0.3503],
        [-0.3490],
        [-0.3508],
        [-0.3522],
        [-0.3512],
        [-0.3509],
        [-0.3504],
        [-0.3517],
        [-0.3516],
        [-0.3503],
        [-0.3488],
        [-0.3507],
        [-0.3483],
        [-0.3497],
        [-0.3510],
        [-0.3491],
        [-0.3509],
        [-0.3483],
        [-0.3489],
        [-0.3478],
        [-0.3485],
        [-0.3485],
        [-0.3490],
        [-0.3483],
        [-0.3479],
        [-0.3468],
        [-0.3485],
        [-0.3475],
        [-0.3480],
        [-0.3463],
        [-0.3472],
        [-0.3475],
        [-0.3477],
        [-0.3472],
        [-0.3460],
        [-0.3474],
        [-0.3462],
        [-0.3478],
        [-0.3469],
        [-0.3470],
        [-0.3476],
        [-0.3466],
        [-0.3478],
        [-0.3462],
        [-0.3457],
        [-0.3448],
        [-0.3453],
        [-0.3454],
        [-0.3459],
        [-0.3450],
        [-0.3457],
        [-0.3451],
        [-0.3459],
        [-0.3444],
        [-0.3459],
        [-0.3459],
        [-0.3458],
        [-0.3454],
        [-0.3455],
        [-0.3446],
        [-0.3463],
        [-0.3465],
        [-0.3482],
        [-0.3476],
        [-0.3507],
        [-0.3507],
        [-0.3539],
        [-0.3526],
        [-0.3523],
        [-0.3531],
        [-0.3520],
        [-0.3521],
        [-0.3516],
        [-0.3541],
        [-0.3530],
        [-0.3523],
        [-0.3539],
        [-0.3532],
        [-0.3516],
        [-0.3507],
        [-0.3515],
        [-0.3508],
        [-0.3516],
        [-0.3517],
        [-0.3510],
        [-0.3511],
        [-0.3516],
        [-0.3497],
        [-0.3500],
        [-0.3506],
        [-0.3516],
        [-0.3495],
        [-0.3499],
        [-0.3483],
        [-0.3497],
        [-0.3497],
        [-0.3493],
        [-0.3484],
        [-0.3503],
        [-0.3488],
        [-0.3494],
        [-0.3499],
        [-0.3481],
        [-0.3496],
        [-0.3496],
        [-0.3489],
        [-0.3506],
        [-0.3487],
        [-0.3500],
        [-0.3503],
        [-0.3505],
        [-0.3535],
        [-0.3530],
        [-0.3544],
        [-0.3547],
        [-0.3536],
        [-0.3549],
        [-0.3541],
        [-0.3525],
        [-0.3523],
        [-0.3534],
        [-0.3548],
        [-0.3528],
        [-0.3531],
        [-0.3542],
        [-0.3522],
        [-0.3530],
        [-0.3521],
        [-0.3522],
        [-0.3534],
        [-0.3516],
        [-0.3521],
        [-0.3518],
        [-0.3506],
        [-0.3530],
        [-0.3510],
        [-0.3516],
        [-0.3516],
        [-0.3522],
        [-0.3518],
        [-0.3522],
        [-0.3517],
        [-0.3547],
        [-0.3555],
        [-0.3540],
        [-0.3547],
        [-0.3555],
        [-0.3551],
        [-0.3544],
        [-0.3542],
        [-0.3533],
        [-0.3543],
        [-0.3522],
        [-0.3540],
        [-0.3533],
        [-0.3537],
        [-0.3535],
        [-0.3536],
        [-0.3540],
        [-0.3531],
        [-0.3540],
        [-0.3534],
        [-0.3536],
        [-0.3540],
        [-0.3560],
        [-0.3570],
        [-0.3545],
        [-0.3554],
        [-0.3560],
        [-0.3539],
        [-0.3540],
        [-0.3548],
        [-0.3539],
        [-0.3545],
        [-0.3535],
        [-0.3546],
        [-0.3544],
        [-0.3560],
        [-0.3566],
        [-0.3589],
        [-0.3546],
        [-0.3559],
        [-0.3548],
        [-0.3559],
        [-0.3537],
        [-0.3534],
        [-0.3541],
        [-0.3542],
        [-0.3569],
        [-0.3567],
        [-0.3572],
        [-0.3551],
        [-0.3562],
        [-0.3547],
        [-0.3560],
        [-0.3569],
        [-0.3550],
        [-0.3555],
        [-0.3539],
        [-0.3587],
        [-0.3589],
        [-0.3568],
        [-0.3581],
        [-0.3589],
        [-0.3551],
        [-0.3580],
        [-0.3550],
        [-0.3578],
        [-0.3598]], device='cuda:0', grad_fn=<CatBackward>), tensor([[62.5334],
        [-0.3609],
        [-0.3631],
        [-0.3625],
        [-0.3630],
        [-0.3644],
        [-0.3649],
        [-0.3661],
        [-0.3666],
        [-0.3660],
        [-0.3669],
        [-0.3669],
        [-0.3667],
        [-0.3684],
        [-0.3676],
        [-0.3675],
        [-0.3678],
        [-0.3680],
        [-0.3679],
        [-0.3677],
        [-0.3672],
        [-0.3677],
        [-0.3680],
        [-0.3679],
        [-0.3674],
        [-0.3671],
        [-0.3674],
        [-0.3679],
        [-0.3675],
        [-0.3679],
        [-0.3674],
        [-0.3676],
        [-0.3675],
        [-0.3672],
        [-0.3671],
        [-0.3676],
        [-0.3678],
        [-0.3669],
        [-0.3665],
        [-0.3667],
        [-0.3657],
        [-0.3664],
        [-0.3659],
        [-0.3660],
        [-0.3652],
        [-0.3640],
        [-0.3648],
        [-0.3652],
        [-0.3652],
        [-0.3632],
        [-0.3654],
        [-0.3636],
        [-0.3616],
        [-0.3630],
        [-0.3643],
        [-0.3607],
        [-0.3630],
        [-0.3614],
        [-0.3611],
        [-0.3621],
        [-0.3607],
        [-0.3620],
        [-0.3588],
        [-0.3612],
        [-0.3607],
        [-0.3578],
        [-0.3604],
        [-0.3583],
        [-0.3603],
        [-0.3594],
        [-0.3583],
        [-0.3594],
        [-0.3608],
        [-0.3622],
        [-0.3625],
        [-0.3641],
        [-0.3662],
        [-0.3653],
        [-0.3664],
        [-0.3660],
        [-0.3667],
        [-0.3674],
        [-0.3669],
        [-0.3657],
        [-0.3677],
        [-0.3677],
        [-0.3680],
        [-0.3674],
        [-0.3669],
        [-0.3669],
        [-0.3671],
        [-0.3670],
        [-0.3678],
        [-0.3669],
        [-0.3663],
        [-0.3657],
        [-0.3645],
        [-0.3662],
        [-0.3664],
        [-0.3648],
        [-0.3620],
        [-0.3646],
        [-0.3646],
        [-0.3623],
        [-0.3644],
        [-0.3622],
        [-0.3626],
        [-0.3630],
        [-0.3618],
        [-0.3615],
        [-0.3628],
        [-0.3601],
        [-0.3607],
        [-0.3612],
        [-0.3596],
        [-0.3612],
        [-0.3617],
        [-0.3602],
        [-0.3606],
        [-0.3602],
        [-0.3607],
        [-0.3604],
        [-0.3609],
        [-0.3610],
        [-0.3589],
        [-0.3623],
        [-0.3632],
        [-0.3644],
        [-0.3636],
        [-0.3649],
        [-0.3656],
        [-0.3655],
        [-0.3656],
        [-0.3649],
        [-0.3654],
        [-0.3655],
        [-0.3645],
        [-0.3640],
        [-0.3616],
        [-0.3636],
        [-0.3636],
        [-0.3641],
        [-0.3617],
        [-0.3629],
        [-0.3619],
        [-0.3599],
        [-0.3610],
        [-0.3627],
        [-0.3598],
        [-0.3630],
        [-0.3614],
        [-0.3611],
        [-0.3600],
        [-0.3601],
        [-0.3594],
        [-0.3615],
        [-0.3595],
        [-0.3602],
        [-0.3613],
        [-0.3613],
        [-0.3647],
        [-0.3625],
        [-0.3637],
        [-0.3607],
        [-0.3597],
        [-0.3632],
        [-0.3624],
        [-0.3633],
        [-0.3622],
        [-0.3616],
        [-0.3622],
        [-0.3627],
        [-0.3606],
        [-0.3633],
        [-0.3607],
        [-0.3624],
        [-0.3606],
        [-0.3584],
        [-0.3609],
        [-0.3579],
        [-0.3595],
        [-0.3611],
        [-0.3603],
        [-0.3631],
        [-0.3593],
        [-0.3627],
        [-0.3598],
        [-0.3594],
        [-0.3619],
        [-0.3611],
        [-0.3600],
        [-0.3626],
        [-0.3619],
        [-0.3595],
        [-0.3612],
        [-0.3602],
        [-0.3616],
        [-0.3586],
        [-0.3609],
        [-0.3604],
        [-0.3589],
        [-0.3594],
        [-0.3612],
        [-0.3618],
        [-0.3609],
        [-0.3598],
        [-0.3588],
        [-0.3589],
        [-0.3582],
        [-0.3609],
        [-0.3599],
        [-0.3610],
        [-0.3624],
        [-0.3611],
        [-0.3577],
        [-0.3610],
        [-0.3599],
        [-0.3597],
        [-0.3587],
        [-0.3601],
        [-0.3596],
        [-0.3586],
        [-0.3606],
        [-0.3602],
        [-0.3598],
        [-0.3586],
        [-0.3584],
        [-0.3596],
        [-0.3597],
        [-0.3589],
        [-0.3596],
        [-0.3618],
        [-0.3614],
        [-0.3611],
        [-0.3579],
        [-0.3605],
        [-0.3591],
        [-0.3600],
        [-0.3573],
        [-0.3586],
        [-0.3591],
        [-0.3565],
        [-0.3581]], device='cuda:0', grad_fn=<CatBackward>), tensor([[57.6250],
        [-0.3541],
        [-0.3471],
        [-0.3437],
        [-0.3418],
        [-0.3411],
        [-0.3393],
        [-0.3386],
        [-0.3386],
        [-0.3395],
        [-0.3389],
        [-0.3396],
        [-0.3399],
        [-0.3395],
        [-0.3399],
        [-0.3386],
        [-0.3400],
        [-0.3400],
        [-0.3397],
        [-0.3406],
        [-0.3410],
        [-0.3402],
        [-0.3404],
        [-0.3407],
        [-0.3407],
        [-0.3411],
        [-0.3404],
        [-0.3416],
        [-0.3413],
        [-0.3415],
        [-0.3402],
        [-0.3423],
        [-0.3420],
        [-0.3431],
        [-0.3423],
        [-0.3430],
        [-0.3427],
        [-0.3426],
        [-0.3428],
        [-0.3433],
        [-0.3435],
        [-0.3423],
        [-0.3441],
        [-0.3448],
        [-0.3441],
        [-0.3429],
        [-0.3430],
        [-0.3446],
        [-0.3443],
        [-0.3446],
        [-0.3449],
        [-0.3449],
        [-0.3459],
        [-0.3445],
        [-0.3445],
        [-0.3449],
        [-0.3463],
        [-0.3460],
        [-0.3464],
        [-0.3459],
        [-0.3465],
        [-0.3463],
        [-0.3465],
        [-0.3465],
        [-0.3474],
        [-0.3479],
        [-0.3471],
        [-0.3474],
        [-0.3467],
        [-0.3480],
        [-0.3490],
        [-0.3487],
        [-0.3473],
        [-0.3513],
        [-0.3490],
        [-0.3484],
        [-0.3514],
        [-0.3493],
        [-0.3499],
        [-0.3505],
        [-0.3504],
        [-0.3521],
        [-0.3509],
        [-0.3506],
        [-0.3516],
        [-0.3504],
        [-0.3510],
        [-0.3516],
        [-0.3509],
        [-0.3516],
        [-0.3519],
        [-0.3529],
        [-0.3518],
        [-0.3518],
        [-0.3519],
        [-0.3523],
        [-0.3523],
        [-0.3526],
        [-0.3522],
        [-0.3531],
        [-0.3519],
        [-0.3535],
        [-0.3527],
        [-0.3538],
        [-0.3540],
        [-0.3536],
        [-0.3538],
        [-0.3535],
        [-0.3527],
        [-0.3533],
        [-0.3529],
        [-0.3538],
        [-0.3547],
        [-0.3548],
        [-0.3544],
        [-0.3544],
        [-0.3543],
        [-0.3540],
        [-0.3556],
        [-0.3556],
        [-0.3534],
        [-0.3563],
        [-0.3555],
        [-0.3548],
        [-0.3558],
        [-0.3548],
        [-0.3563],
        [-0.3555],
        [-0.3549],
        [-0.3549],
        [-0.3552],
        [-0.3545],
        [-0.3559],
        [-0.3561],
        [-0.3563],
        [-0.3571],
        [-0.3549],
        [-0.3575],
        [-0.3550],
        [-0.3569],
        [-0.3559],
        [-0.3554],
        [-0.3564],
        [-0.3555],
        [-0.3564],
        [-0.3571],
        [-0.3578],
        [-0.3580],
        [-0.3567],
        [-0.3584],
        [-0.3580],
        [-0.3578],
        [-0.3563]], device='cuda:0', grad_fn=<CatBackward>), tensor([[63.6264],
        [-0.3580],
        [-0.3580],
        [-0.3594],
        [-0.3601],
        [-0.3606],
        [-0.3616],
        [-0.3622],
        [-0.3636],
        [-0.3628],
        [-0.3641],
        [-0.3633],
        [-0.3656],
        [-0.3645],
        [-0.3647],
        [-0.3649],
        [-0.3637],
        [-0.3649],
        [-0.3648],
        [-0.3649],
        [-0.3646],
        [-0.3649],
        [-0.3658],
        [-0.3658],
        [-0.3660],
        [-0.3649],
        [-0.3655],
        [-0.3656],
        [-0.3651],
        [-0.3648],
        [-0.3657],
        [-0.3665],
        [-0.3653],
        [-0.3655],
        [-0.3657],
        [-0.3662],
        [-0.3656],
        [-0.3659],
        [-0.3662],
        [-0.3659],
        [-0.3654],
        [-0.3661],
        [-0.3661],
        [-0.3659],
        [-0.3662],
        [-0.3665],
        [-0.3665],
        [-0.3665],
        [-0.3663],
        [-0.3673],
        [-0.3664],
        [-0.3660],
        [-0.3672],
        [-0.3676],
        [-0.3660],
        [-0.3664],
        [-0.3664],
        [-0.3667],
        [-0.3670],
        [-0.3679],
        [-0.3672],
        [-0.3668],
        [-0.3670],
        [-0.3660],
        [-0.3674],
        [-0.3668],
        [-0.3667],
        [-0.3675],
        [-0.3678],
        [-0.3670],
        [-0.3673],
        [-0.3672],
        [-0.3652],
        [-0.3669],
        [-0.3672],
        [-0.3661],
        [-0.3661],
        [-0.3654],
        [-0.3658],
        [-0.3652],
        [-0.3664],
        [-0.3654],
        [-0.3653],
        [-0.3645],
        [-0.3647],
        [-0.3649],
        [-0.3650],
        [-0.3641],
        [-0.3658],
        [-0.3647],
        [-0.3631],
        [-0.3638],
        [-0.3625],
        [-0.3620],
        [-0.3622],
        [-0.3646],
        [-0.3649],
        [-0.3631],
        [-0.3632],
        [-0.3646],
        [-0.3613],
        [-0.3625],
        [-0.3632],
        [-0.3628],
        [-0.3634],
        [-0.3592],
        [-0.3634],
        [-0.3615],
        [-0.3627],
        [-0.3633],
        [-0.3605],
        [-0.3626],
        [-0.3600],
        [-0.3624],
        [-0.3612],
        [-0.3632],
        [-0.3607],
        [-0.3613],
        [-0.3623],
        [-0.3621],
        [-0.3626],
        [-0.3618],
        [-0.3610],
        [-0.3607],
        [-0.3599],
        [-0.3612],
        [-0.3621],
        [-0.3600],
        [-0.3586],
        [-0.3601],
        [-0.3621],
        [-0.3601],
        [-0.3593],
        [-0.3590],
        [-0.3611],
        [-0.3603],
        [-0.3614],
        [-0.3597],
        [-0.3606],
        [-0.3589],
        [-0.3582],
        [-0.3610],
        [-0.3600],
        [-0.3620],
        [-0.3592],
        [-0.3607],
        [-0.3598],
        [-0.3601],
        [-0.3603],
        [-0.3618],
        [-0.3607],
        [-0.3615]], device='cuda:0', grad_fn=<CatBackward>), tensor([[56.7670],
        [-0.3604],
        [-0.3593],
        [-0.3541],
        [-0.3564],
        [-0.3551],
        [-0.3544],
        [-0.3552],
        [-0.3544],
        [-0.3556],
        [-0.3526],
        [-0.3528],
        [-0.3544],
        [-0.3544],
        [-0.3532],
        [-0.3534],
        [-0.3542],
        [-0.3531],
        [-0.3529],
        [-0.3530],
        [-0.3537],
        [-0.3512],
        [-0.3518],
        [-0.3521],
        [-0.3514],
        [-0.3513],
        [-0.3532],
        [-0.3526],
        [-0.3509],
        [-0.3513],
        [-0.3509],
        [-0.3504],
        [-0.3502],
        [-0.3506],
        [-0.3497],
        [-0.3498],
        [-0.3496],
        [-0.3506],
        [-0.3503],
        [-0.3514],
        [-0.3482],
        [-0.3477],
        [-0.3489],
        [-0.3485],
        [-0.3486],
        [-0.3505],
        [-0.3490],
        [-0.3485],
        [-0.3484],
        [-0.3473],
        [-0.3480],
        [-0.3477],
        [-0.3491],
        [-0.3481],
        [-0.3486],
        [-0.3480],
        [-0.3477],
        [-0.3478],
        [-0.3466],
        [-0.3472],
        [-0.3471],
        [-0.3465],
        [-0.3463],
        [-0.3469],
        [-0.3470],
        [-0.3472],
        [-0.3488],
        [-0.3462],
        [-0.3461],
        [-0.3458],
        [-0.3470],
        [-0.3460],
        [-0.3458],
        [-0.3450],
        [-0.3453],
        [-0.3461],
        [-0.3464],
        [-0.3459],
        [-0.3455],
        [-0.3460],
        [-0.3469],
        [-0.3463],
        [-0.3472],
        [-0.3466],
        [-0.3483],
        [-0.3491],
        [-0.3523],
        [-0.3530],
        [-0.3532],
        [-0.3527],
        [-0.3549],
        [-0.3540],
        [-0.3535],
        [-0.3545],
        [-0.3551],
        [-0.3540],
        [-0.3527],
        [-0.3509],
        [-0.3544],
        [-0.3534],
        [-0.3534],
        [-0.3527],
        [-0.3543],
        [-0.3503],
        [-0.3513],
        [-0.3528],
        [-0.3529],
        [-0.3523],
        [-0.3518],
        [-0.3509],
        [-0.3523],
        [-0.3517],
        [-0.3510],
        [-0.3521],
        [-0.3515],
        [-0.3526],
        [-0.3499],
        [-0.3502],
        [-0.3504],
        [-0.3508],
        [-0.3500],
        [-0.3504],
        [-0.3491],
        [-0.3500],
        [-0.3489],
        [-0.3503],
        [-0.3484],
        [-0.3496],
        [-0.3497],
        [-0.3485],
        [-0.3490],
        [-0.3498],
        [-0.3499],
        [-0.3501],
        [-0.3486],
        [-0.3485],
        [-0.3489],
        [-0.3502],
        [-0.3489],
        [-0.3501],
        [-0.3506],
        [-0.3516],
        [-0.3537],
        [-0.3538],
        [-0.3535],
        [-0.3540],
        [-0.3559],
        [-0.3534],
        [-0.3564],
        [-0.3534],
        [-0.3531],
        [-0.3555],
        [-0.3533],
        [-0.3532],
        [-0.3515],
        [-0.3520],
        [-0.3525],
        [-0.3532],
        [-0.3531],
        [-0.3522],
        [-0.3541],
        [-0.3530],
        [-0.3528],
        [-0.3519],
        [-0.3520],
        [-0.3516],
        [-0.3520],
        [-0.3523],
        [-0.3521],
        [-0.3512],
        [-0.3511],
        [-0.3508],
        [-0.3518],
        [-0.3494],
        [-0.3526],
        [-0.3537],
        [-0.3520],
        [-0.3532],
        [-0.3507],
        [-0.3535],
        [-0.3554],
        [-0.3543],
        [-0.3561],
        [-0.3558],
        [-0.3559],
        [-0.3544],
        [-0.3536],
        [-0.3554],
        [-0.3529],
        [-0.3541],
        [-0.3543],
        [-0.3536],
        [-0.3542],
        [-0.3537],
        [-0.3533],
        [-0.3538],
        [-0.3520],
        [-0.3537],
        [-0.3552],
        [-0.3534],
        [-0.3541],
        [-0.3526],
        [-0.3539],
        [-0.3523],
        [-0.3534],
        [-0.3524],
        [-0.3568],
        [-0.3549],
        [-0.3567],
        [-0.3552],
        [-0.3555],
        [-0.3561],
        [-0.3562],
        [-0.3562],
        [-0.3543],
        [-0.3559],
        [-0.3535],
        [-0.3546],
        [-0.3537],
        [-0.3569],
        [-0.3549],
        [-0.3529],
        [-0.3579],
        [-0.3539],
        [-0.3578],
        [-0.3550],
        [-0.3569],
        [-0.3573],
        [-0.3533],
        [-0.3557],
        [-0.3589],
        [-0.3539],
        [-0.3570],
        [-0.3557],
        [-0.3552],
        [-0.3559],
        [-0.3549],
        [-0.3544],
        [-0.3570],
        [-0.3566],
        [-0.3550],
        [-0.3554],
        [-0.3577],
        [-0.3565],
        [-0.3563],
        [-0.3562],
        [-0.3556],
        [-0.3562],
        [-0.3590],
        [-0.3547],
        [-0.3572],
        [-0.3575],
        [-0.3563],
        [-0.3555],
        [-0.3568],
        [-0.3570],
        [-0.3597],
        [-0.3571],
        [-0.3548],
        [-0.3573],
        [-0.3598],
        [-0.3608],
        [-0.3602]], device='cuda:0', grad_fn=<CatBackward>), tensor([[70.9052],
        [-0.3576],
        [-0.3575],
        [-0.3594],
        [-0.3599],
        [-0.3609],
        [-0.3619],
        [-0.3619],
        [-0.3619],
        [-0.3613],
        [-0.3628],
        [-0.3636],
        [-0.3624],
        [-0.3639],
        [-0.3659],
        [-0.3644],
        [-0.3645],
        [-0.3646],
        [-0.3656],
        [-0.3651],
        [-0.3630],
        [-0.3649],
        [-0.3640],
        [-0.3680],
        [-0.3658],
        [-0.3643],
        [-0.3634],
        [-0.3657],
        [-0.3669],
        [-0.3663],
        [-0.3645],
        [-0.3662],
        [-0.3653],
        [-0.3654],
        [-0.3667],
        [-0.3663],
        [-0.3673],
        [-0.3649],
        [-0.3670],
        [-0.3668],
        [-0.3672],
        [-0.3673],
        [-0.3658],
        [-0.3673],
        [-0.3665],
        [-0.3669],
        [-0.3668],
        [-0.3669],
        [-0.3675],
        [-0.3678],
        [-0.3678],
        [-0.3668],
        [-0.3669],
        [-0.3670],
        [-0.3667],
        [-0.3685],
        [-0.3679],
        [-0.3676],
        [-0.3684],
        [-0.3676],
        [-0.3664],
        [-0.3663],
        [-0.3679],
        [-0.3671],
        [-0.3672],
        [-0.3674],
        [-0.3665],
        [-0.3667],
        [-0.3668],
        [-0.3668],
        [-0.3659],
        [-0.3663],
        [-0.3678],
        [-0.3676],
        [-0.3654],
        [-0.3661],
        [-0.3654],
        [-0.3656],
        [-0.3659],
        [-0.3644],
        [-0.3650],
        [-0.3642],
        [-0.3646],
        [-0.3646],
        [-0.3636],
        [-0.3638],
        [-0.3639],
        [-0.3624],
        [-0.3612],
        [-0.3629],
        [-0.3626],
        [-0.3614],
        [-0.3622],
        [-0.3596],
        [-0.3620],
        [-0.3624],
        [-0.3591],
        [-0.3603],
        [-0.3603],
        [-0.3602],
        [-0.3597],
        [-0.3595],
        [-0.3595],
        [-0.3577],
        [-0.3600],
        [-0.3588],
        [-0.3571],
        [-0.3566],
        [-0.3583],
        [-0.3596],
        [-0.3578],
        [-0.3591],
        [-0.3579],
        [-0.3569],
        [-0.3581],
        [-0.3570],
        [-0.3596],
        [-0.3553],
        [-0.3564],
        [-0.3567],
        [-0.3589],
        [-0.3569],
        [-0.3586],
        [-0.3592],
        [-0.3598],
        [-0.3607],
        [-0.3632],
        [-0.3631],
        [-0.3626],
        [-0.3633],
        [-0.3635],
        [-0.3638],
        [-0.3645],
        [-0.3657],
        [-0.3654],
        [-0.3657],
        [-0.3656],
        [-0.3669],
        [-0.3666],
        [-0.3658],
        [-0.3663],
        [-0.3672],
        [-0.3670],
        [-0.3674],
        [-0.3664],
        [-0.3675],
        [-0.3663],
        [-0.3664],
        [-0.3675],
        [-0.3676],
        [-0.3677],
        [-0.3674],
        [-0.3678],
        [-0.3677],
        [-0.3678],
        [-0.3678],
        [-0.3682],
        [-0.3676],
        [-0.3675],
        [-0.3671],
        [-0.3678],
        [-0.3672],
        [-0.3663],
        [-0.3664],
        [-0.3669],
        [-0.3663],
        [-0.3667],
        [-0.3661],
        [-0.3666],
        [-0.3646],
        [-0.3664],
        [-0.3648],
        [-0.3650],
        [-0.3647],
        [-0.3637],
        [-0.3637],
        [-0.3649],
        [-0.3608],
        [-0.3629],
        [-0.3621],
        [-0.3629],
        [-0.3630],
        [-0.3610],
        [-0.3605],
        [-0.3616],
        [-0.3611],
        [-0.3600],
        [-0.3594],
        [-0.3634],
        [-0.3591],
        [-0.3615],
        [-0.3572],
        [-0.3589],
        [-0.3584],
        [-0.3593],
        [-0.3578],
        [-0.3587],
        [-0.3562],
        [-0.3580],
        [-0.3581],
        [-0.3573],
        [-0.3567],
        [-0.3560],
        [-0.3595],
        [-0.3579],
        [-0.3582],
        [-0.3586],
        [-0.3591],
        [-0.3597],
        [-0.3622],
        [-0.3617],
        [-0.3637],
        [-0.3650],
        [-0.3661],
        [-0.3658],
        [-0.3669],
        [-0.3664],
        [-0.3662],
        [-0.3655],
        [-0.3668],
        [-0.3688],
        [-0.3657],
        [-0.3662],
        [-0.3673],
        [-0.3663],
        [-0.3670],
        [-0.3662],
        [-0.3666],
        [-0.3657],
        [-0.3661],
        [-0.3658],
        [-0.3655],
        [-0.3647],
        [-0.3628],
        [-0.3654],
        [-0.3658],
        [-0.3655],
        [-0.3639],
        [-0.3620],
        [-0.3603],
        [-0.3632],
        [-0.3625],
        [-0.3592],
        [-0.3624],
        [-0.3610],
        [-0.3637],
        [-0.3613],
        [-0.3628],
        [-0.3635],
        [-0.3578],
        [-0.3587],
        [-0.3590],
        [-0.3585],
        [-0.3580],
        [-0.3590],
        [-0.3580],
        [-0.3602],
        [-0.3573],
        [-0.3580],
        [-0.3590],
        [-0.3579],
        [-0.3577],
        [-0.3574],
        [-0.3575],
        [-0.3585],
        [-0.3612],
        [-0.3624],
        [-0.3632],
        [-0.3645],
        [-0.3643],
        [-0.3643],
        [-0.3641],
        [-0.3652],
        [-0.3641],
        [-0.3648],
        [-0.3646],
        [-0.3647],
        [-0.3642],
        [-0.3636],
        [-0.3636],
        [-0.3648],
        [-0.3618],
        [-0.3629],
        [-0.3606],
        [-0.3620],
        [-0.3603],
        [-0.3584],
        [-0.3605],
        [-0.3609],
        [-0.3620],
        [-0.3612],
        [-0.3606],
        [-0.3579],
        [-0.3600],
        [-0.3598],
        [-0.3594],
        [-0.3577],
        [-0.3588],
        [-0.3588],
        [-0.3577],
        [-0.3580],
        [-0.3598],
        [-0.3596],
        [-0.3580],
        [-0.3633],
        [-0.3624],
        [-0.3627],
        [-0.3618],
        [-0.3623],
        [-0.3641],
        [-0.3633],
        [-0.3614],
        [-0.3634],
        [-0.3592],
        [-0.3625],
        [-0.3607],
        [-0.3614],
        [-0.3600],
        [-0.3602],
        [-0.3609],
        [-0.3611],
        [-0.3591],
        [-0.3583],
        [-0.3603],
        [-0.3593],
        [-0.3565],
        [-0.3570],
        [-0.3594],
        [-0.3610],
        [-0.3561],
        [-0.3606],
        [-0.3627],
        [-0.3635],
        [-0.3614],
        [-0.3627],
        [-0.3615],
        [-0.3591],
        [-0.3597],
        [-0.3607],
        [-0.3603],
        [-0.3615],
        [-0.3580],
        [-0.3572],
        [-0.3586],
        [-0.3580],
        [-0.3592],
        [-0.3599],
        [-0.3577],
        [-0.3605],
        [-0.3590],
        [-0.3608],
        [-0.3611],
        [-0.3604],
        [-0.3608],
        [-0.3590],
        [-0.3595],
        [-0.3592],
        [-0.3590],
        [-0.3598],
        [-0.3586],
        [-0.3589],
        [-0.3616],
        [-0.3584],
        [-0.3582],
        [-0.3608],
        [-0.3594],
        [-0.3583],
        [-0.3593],
        [-0.3584],
        [-0.3598],
        [-0.3599],
        [-0.3591],
        [-0.3592],
        [-0.3597],
        [-0.3573],
        [-0.3606],
        [-0.3597],
        [-0.3598],
        [-0.3577],
        [-0.3593],
        [-0.3577],
        [-0.3600],
        [-0.3576],
        [-0.3582],
        [-0.3555]], device='cuda:0', grad_fn=<CatBackward>), tensor([[65.8517],
        [-0.3540],
        [-0.3524],
        [-0.3493],
        [-0.3489],
        [-0.3480],
        [-0.3468],
        [-0.3465],
        [-0.3478],
        [-0.3472],
        [-0.3483],
        [-0.3474],
        [-0.3471],
        [-0.3490],
        [-0.3474],
        [-0.3480],
        [-0.3475],
        [-0.3483],
        [-0.3466],
        [-0.3475],
        [-0.3488],
        [-0.3481],
        [-0.3480],
        [-0.3485],
        [-0.3485],
        [-0.3501],
        [-0.3480],
        [-0.3491],
        [-0.3491],
        [-0.3490],
        [-0.3494],
        [-0.3494],
        [-0.3492],
        [-0.3496],
        [-0.3491],
        [-0.3497],
        [-0.3502],
        [-0.3498],
        [-0.3505],
        [-0.3502],
        [-0.3505],
        [-0.3499],
        [-0.3501],
        [-0.3510],
        [-0.3513],
        [-0.3496],
        [-0.3517],
        [-0.3520],
        [-0.3507],
        [-0.3514],
        [-0.3523],
        [-0.3511],
        [-0.3510],
        [-0.3526],
        [-0.3517],
        [-0.3528],
        [-0.3535],
        [-0.3527],
        [-0.3520],
        [-0.3525],
        [-0.3529],
        [-0.3522],
        [-0.3537],
        [-0.3540],
        [-0.3524],
        [-0.3540],
        [-0.3545],
        [-0.3528],
        [-0.3533],
        [-0.3532],
        [-0.3526],
        [-0.3540],
        [-0.3537],
        [-0.3537],
        [-0.3551],
        [-0.3558],
        [-0.3555],
        [-0.3535],
        [-0.3534],
        [-0.3545],
        [-0.3550],
        [-0.3551],
        [-0.3540],
        [-0.3552],
        [-0.3548],
        [-0.3565],
        [-0.3547],
        [-0.3548],
        [-0.3547],
        [-0.3573],
        [-0.3553],
        [-0.3547],
        [-0.3558],
        [-0.3535],
        [-0.3570],
        [-0.3550],
        [-0.3566],
        [-0.3566],
        [-0.3552],
        [-0.3567],
        [-0.3551],
        [-0.3550],
        [-0.3557],
        [-0.3562],
        [-0.3570],
        [-0.3565],
        [-0.3556],
        [-0.3562],
        [-0.3569],
        [-0.3579],
        [-0.3554],
        [-0.3598],
        [-0.3561],
        [-0.3551],
        [-0.3564],
        [-0.3563],
        [-0.3556],
        [-0.3571],
        [-0.3590],
        [-0.3556],
        [-0.3576],
        [-0.3576],
        [-0.3564],
        [-0.3570],
        [-0.3586],
        [-0.3555],
        [-0.3559],
        [-0.3563],
        [-0.3551],
        [-0.3555],
        [-0.3587],
        [-0.3583]], device='cuda:0', grad_fn=<CatBackward>)]
Traceback (most recent call last):
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 691, in <module>
    optimizer=optimizer, epoch=epoch, n_epochs=n_epochs, vis_signal=vis_signal, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 382, in train
    output_train_depth, input_trajectory_train_uv = cumsum_decumulate_trajectory(depth=output_train_depth, uv=input_trajectory_train_gt[..., :-1], trajectory_startpos=input_trajectory_train_startpos, lengths=input_trajectory_train_lengths, eot=input_trajectory_train_gt[..., -1].unsqueeze(dim=-1), projection_matrix=projection_matrix, camera_to_world_matrix=camera_to_world_matrix, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 344, in cumsum_decumulate_trajectory
    depth_cumsum = [split_cumsum(reset_idx=reset_idx[i][0], lengths=lengths, reset_depth=reset_depth[i], depth=depth[i]) for i in range(trajectory_startpos.shape[0])]
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 344, in <listcomp>
    depth_cumsum = [split_cumsum(reset_idx=reset_idx[i][0], lengths=lengths, reset_depth=reset_depth[i], depth=depth[i]) for i in range(trajectory_startpos.shape[0])]
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 313, in split_cumsum
    depth_chunk_cumsum = [pt.cumsum(depth_chunk[i]) for i in range(len(depth_chunk))]
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 313, in <listcomp>
    depth_chunk_cumsum = [pt.cumsum(depth_chunk[i]) for i in range(len(depth_chunk))]
TypeError: cumsum() received an invalid combination of arguments - got (Tensor), but expected one of:
 * (Tensor input, name dim, torch.dtype dtype, Tensor out)
 * (Tensor input, int dim, torch.dtype dtype, Tensor out)

