[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                                 | 0/1 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.11it/s]
===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18.76it/s]===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18.70it/s]
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 951, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 869, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 941, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 941, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 941, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 879, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 879, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 879, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 880, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 894, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 894, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 894, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 853, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 853, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 853, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 854, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 907, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 895, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 895, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 896, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 908, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 908, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 909, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 960, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 960, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 960, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 961, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 785, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 785, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 785, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 786, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 851, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 851, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 851, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 852, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 928, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 856, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 856, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 856, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 857, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 860, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 860, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 860, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 861, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........tensor([ 9.0661e-04,  1.7527e-01,  3.4691e-01,  5.1583e-01,  6.8202e-01,
         8.4548e-01,  1.0062e+00,  1.1642e+00,  1.3195e+00,  1.4721e+00,
         1.6219e+00,  1.7691e+00,  1.9134e+00,  2.0551e+00,  2.1940e+00,
         2.3303e+00,  2.4638e+00,  2.5945e+00,  2.7226e+00,  2.8479e+00,
         2.9705e+00,  3.0903e+00,  3.2075e+00,  3.3219e+00,  3.4336e+00,
         3.5425e+00,  3.6488e+00,  3.7523e+00,  3.8531e+00,  3.9512e+00,
         4.0465e+00,  4.1391e+00,  4.2290e+00,  4.3162e+00,  4.4006e+00,
         4.4823e+00,  4.5613e+00,  4.6376e+00,  4.7111e+00,  4.7819e+00,
         4.8500e+00,  4.9154e+00,  4.9780e+00,  5.0379e+00,  5.0951e+00,
         5.1496e+00,  5.2013e+00,  5.2504e+00,  5.2966e+00,  5.3402e+00,
         5.3811e+00,  5.4192e+00,  5.4546e+00,  5.4872e+00,  5.5172e+00,
         5.5444e+00,  5.5689e+00,  5.5906e+00,  5.6097e+00,  5.6260e+00,
         5.6396e+00,  5.6504e+00,  5.6586e+00,  5.6640e+00,  5.6667e+00,
         5.6667e+00,  5.6639e+00,  5.6584e+00,  5.6502e+00,  5.6393e+00,
         5.6256e+00,  5.6092e+00,  5.5901e+00,  5.5683e+00,  5.5437e+00,
         5.5164e+00,  5.4864e+00,  5.4537e+00,  5.4182e+00,  5.3800e+00,
         5.3391e+00,  5.2955e+00,  5.2491e+00,  5.2001e+00,  5.1482e+00,
         5.0937e+00,  5.0365e+00,  4.9765e+00,  4.9138e+00,  4.8483e+00,
         4.7802e+00,  4.7093e+00,  4.6357e+00,  4.5593e+00,  4.4803e+00,
         4.3985e+00,  4.3140e+00,  4.2267e+00,  4.1368e+00,  4.0441e+00,
         3.9487e+00,  3.8506e+00,  3.7497e+00,  3.6461e+00,  3.5398e+00,
         3.4308e+00,  3.3190e+00,  3.2045e+00,  3.0873e+00,  2.9674e+00,
         2.8447e+00,  2.7193e+00,  2.5912e+00,  2.4604e+00,  2.3268e+00,
         2.1906e+00,  2.0515e+00,  1.9098e+00,  1.7653e+00,  1.6182e+00,
         1.4683e+00,  1.3156e+00,  1.1603e+00,  1.0022e+00,  8.4136e-01,
         6.7783e-01,  5.1157e-01,  3.4258e-01,  1.7088e-01, -3.5592e-03,
         1.1159e-01,  2.2402e-01,  3.3373e-01,  4.4071e-01,  5.4496e-01,
         6.4649e-01,  7.4529e-01,  8.4137e-01,  9.3472e-01,  1.0254e+00,
         1.1133e+00,  1.1984e+00,  1.2809e+00,  1.3606e+00,  1.4376e+00,
         1.5119e+00,  1.5835e+00,  1.6523e+00,  1.7184e+00,  1.7818e+00,
         1.8424e+00,  1.9003e+00,  1.9556e+00,  2.0080e+00,  2.0578e+00,
         2.1048e+00,  2.1491e+00,  2.1907e+00,  2.2295e+00,  2.2657e+00,
         2.2991e+00,  2.3298e+00,  2.3577e+00,  2.3829e+00,  2.4054e+00,
         2.4252e+00,  2.4423e+00,  2.4566e+00,  2.4682e+00,  2.4771e+00,
         2.4832e+00,  2.4867e+00,  2.4874e+00,  2.4854e+00,  2.4806e+00,
         2.4731e+00,  2.4629e+00,  2.4500e+00,  2.4344e+00,  2.4160e+00,
         2.3949e+00,  2.3711e+00,  2.3445e+00,  2.3153e+00,  2.2833e+00,
         2.2485e+00,  2.2111e+00,  2.1709e+00,  2.1280e+00,  2.0824e+00,
         2.0341e+00,  1.9830e+00,  1.9292e+00,  1.8727e+00,  1.8134e+00,
         1.7515e+00,  1.6868e+00,  1.6193e+00,  1.5492e+00,  1.4763e+00,
         1.4007e+00,  1.3224e+00,  1.2414e+00,  1.1576e+00,  1.0711e+00,
         9.8187e-01,  8.8993e-01,  7.9526e-01,  6.9786e-01,  5.9774e-01,
         4.9489e-01,  3.8932e-01,  2.8102e-01,  1.7000e-01,  5.6256e-02,
        -6.0215e-02,  1.7263e-02,  9.2015e-02,  1.6404e-01,  2.3335e-01,
         2.9992e-01,  3.6378e-01,  4.2490e-01,  4.8331e-01,  5.3898e-01,
         5.9194e-01,  6.4216e-01,  6.8967e-01,  7.3444e-01,  7.7650e-01,
         8.1582e-01,  8.5243e-01,  8.8630e-01,  9.1746e-01,  9.4588e-01,
         9.7159e-01,  9.9456e-01,  1.0148e+00,  1.0323e+00,  1.0471e+00,
         1.0592e+00,  1.0686e+00,  1.0752e+00,  1.0791e+00,  1.0803e+00,
         1.0787e+00,  1.0745e+00,  1.0675e+00,  1.0577e+00,  1.0453e+00,
         1.0301e+00,  1.0122e+00,  9.9160e-01,  9.6826e-01,  9.4218e-01,
         9.1339e-01,  8.8186e-01,  8.4762e-01,  8.1064e-01,  7.7095e-01,
         7.2852e-01,  6.8338e-01,  6.3550e-01,  5.8491e-01,  5.3158e-01,
         4.7554e-01,  4.1676e-01,  3.5527e-01,  2.9104e-01,  2.2410e-01,
         1.5442e-01,  8.2027e-02,  6.9046e-03,  5.7506e-02,  1.0538e-01,
         1.5053e-01,  1.9296e-01,  2.3266e-01,  2.6963e-01,  3.0389e-01,
         3.3541e-01,  3.6421e-01,  3.9029e-01,  4.1364e-01,  4.3426e-01,
         4.5217e-01,  4.6734e-01,  4.7979e-01,  4.8952e-01,  4.9652e-01,
         5.0080e-01,  5.0235e-01,  5.0117e-01,  4.9727e-01,  4.9065e-01,
         4.8130e-01,  4.6923e-01,  4.5443e-01,  4.3690e-01,  4.1665e-01,
         3.9368e-01,  3.6798e-01,  3.3956e-01,  3.0841e-01,  2.7453e-01,
         2.3793e-01,  1.9861e-01,  1.5656e-01,  1.1179e-01,  6.4286e-02,
         1.4062e-02,  4.8479e-02,  8.0171e-02,  1.0914e-01,  1.3538e-01,
         1.5890e-01,  1.7969e-01,  1.9776e-01,  2.1310e-01,  2.2571e-01,
         2.3561e-01,  2.4277e-01,  2.4722e-01,  2.4893e-01,  2.4792e-01,
         2.4419e-01,  2.3773e-01,  2.2855e-01,  2.1664e-01,  2.0201e-01,
         1.8465e-01,  1.6457e-01,  1.4176e-01,  1.1623e-01,  8.7969e-02,
         5.6986e-02,  2.3277e-02, -1.3156e-02,  1.2297e-02,  3.5025e-02,
         5.5028e-02,  7.2305e-02,  8.6858e-02,  9.8686e-02,  1.0779e-01,
         1.1417e-01,  1.1782e-01,  1.1875e-01,  1.1695e-01,  1.1243e-01,
         1.0518e-01,  9.5208e-02,  8.2510e-02,  6.7088e-02,  4.8941e-02,
         2.8068e-02,  4.4711e-03,  2.1581e-02,  3.5965e-02,  4.7625e-02,
         5.6559e-02,  6.2769e-02,  6.6253e-02,  6.7013e-02,  6.5047e-02,
         6.0356e-02,  5.2941e-02,  4.2800e-02,  2.9935e-02,  1.4344e-02,
         2.6249e-02,  3.5430e-02,  4.1885e-02,  4.5615e-02,  4.6620e-02,
         4.4900e-02,  4.0455e-02,  3.3285e-02,  2.3390e-02,  1.0770e-02,
         2.0744e-02,  2.7994e-02,  3.2518e-02,  3.4317e-02,  3.3391e-02,
         2.9740e-02,  2.3365e-02,  1.4264e-02,  2.8528e-03,  1.2041e-02,
         1.8505e-02,  2.2243e-02,  2.3256e-02,  2.1545e-02,  1.7108e-02,
         9.9467e-03,  1.9893e-03,  3.9783e-04,  7.9570e-05,  1.5934e-05,
         3.1360e-06,  6.6237e-07,  6.9552e-08, -4.9657e-08, -6.5552e-08,
        -6.9404e-09, -3.7488e-08, -6.9525e-08, -1.0417e-08, -1.7371e-08,
        -3.6743e-08, -4.2952e-08, -7.5982e-08, -4.4569e-09, -2.8299e-08,
        -5.2141e-08, -8.5668e-08, -4.2086e-09, -3.7985e-08, -6.1826e-08,
         9.6992e-09, -1.4143e-08, -3.7985e-08, -6.1826e-08,  9.6992e-09,
        -1.4143e-08, -3.7985e-08, -6.1826e-08,  5.4772e-09, -2.2587e-08,
        -4.6429e-08, -6.6048e-08,  5.4772e-09, -1.8365e-08, -4.6429e-08,
        -6.1578e-08,  1.4170e-08, -7.4372e-09, -3.4508e-08, -6.7042e-08,
         7.2156e-09, -1.6626e-08, -4.6925e-08, -6.6545e-08,  2.7453e-09,
        -2.7554e-08, -4.7174e-08, -7.5237e-08,  5.1009e-10, -2.3332e-08,
        -3.8233e-08, -6.2075e-08,  1.8391e-08, -1.2284e-09, -1.6130e-08,
        -3.9971e-08, -5.4873e-08, -7.8714e-08,  1.7519e-09, -2.2090e-08,
        -3.6991e-08, -6.0833e-08,  1.9633e-08,  1.3673e-08, -1.2284e-09,
        -7.1888e-09, -2.2090e-08, -2.8050e-08, -4.2952e-08, -4.8912e-08,
        -6.3813e-08,  2.5594e-08,  1.0693e-08,  4.7321e-09, -1.9110e-08,
        -4.2952e-08, -8.4178e-08, -1.2653e-08, -3.6494e-08, -6.0336e-08,
        -4.7053e-09, -4.4193e-08, -8.3185e-08, -2.6312e-08, -3.5749e-08,
        -5.9591e-08, -8.3433e-08, -1.1908e-08, -2.2835e-08, -4.6677e-08,
        -7.0519e-08,  1.0068e-09, -2.2835e-08, -5.8350e-08, -7.0767e-08,
         7.5843e-10, -2.3083e-08, -5.7356e-08, -9.1380e-08, -2.9789e-08,
        -6.3317e-08, -1.2284e-09, -2.5070e-08, -4.8912e-08, -7.2754e-08,
        -1.2284e-09, -3.3514e-08, -4.9160e-08, -6.4807e-08, -1.2284e-09,
        -3.2769e-08, -5.6611e-08, -8.0453e-08, -8.9273e-09, -3.9723e-08,
        -5.6611e-08, -8.0453e-08, -8.9273e-09, -3.8978e-08, -5.6859e-08,
        -7.4741e-08,  2.4969e-09, -2.1345e-08, -5.0651e-08, -7.4492e-08,
        -8.1823e-09, -3.7240e-08, -6.1081e-08,  1.5411e-08, -1.3149e-08,
        -3.2272e-08, -6.0833e-08,  1.0693e-08, -1.7620e-08, -4.1462e-08,
        -6.1081e-08,  1.0444e-08, -1.3398e-08, -3.7240e-08, -6.4807e-08,
         2.9936e-09, -2.4574e-08, -4.8415e-08, -7.2257e-08,  2.7452e-09,
        -2.4574e-08, -4.8415e-08, -7.5486e-08, -3.9603e-09, -2.7802e-08,
        -4.8664e-08, -7.5486e-08], device='cuda:0')
Traceback (most recent call last):
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 730, in <module>
    optimizer=optimizer, epoch=epoch, n_epochs=n_epochs, vis_signal=vis_signal, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 416, in train
    train_gravity_loss = GravityLoss(output=output_train_xyz, trajectory_gt=output_trajectory_train_xyz[..., :-1], mask=output_trajectory_train_mask[..., :-1], lengths=output_trajectory_train_lengths)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 165, in GravityLoss
    print(trajectory_gt_yaxis_2nd_finite_difference.shape[i][:, :lengths[i]+1])
TypeError: 'int' object is not subscriptable
