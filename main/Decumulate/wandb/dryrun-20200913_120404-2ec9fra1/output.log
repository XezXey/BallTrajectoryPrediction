[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                | 0/1 [00:00<?, ?it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.77it/s]
===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed:   0%|                                                                                                | 0/2 [00:00<?, ?it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18.33it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18.29it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 929, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 839, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 840, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 840, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 870, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 911, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 911, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 912, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 912, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 851, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 851, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 852, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 852, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 918, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 918, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 919, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 919, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 952, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 960, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 960, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 961, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 961, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 854, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 854, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 855, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 855, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 908, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 894, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 894, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 895, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 820, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 820, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 821, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 821, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 777, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 777, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 778, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 778, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 879, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 879, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 880, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 880, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 941, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 941, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 942, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........tensor([286, 447], device='cuda:0')
tensor([[  0],
        [285],
        [446]], device='cuda:0') 826
tensor([[  0],
        [285],
        [826]], device='cuda:0') 826
tensor([[ 54.0583],
        [ 53.8860],
        [ 53.7139],
        [ 53.5420],
        [ 53.3703],
        [ 53.1985],
        [ 53.0265],
        [ 52.8544],
        [ 52.6823],
        [ 52.5100],
        [ 52.3376],
        [ 52.1652],
        [ 51.9928],
        [ 51.8204],
        [ 51.6478],
        [ 51.4753],
        [ 51.3028],
        [ 51.1302],
        [ 50.9576],
        [ 50.7851],
        [ 50.6125],
        [ 50.4401],
        [ 50.2674],
        [ 50.0947],
        [ 49.9219],
        [ 49.7491],
        [ 49.5764],
        [ 49.4035],
        [ 49.2308],
        [ 49.0579],
        [ 48.8853],
        [ 48.7122],
        [ 48.5393],
        [ 48.3665],
        [ 48.1937],
        [ 48.0209],
        [ 47.8482],
        [ 47.6750],
        [ 47.5021],
        [ 47.3293],
        [ 47.1565],
        [ 46.9839],
        [ 46.8115],
        [ 46.6385],
        [ 46.4656],
        [ 46.2932],
        [ 46.1207],
        [ 45.9484],
        [ 45.7757],
        [ 45.6033],
        [ 45.4302],
        [ 45.2572],
        [ 45.0843],
        [ 44.9104],
        [ 44.7368],
        [ 44.5635],
        [ 44.3900],
        [ 44.2167],
        [ 44.0436],
        [ 43.8701],
        [ 43.6968],
        [ 43.5236],
        [ 43.3501],
        [ 43.1766],
        [ 43.0030],
        [ 42.8291],
        [ 42.6557],
        [ 42.4824],
        [ 42.3083],
        [ 42.1342],
        [ 41.9604],
        [ 41.7865],
        [ 41.6127],
        [ 41.4387],
        [ 41.2643],
        [ 41.0903],
        [ 40.9158],
        [ 40.7414],
        [ 40.5671],
        [ 40.3926],
        [ 40.2180],
        [ 40.0432],
        [ 39.8687],
        [ 39.6940],
        [ 39.5193],
        [ 39.3445],
        [ 39.1701],
        [ 38.9955],
        [ 38.8208],
        [ 38.6463],
        [ 38.4720],
        [ 38.2982],
        [ 38.1242],
        [ 37.9511],
        [ 37.7800],
        [ 37.6094],
        [ 37.4385],
        [ 37.2670],
        [ 37.0951],
        [ 36.9232],
        [ 36.7509],
        [ 36.5789],
        [ 36.4066],
        [ 36.2342],
        [ 36.0618],
        [ 35.8892],
        [ 35.7168],
        [ 35.5445],
        [ 35.3718],
        [ 35.1995],
        [ 35.0272],
        [ 34.8552],
        [ 34.6831],
        [ 34.5108],
        [ 34.3385],
        [ 34.1662],
        [ 33.9943],
        [ 33.8224],
        [ 33.6504],
        [ 33.4782],
        [ 33.3061],
        [ 33.1344],
        [ 32.9625],
        [ 32.7903],
        [ 32.6190],
        [ 32.4473],
        [ 32.2756],
        [ 32.1034],
        [ 31.9312],
        [ 31.7591],
        [ 31.5875],
        [ 31.4153],
        [ 31.2428],
        [ 31.0705],
        [ 30.8982],
        [ 30.7259],
        [ 30.5533],
        [ 30.3804],
        [ 30.2081],
        [ 30.0358],
        [ 29.8632],
        [ 29.6901],
        [ 29.5170],
        [ 29.3444],
        [ 29.1711],
        [ 28.9978],
        [ 28.8244],
        [ 28.6513],
        [ 28.4781],
        [ 28.3050],
        [ 28.1319],
        [ 27.9589],
        [ 27.7861],
        [ 27.6128],
        [ 27.4402],
        [ 27.2683],
        [ 27.0970],
        [ 26.9263],
        [ 26.7556],
        [ 26.5845],
        [ 26.4132],
        [ 26.2416],
        [ 26.0699],
        [ 25.8984],
        [ 25.7268],
        [ 25.5550],
        [ 25.3832],
        [ 25.2119],
        [ 25.0403],
        [ 24.8689],
        [ 24.6972],
        [ 24.5254],
        [ 24.3540],
        [ 24.1826],
        [ 24.0115],
        [ 23.8400],
        [ 23.6686],
        [ 23.4973],
        [ 23.3256],
        [ 23.1544],
        [ 22.9824],
        [ 22.8104],
        [ 22.6385],
        [ 22.4663],
        [ 22.2943],
        [ 22.1228],
        [ 21.9506],
        [ 21.7783],
        [ 21.6060],
        [ 21.4339],
        [ 21.2617],
        [ 21.0890],
        [ 20.9165],
        [ 20.7439],
        [ 20.5721],
        [ 20.4002],
        [ 20.2284],
        [ 20.0567],
        [ 19.8856],
        [ 19.7149],
        [ 19.5442],
        [ 19.3731],
        [ 19.2021],
        [ 19.0309],
        [ 18.8596],
        [ 18.6884],
        [ 18.5171],
        [ 18.3460],
        [ 18.1747],
        [ 18.0033],
        [ 17.8323],
        [ 17.6612],
        [ 17.4900],
        [ 17.3188],
        [ 17.1476],
        [ 16.9762],
        [ 16.8052],
        [ 16.6330],
        [ 16.4614],
        [ 16.2902],
        [ 16.1178],
        [ 15.9461],
        [ 15.7741],
        [ 15.6025],
        [ 15.4314],
        [ 15.2595],
        [ 15.0876],
        [ 14.9167],
        [ 14.7458],
        [ 14.5746],
        [ 14.4038],
        [ 14.2329],
        [ 14.0617],
        [ 13.8907],
        [ 13.7197],
        [ 13.5484],
        [ 13.3775],
        [ 13.2063],
        [ 13.0352],
        [ 12.8639],
        [ 12.6929],
        [ 12.5217],
        [ 12.3504],
        [ 12.1793],
        [ 12.0077],
        [ 11.8364],
        [ 11.6648],
        [ 11.4933],
        [ 11.3225],
        [ 11.1518],
        [ 10.9810],
        [ 10.8098],
        [ 10.6388],
        [ 10.4680],
        [ 10.2968],
        [ 10.1256],
        [  9.9543],
        [  9.7833],
        [  9.6120],
        [  9.4409],
        [  9.2693],
        [  9.0983],
        [  8.9268],
        [  8.7561],
        [  8.5848],
        [  8.4138],
        [  8.2429],
        [  8.0719],
        [  7.9009],
        [  7.7295],
        [  7.5585],
        [  7.3875],
        [  7.2162],
        [  7.0447],
        [  6.8738],
        [  6.7023],
        [  6.5314],
        [  6.3602],
        [  6.1890],
        [  6.0178],
        [  5.8460],
        [  5.6741],
        [  5.5019],
        [  5.3293],
        [  5.1565],
        [  4.9820],
        [ 64.1586],
        [ 63.9823],
        [ 63.8058],
        [ 63.6295],
        [ 63.4530],
        [ 63.2778],
        [ 63.1024],
        [ 62.9268],
        [ 62.7518],
        [ 62.5762],
        [ 62.4008],
        [ 62.2263],
        [ 62.0512],
        [ 61.8762],
        [ 61.7011],
        [ 61.5259],
        [ 61.3512],
        [ 61.1764],
        [ 61.0013],
        [ 60.8261],
        [ 60.6511],
        [ 60.4762],
        [ 60.3010],
        [ 60.1257],
        [ 59.9503],
        [ 59.7753],
        [ 59.5999],
        [ 59.4251],
        [ 59.2500],
        [ 59.0748],
        [ 58.8996],
        [ 58.7244],
        [ 58.5492],
        [ 58.3740],
        [ 58.1989],
        [ 58.0238],
        [ 57.8483],
        [ 57.6730],
        [ 57.4975],
        [ 57.3222],
        [ 57.1469],
        [ 56.9717],
        [ 56.7964],
        [ 56.6210],
        [ 56.4453],
        [ 56.2701],
        [ 56.0949],
        [ 55.9195],
        [ 55.7439],
        [ 55.5687],
        [ 55.3932],
        [ 55.2177],
        [ 55.0423],
        [ 54.8671],
        [ 54.6916],
        [ 54.5161],
        [ 54.3409],
        [ 54.1655],
        [ 53.9901],
        [ 53.8149],
        [ 53.6397],
        [ 53.4644],
        [ 53.2893],
        [ 53.1144],
        [ 52.9395],
        [ 52.7644],
        [ 52.5896],
        [ 52.4146],
        [ 52.2396],
        [ 52.0648],
        [ 51.8902],
        [ 51.7154],
        [ 51.5409],
        [ 51.3661],
        [ 51.1913],
        [ 51.0166],
        [ 50.8419],
        [ 50.6675],
        [ 50.4932],
        [ 50.3188],
        [ 50.1443],
        [ 49.9699],
        [ 49.7958],
        [ 49.6217],
        [ 49.4474],
        [ 49.2735],
        [ 49.0995],
        [ 48.9254],
        [ 48.7511],
        [ 48.5770],
        [ 48.4030],
        [ 48.2289],
        [ 48.0553],
        [ 47.8821],
        [ 47.7084],
        [ 47.5350],
        [ 47.3611],
        [ 47.1874],
        [ 47.0137],
        [ 46.8402],
        [ 46.6668],
        [ 46.4937],
        [ 46.3203],
        [ 46.1473],
        [ 45.9742],
        [ 45.8008],
        [ 45.6279],
        [ 45.4547],
        [ 45.2821],
        [ 45.1094],
        [ 44.9367],
        [ 44.7641],
        [ 44.5913],
        [ 44.4186],
        [ 44.2459],
        [ 44.0730],
        [ 43.9007],
        [ 43.7282],
        [ 43.5558],
        [ 43.3830],
        [ 43.2108],
        [ 43.0378],
        [ 42.8656],
        [ 42.6931],
        [ 42.5207],
        [ 42.3489],
        [ 42.1764],
        [ 42.0042],
        [ 41.8321],
        [ 41.6601],
        [ 41.4884],
        [ 41.3164],
        [ 41.1441],
        [ 40.9719],
        [ 40.7999],
        [ 40.6283],
        [ 40.4561],
        [ 40.2837],
        [ 40.1117],
        [ 39.9399],
        [ 39.7680],
        [ 39.5964],
        [ 39.4241],
        [ 39.2520],
        [ 39.0804],
        [ 38.9085],
        [ 38.7367],
        [ 38.5648],
        [ 38.3935],
        [ 38.2219],
        [ 38.0507],
        [ 37.8789],
        [ 37.7070],
        [ 37.5356],
        [ 37.3639],
        [ 37.1922],
        [ 37.0208],
        [ 36.8490],
        [ 36.6773],
        [ 36.5050],
        [ 36.3324],
        [ 36.1581],
        [ 35.9837],
        [ 35.8094],
        [ 35.6350],
        [ 35.4607],
        [ 35.2864],
        [ 35.1120],
        [ 34.9377],
        [ 34.7633],
        [ 34.5890],
        [ 34.4147],
        [ 34.2403],
        [ 34.0660],
        [ 33.8917],
        [ 33.7173],
        [ 33.5430],
        [ 33.3686],
        [ 33.1943],
        [ 33.0200],
        [ 32.8456],
        [ 32.6713],
        [ 32.4969],
        [ 32.3226],
        [ 32.1483],
        [ 31.9739],
        [ 31.7996],
        [ 31.6252],
        [ 31.4509],
        [ 31.2766],
        [ 31.1022],
        [ 30.9279],
        [ 30.7535],
        [ 30.5792],
        [ 30.4049],
        [ 30.2305],
        [ 30.0562],
        [ 29.8818],
        [ 29.7075],
        [ 29.5332],
        [ 29.3588],
        [ 29.1845],
        [ 29.0102],
        [ 28.8358],
        [ 28.6615],
        [ 28.4871],
        [ 28.3128],
        [ 28.1385],
        [ 27.9641],
        [ 27.7898],
        [ 27.6154],
        [ 27.4411],
        [ 27.2668],
        [ 27.0924],
        [ 26.9181],
        [ 26.7437],
        [ 26.5694],
        [ 26.3951],
        [ 26.2207],
        [ 26.0464],
        [ 25.8720],
        [ 25.6977],
        [ 25.5234],
        [ 25.3490],
        [ 25.1747],
        [ 25.0003],
        [ 24.8260],
        [ 24.6517],
        [ 24.4773],
        [ 24.3030],
        [ 24.1287],
        [ 23.9543],
        [ 23.7800],
        [ 23.6056],
        [ 23.4313],
        [ 23.2570],
        [ 23.0826],
        [ 22.9083],
        [ 22.7339],
        [ 22.5596],
        [ 22.3853],
        [ 22.2109],
        [ 22.0366],
        [ 21.8622],
        [ 21.6879],
        [ 21.5136],
        [ 21.3392],
        [ 21.1649],
        [ 20.9905],
        [ 20.8162],
        [ 20.6419],
        [ 20.4675],
        [ 20.2932],
        [ 20.1188],
        [ 19.9445],
        [ 19.7702],
        [ 19.5958],
        [ 19.4215],
        [ 19.2472],
        [ 19.0728],
        [ 18.8985],
        [ 18.7241],
        [ 18.5498],
        [ 18.3755],
        [ 18.2011],
        [ 18.0268],
        [ 17.8524],
        [ 17.6781],
        [ 17.5038],
        [ 17.3294],
        [ 17.1551],
        [ 16.9807],
        [ 16.8064],
        [ 16.6321],
        [ 16.4577],
        [ 16.2834],
        [ 16.1090],
        [ 15.9347],
        [ 15.7604],
        [ 15.5860],
        [ 15.4117],
        [ 15.2374],
        [ 15.0630],
        [ 14.8887],
        [ 14.7143],
        [ 14.5400],
        [ 14.3657],
        [ 14.1913],
        [ 14.0170],
        [ 13.8426],
        [ 13.6683],
        [ 13.4940],
        [ 13.3196],
        [ 13.1453],
        [ 12.9710],
        [ 12.7966],
        [ 12.6223],
        [ 12.4479],
        [ 12.2736],
        [ 12.0993],
        [ 11.9249],
        [ 11.7506],
        [ 11.5762],
        [ 11.4019],
        [ 11.2276],
        [ 11.0532],
        [ 10.8789],
        [ 10.7046],
        [ 10.5302],
        [ 10.3559],
        [ 10.1815],
        [ 10.0072],
        [  9.8329],
        [  9.6585],
        [  9.4842],
        [  9.3098],
        [  9.1355],
        [  8.9612],
        [  8.7868],
        [  8.6125],
        [  8.4382],
        [  8.2638],
        [  8.0895],
        [  7.9151],
        [  7.7408],
        [  7.5665],
        [  7.3921],
        [  7.2178],
        [  7.0434],
        [  6.8691],
        [  6.6948],
        [  6.5204],
        [  6.3461],
        [  6.1718],
        [  5.9974],
        [  5.8231],
        [  5.6487],
        [  5.4744],
        [  5.3001],
        [  5.1257],
        [  4.9514],
        [  4.7770],
        [  4.6027],
        [  4.4284],
        [  4.2540],
        [  4.0797],
        [  3.9053],
        [  3.7310],
        [  3.5567],
        [  3.3823],
        [  3.2080],
        [  3.0337],
        [  2.8593],
        [  2.6850],
        [  2.5106],
        [  2.3363],
        [  2.1620],
        [  1.9876],
        [  1.8133],
        [  1.6389],
        [  1.4646],
        [  1.2903],
        [  1.1159],
        [  0.9416],
        [  0.7672],
        [  0.5929],
        [  0.4186],
        [  0.2442],
        [  0.0699],
        [ -0.1044],
        [ -0.2788],
        [ -0.4531],
        [ -0.6275],
        [ -0.8018],
        [ -0.9761],
        [ -1.1505],
        [ -1.3248],
        [ -1.4992],
        [ -1.6735],
        [ -1.8478],
        [ -2.0222],
        [ -2.1965],
        [ -2.3708],
        [ -2.5452],
        [ -2.7195],
        [ -2.8939],
        [ -3.0682],
        [ -3.2425],
        [ -3.4169],
        [ -3.5912],
        [ -3.7656],
        [ -3.9399],
        [ -4.1142],
        [ -4.2886],
        [ -4.4629],
        [ -4.6373],
        [ -4.8116],
        [ -4.9859],
        [ -5.1603],
        [ -5.3346],
        [ -5.5089],
        [ -5.6833],
        [ -5.8576],
        [ -6.0320],
        [ -6.2063],
        [ -6.3806],
        [ -6.5550],
        [ -6.7293],
        [ -6.9037],
        [ -7.0780],
        [ -7.2523],
        [ -7.4267],
        [ -7.6010],
        [ -7.7754],
        [ -7.9497],
        [ -8.1240],
        [ -8.2984],
        [ -8.4727],
        [ -8.6470],
        [ -8.8214],
        [ -8.9957],
        [ -9.1701],
        [ -9.3444],
        [ -9.5187],
        [ -9.6931],
        [ -9.8674],
        [-10.0418],
        [-10.2161],
        [-10.3904],
        [-10.5648],
        [-10.7391],
        [-10.9134],
        [-11.0878],
        [-11.2621],
        [-11.4365],
        [-11.6108],
        [-11.7851],
        [-11.9595],
        [-12.1338],
        [-12.3082],
        [-12.4825],
        [-12.6568],
        [-12.8312],
        [-13.0055],
        [-13.1798],
        [-13.3542],
        [-13.5285],
        [-13.7029],
        [-13.8772],
        [-14.0515],
        [-14.2259],
        [-14.4002],
        [-14.5745],
        [-14.7489],
        [-14.9232],
        [-15.0976],
        [-15.2719],
        [-15.4462],
        [-15.6206],
        [-15.7949],
        [-15.9693],
        [-16.1436],
        [-16.3179],
        [-16.4923],
        [-16.6666],
        [-16.8410],
        [-17.0153],
        [-17.1896],
        [-17.3640],
        [-17.5383],
        [-17.7126],
        [-17.8870],
        [-18.0613],
        [-18.2357],
        [-18.4100],
        [-18.5843],
        [-18.7587],
        [-18.9330],
        [-19.1074],
        [-19.2817],
        [-19.4560],
        [-19.6304],
        [-19.8047],
        [-19.9791],
        [-20.1534],
        [-20.3277],
        [-20.5021],
        [-20.6764],
        [-20.8508],
        [-21.0251],
        [-21.1994],
        [-21.3738],
        [-21.5481],
        [-21.7225],
        [-21.8968],
        [-22.0711],
        [-22.2455],
        [-22.4198],
        [-22.5941],
        [-22.7685],
        [-22.9428],
        [-23.1172],
        [-23.2915],
        [-23.4658],
        [-23.6402],
        [-23.8145],
        [-23.9889],
        [-24.1632],
        [-24.3375],
        [-24.5119],
        [-24.6862],
        [-24.8606],
        [-25.0349],
        [-25.2092],
        [-25.3836],
        [-25.5579],
        [-25.7323],
        [-25.9066],
        [-26.0809],
        [-26.2553],
        [-26.4296],
        [-26.6040],
        [-26.7783],
        [-26.9526],
        [-27.1270],
        [-27.3013],
        [-27.4756],
        [-27.6500],
        [-27.8243],
        [-27.9987],
        [-28.1730],
        [-28.3473],
        [-28.5217],
        [-28.6960],
        [-28.8704],
        [-29.0447],
        [-29.2190],
        [-29.3934],
        [-29.5677],
        [-29.7421],
        [-29.9164]], device='cuda:0', grad_fn=<CatBackward>)
