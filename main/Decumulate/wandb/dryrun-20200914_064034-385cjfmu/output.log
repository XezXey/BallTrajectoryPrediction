[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                                 | 0/1 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.00it/s]
===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.76it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.71it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 895, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 895, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 896, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 869, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 904, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 904, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 904, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 905, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 960, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 960, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 960, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 961, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 907, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 839, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 839, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 840, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 864, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 864, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 864, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 865, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 908, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 908, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 909, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 894, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 894, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 894, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 951, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 826, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 826, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 826, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 827, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 888, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 888, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 888, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 889, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 918, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 918, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 918, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 919, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 928, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 838, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 838, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 838, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........tensor([ 1.2325e-03,  1.2604e-01,  2.4812e-01,  3.6747e-01,  4.8410e-01,
         5.9800e-01,  7.0918e-01,  8.1764e-01,  9.2337e-01,  1.0264e+00,
         1.1267e+00,  1.2242e+00,  1.3190e+00,  1.4111e+00,  1.5005e+00,
         1.5872e+00,  1.6711e+00,  1.7523e+00,  1.8308e+00,  1.9065e+00,
         1.9796e+00,  2.0499e+00,  2.1175e+00,  2.1823e+00,  2.2444e+00,
         2.3038e+00,  2.3605e+00,  2.4145e+00,  2.4657e+00,  2.5142e+00,
         2.5600e+00,  2.6030e+00,  2.6434e+00,  2.6810e+00,  2.7159e+00,
         2.7480e+00,  2.7774e+00,  2.8041e+00,  2.8281e+00,  2.8494e+00,
         2.8679e+00,  2.8837e+00,  2.8968e+00,  2.9071e+00,  2.9148e+00,
         2.9197e+00,  2.9219e+00,  2.9213e+00,  2.9180e+00,  2.9120e+00,
         2.9033e+00,  2.8919e+00,  2.8777e+00,  2.8608e+00,  2.8412e+00,
         2.8188e+00,  2.7938e+00,  2.7660e+00,  2.7355e+00,  2.7022e+00,
         2.6662e+00,  2.6276e+00,  2.5861e+00,  2.5420e+00,  2.4951e+00,
         2.4455e+00,  2.3932e+00,  2.3382e+00,  2.2804e+00,  2.2199e+00,
         2.1567e+00,  2.0907e+00,  2.0220e+00,  1.9507e+00,  1.8765e+00,
         1.7997e+00,  1.7201e+00,  1.6378e+00,  1.5528e+00,  1.4651e+00,
         1.3746e+00,  1.2814e+00,  1.1855e+00,  1.0868e+00,  9.8545e-01,
         8.8135e-01,  7.7453e-01,  6.6499e-01,  5.5272e-01,  4.3772e-01,
         3.2000e-01,  1.9956e-01,  7.6385e-02, -4.9510e-02,  3.4093e-02,
         1.1497e-01,  1.9312e-01,  2.6855e-01,  3.4126e-01,  4.1123e-01,
         4.7849e-01,  5.4302e-01,  6.0482e-01,  6.6390e-01,  7.2025e-01,
         7.7388e-01,  8.2478e-01,  8.7296e-01,  9.1842e-01,  9.6114e-01,
         1.0011e+00,  1.0384e+00,  1.0730e+00,  1.1048e+00,  1.1339e+00,
         1.1603e+00,  1.1839e+00,  1.2049e+00,  1.2231e+00,  1.2386e+00,
         1.2513e+00,  1.2613e+00,  1.2686e+00,  1.2732e+00,  1.2751e+00,
         1.2742e+00,  1.2706e+00,  1.2643e+00,  1.2552e+00,  1.2435e+00,
         1.2290e+00,  1.2117e+00,  1.1918e+00,  1.1691e+00,  1.1437e+00,
         1.1156e+00,  1.0848e+00,  1.0512e+00,  1.0149e+00,  9.7587e-01,
         9.3412e-01,  8.8965e-01,  8.4245e-01,  7.9253e-01,  7.3989e-01,
         6.8451e-01,  6.2642e-01,  5.6560e-01,  5.0205e-01,  4.3578e-01,
         3.6678e-01,  2.9506e-01,  2.2061e-01,  1.4344e-01,  6.3544e-02,
        -1.9078e-02,  3.6397e-02,  8.9148e-02,  1.3917e-01,  1.8647e-01,
         2.3105e-01,  2.7290e-01,  3.1202e-01,  3.4842e-01,  3.8210e-01,
         4.1305e-01,  4.4128e-01,  4.6678e-01,  4.8955e-01,  5.0960e-01,
         5.2693e-01,  5.4153e-01,  5.5340e-01,  5.6255e-01,  5.6898e-01,
         5.7268e-01,  5.7365e-01,  5.7190e-01,  5.6743e-01,  5.6023e-01,
         5.5031e-01,  5.3766e-01,  5.2228e-01,  5.0418e-01,  4.8336e-01,
         4.5981e-01,  4.3353e-01,  4.0453e-01,  3.7281e-01,  3.3836e-01,
         3.0118e-01,  2.6128e-01,  2.1866e-01,  1.7331e-01,  1.2523e-01,
         7.4434e-02,  2.0910e-02, -3.5340e-02,  2.9933e-03,  3.8602e-02,
         7.1485e-02,  1.0164e-01,  1.2908e-01,  1.5379e-01,  1.7577e-01,
         1.9503e-01,  2.1156e-01,  2.2537e-01,  2.3645e-01,  2.4481e-01,
         2.5045e-01,  2.5335e-01,  2.5354e-01,  2.5100e-01,  2.4573e-01,
         2.3774e-01,  2.2702e-01,  2.1358e-01,  1.9741e-01,  1.7852e-01,
         1.5691e-01,  1.3257e-01,  1.0550e-01,  7.5707e-02,  4.3191e-02,
         7.9492e-03,  3.2627e-02,  5.4581e-02,  7.3809e-02,  9.0312e-02,
         1.0409e-01,  1.1514e-01,  1.2347e-01,  1.2907e-01,  1.3195e-01,
         1.3211e-01,  1.2953e-01,  1.2424e-01,  1.1622e-01,  1.0547e-01,
         9.1997e-02,  7.5800e-02,  5.6878e-02,  3.5232e-02,  1.0860e-02,
         2.8473e-02,  4.3361e-02,  5.5524e-02,  6.4962e-02,  7.1674e-02,
         7.5662e-02,  7.6925e-02,  7.5463e-02,  7.1276e-02,  6.4364e-02,
         5.4727e-02,  4.2365e-02,  2.7278e-02,  9.4658e-03,  2.2815e-02,
         3.3439e-02,  4.1338e-02,  4.6512e-02,  4.8961e-02,  4.8685e-02,
         4.5684e-02,  3.9959e-02,  3.1508e-02,  2.0332e-02,  6.4309e-03,
         1.7238e-02,  2.5320e-02,  3.0676e-02,  3.3308e-02,  3.3215e-02,
         3.0397e-02,  2.4854e-02,  1.6586e-02,  5.5924e-03,  1.4509e-02,
         2.0701e-02,  2.4168e-02,  2.4910e-02,  2.2926e-02,  1.8218e-02,
         1.0785e-02,  2.1570e-03,  4.3135e-04,  1.7201e-01,  3.4087e-01,
         5.0701e-01,  6.7042e-01,  8.3110e-01,  9.8906e-01,  1.1443e+00,
         1.2968e+00,  1.4466e+00,  1.5936e+00,  1.7380e+00,  1.8796e+00,
         2.0185e+00,  2.1546e+00,  2.2881e+00,  2.4188e+00,  2.5468e+00,
         2.6720e+00,  2.7945e+00,  2.9144e+00,  3.0314e+00,  3.1458e+00,
         3.2574e+00,  3.3663e+00,  3.4725e+00,  3.5760e+00,  3.6767e+00,
         3.7747e+00,  3.8700e+00,  3.9626e+00,  4.0524e+00,  4.1395e+00,
         4.2239e+00,  4.3055e+00,  4.3845e+00,  4.4607e+00,  4.5342e+00,
         4.6049e+00,  4.6730e+00,  4.7383e+00,  4.8009e+00,  4.8607e+00,
         4.9179e+00,  4.9723e+00,  5.0239e+00,  5.0729e+00,  5.1191e+00,
         5.1626e+00,  5.2034e+00,  5.2415e+00,  5.2768e+00,  5.3094e+00,
         5.3393e+00,  5.3665e+00,  5.3909e+00,  5.4126e+00,  5.4316e+00,
         5.4479e+00,  5.4614e+00,  5.4722e+00,  5.4803e+00,  5.4856e+00,
         5.4883e+00,  5.4882e+00,  5.4854e+00,  5.4798e+00,  5.4716e+00,
         5.4606e+00,  5.4468e+00,  5.4304e+00,  5.4112e+00,  5.3893e+00,
         5.3647e+00,  5.3374e+00,  5.3073e+00,  5.2745e+00,  5.2390e+00,
         5.2008e+00,  5.1598e+00,  5.1161e+00,  5.0697e+00,  5.0206e+00,
         4.9687e+00,  4.9141e+00,  4.8568e+00,  4.7967e+00,  4.7340e+00,
         4.6685e+00,  4.6003e+00,  4.5293e+00,  4.4557e+00,  4.3793e+00,
         4.3002e+00,  4.2183e+00,  4.1337e+00,  4.0465e+00,  3.9564e+00,
         3.8637e+00,  3.7682e+00,  3.6700e+00,  3.5691e+00,  3.4655e+00,
         3.3591e+00,  3.2500e+00,  3.1382e+00,  3.0237e+00,  2.9064e+00,
         2.7864e+00,  2.6637e+00,  2.5382e+00,  2.4101e+00,  2.2792e+00,
         2.1456e+00,  2.0092e+00,  1.8702e+00,  1.7284e+00,  1.5839e+00,
         1.4366e+00,  1.2867e+00,  1.1340e+00,  9.7854e-01,  8.2040e-01,
         6.5954e-01,  4.9594e-01,  3.2963e-01,  1.6059e-01, -1.1179e-02,
         1.0224e-01,  2.1293e-01,  3.2090e-01,  4.2615e-01,  5.2867e-01,
         6.2846e-01,  7.2553e-01,  8.1988e-01,  9.1150e-01,  1.0004e+00,
         1.0866e+00,  1.1700e+00,  1.2507e+00,  1.3287e+00,  1.4040e+00,
         1.4765e+00,  1.5464e+00,  1.6134e+00,  1.6778e+00,  1.7395e+00,
         1.7984e+00,  1.8546e+00,  1.9080e+00,  1.9588e+00,  2.0068e+00,
         2.0521e+00,  2.0947e+00,  2.1345e+00,  2.1716e+00,  2.2060e+00,
         2.2377e+00,  2.2666e+00,  2.2929e+00,  2.3164e+00,  2.3371e+00,
         2.3552e+00,  2.3705e+00,  2.3831e+00,  2.3930e+00,  2.4001e+00,
         2.4045e+00,  2.4062e+00,  2.4052e+00,  2.4014e+00,  2.3949e+00,
         2.3857e+00,  2.3738e+00,  2.3592e+00,  2.3418e+00,  2.3217e+00,
         2.2988e+00,  2.2733e+00,  2.2450e+00,  2.2140e+00,  2.1803e+00,
         2.1438e+00,  2.1046e+00,  2.0627e+00,  2.0181e+00,  1.9707e+00,
         1.9207e+00,  1.8678e+00,  1.8123e+00,  1.7541e+00,  1.6931e+00,
         1.6294e+00,  1.5629e+00,  1.4938e+00,  1.4219e+00,  1.3473e+00,
         1.2700e+00,  1.1899e+00,  1.1071e+00,  1.0216e+00,  9.3340e-01,
         8.4244e-01,  7.4876e-01,  6.5236e-01,  5.5323e-01,  4.5137e-01,
         3.4679e-01,  2.3949e-01,  1.2946e-01,  1.6700e-02,  9.1762e-02,
         1.6410e-01,  2.3371e-01,  3.0060e-01,  3.6476e-01,  4.2620e-01,
         4.8491e-01,  5.4090e-01,  5.9416e-01,  6.4470e-01,  6.9251e-01,
         7.3760e-01,  7.7996e-01,  8.1960e-01,  8.5651e-01,  8.9070e-01,
         9.2216e-01,  9.5090e-01,  9.7691e-01,  1.0002e+00,  1.0208e+00,
         1.0386e+00,  1.0537e+00,  1.0661e+00,  1.0758e+00,  1.0827e+00,
         1.0869e+00,  1.0884e+00,  1.0872e+00,  1.0832e+00,  1.0765e+00,
         1.0671e+00,  1.0550e+00,  1.0401e+00,  1.0225e+00,  1.0022e+00,
         9.7916e-01,  9.5339e-01,  9.2491e-01,  8.9369e-01,  8.5976e-01,
         8.2309e-01,  7.8371e-01,  7.4159e-01,  6.9676e-01,  6.4919e-01,
         5.9891e-01,  5.4589e-01,  4.9015e-01,  4.3169e-01,  3.7050e-01,
         3.0659e-01,  2.3995e-01,  1.7059e-01,  9.8504e-02,  2.3691e-02,
        -5.3847e-02, -1.6760e-03,  4.7770e-02,  9.4490e-02,  1.3849e-01,
         1.7976e-01,  2.1830e-01,  2.5412e-01,  2.8722e-01,  3.1759e-01,
         3.4524e-01,  3.7016e-01,  3.9235e-01,  4.1182e-01,  4.2857e-01,
         4.4259e-01,  4.5388e-01,  4.6246e-01,  4.6830e-01,  4.7142e-01,
         4.7182e-01,  4.6949e-01,  4.6443e-01,  4.5665e-01,  4.4615e-01,
         4.3292e-01,  4.1697e-01,  3.9829e-01,  3.7688e-01,  3.5275e-01,
         3.2590e-01,  2.9632e-01,  2.6402e-01,  2.2899e-01,  1.9123e-01,
         1.5075e-01,  1.0755e-01,  6.1620e-02,  1.2965e-02,  4.6362e-02,
         7.7033e-02,  1.0498e-01,  1.3020e-01,  1.5270e-01,  1.7247e-01,
         1.8952e-01,  2.0384e-01,  2.1543e-01,  2.2431e-01,  2.3045e-01,
         2.3387e-01,  2.3457e-01,  2.3254e-01,  2.2779e-01,  2.2031e-01,
         2.1011e-01,  1.9718e-01,  1.8152e-01,  1.6315e-01,  1.4204e-01,
         1.1821e-01,  9.1661e-02,  6.2382e-02,  3.0379e-02, -4.3498e-03,
         1.9995e-02,  4.1615e-02,  6.0509e-02,  7.6679e-02,  9.0124e-02,
         1.0084e-01,  1.0884e-01,  1.1411e-01,  1.1665e-01,  1.1647e-01,
         1.1357e-01,  1.0794e-01,  9.9582e-02,  8.8502e-02,  7.4696e-02,
         5.8166e-02,  3.8911e-02,  1.6931e-02,  3.2989e-02,  4.6323e-02,
         5.6931e-02,  6.4814e-02,  6.9973e-02,  7.2406e-02,  7.2115e-02,
         6.9098e-02,  6.3356e-02,  5.4890e-02,  4.3698e-02,  2.9782e-02,
         1.3140e-02,  2.5728e-02,  3.5592e-02,  4.2730e-02,  4.7143e-02,
         4.8831e-02,  4.7795e-02,  4.4033e-02,  3.7546e-02,  2.8335e-02,
         1.6398e-02,  3.2795e-03,  1.3578e-02,  2.1151e-02,  2.5999e-02,
         2.8122e-02,  2.7520e-02,  2.4194e-02,  1.8142e-02,  9.3650e-03,
         1.8729e-03], device='cuda:0')
Traceback (most recent call last):
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 730, in <module>
    optimizer=optimizer, epoch=epoch, n_epochs=n_epochs, vis_signal=vis_signal, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 416, in train
    train_gravity_loss = GravityLoss(output=output_train_xyz, trajectory_gt=output_trajectory_train_xyz[..., :-1], mask=output_trajectory_train_mask[..., :-1], lengths=output_trajectory_train_lengths)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 165, in GravityLoss
    print(trajectory_gt_yaxis_2nd_finite_difference[:lengths[i], 1])
IndexError: index 1 is out of bounds for dimension 1 with size 1
