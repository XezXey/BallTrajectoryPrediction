[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                                 | 0/1 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.65it/s]
===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.01it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 928, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 839, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 839, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 840, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 951, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 960, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 960, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 960, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 961, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 894, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 894, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 894, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 941, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 941, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 941, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 907, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 826, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 826, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 826, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 827, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 888, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 888, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 888, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 889, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 837, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 837, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 837, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 838, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 911, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 911, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 911, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 912, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 869, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 908, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 908, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 909, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 829, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 829, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 829, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 830, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 918, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 918, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 918, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 919, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........tensor([[[-1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0901e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0900e-02,
          -1.0899e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0901e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0901e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0899e-02, -1.0900e-02, -1.0900e-02, -1.0901e-02,
          -1.0901e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0901e-02, -1.0900e-02,
          -1.0900e-02, -1.0899e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02,  5.3163e-03,  8.6398e-02,  2.3234e-01,
           3.1343e-01,  2.3234e-01,  8.6398e-02,  5.3163e-03, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0899e-02, -1.0900e-02, -1.0900e-02, -1.0901e-02,
          -1.0900e-02, -1.0899e-02, -1.0899e-02, -1.0900e-02, -1.0901e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -8.3417e-05,  5.4000e-02,
           1.5135e-01,  2.0543e-01,  1.5135e-01,  5.4000e-02, -8.3424e-05,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -3.8800e-03,  3.1220e-02,
           9.4400e-02,  1.2950e-01,  9.4400e-02,  3.1220e-02, -3.8800e-03,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -5.9085e-03,  1.9049e-02,  6.3973e-02,
           8.8931e-02,  6.3973e-02,  1.9049e-02, -5.9085e-03, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -7.6812e-03,  8.4132e-03,  3.7383e-02,  5.3477e-02,  3.7383e-02,
           8.4132e-03, -7.6811e-03, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -8.4960e-03,  3.5239e-03,  2.5160e-02,  3.7180e-02,
           2.5160e-02,  3.5239e-03, -8.4960e-03, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0706e-02, -7.9634e-03,
           2.6355e-03,  1.9535e-02,  2.7412e-02,  1.6813e-02, -8.7230e-05,
          -9.1303e-03, -1.0900e-02, -1.0900e-02, -9.5212e-03, -2.6270e-03,
           9.7824e-03,  1.6677e-02,  9.7824e-03, -2.6270e-03, -9.5212e-03,
          -1.0727e-02, -9.3084e-03, -4.7469e-03,  2.4947e-03,  7.6995e-03,
           7.3239e-03,  3.7807e-03,  1.1359e-03,  2.2718e-04,  4.5434e-05,
           9.0823e-06,  1.8358e-06,  4.0531e-07,  8.9407e-08, -1.7881e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08, -6.2500e-01]]],
       device='cuda:0')
