[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                                 | 0/1 [00:00<?, ?it/s]===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.38it/s]
Mixed:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.10it/s]
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 960, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 960, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 960, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 961, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 888, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 888, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 888, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 889, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 904, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 904, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 904, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 905, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 869, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 869, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 837, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 837, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 837, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 838, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 854, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 854, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 854, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 855, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 838, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 838, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 838, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 951, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 834, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 834, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 834, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 835, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 895, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 895, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 896, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 750, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 750, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 750, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 751, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 941, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 941, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 941, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 808, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 808, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 808, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 809, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 928, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........tensor([ 2.9495e-08,  5.6527e-09, -1.8189e-08, -4.2031e-08, -7.0343e-08,
         1.1823e-09, -1.6451e-08, -4.0293e-08, -6.4134e-08,  7.3912e-09,
        -2.5391e-08, -4.9233e-08, -7.3075e-08, -1.1980e-08, -3.5822e-08,
        -5.9664e-08,  4.3727e-10, -2.3405e-08, -4.7246e-08, -6.4880e-08,
         6.6461e-09, -1.7196e-08, -5.3952e-08, -7.1088e-08,  7.1428e-09,
        -2.3653e-08, -4.7495e-08, -7.1337e-08, -6.7650e-09, -3.0607e-08,
        -5.4449e-08, -8.5493e-08, -1.3967e-08, -3.4332e-08, -6.0161e-08,
         1.4842e-08, -3.5364e-09, -2.7378e-08, -4.5260e-08, -7.6552e-08,
        -2.2946e-09, -2.9862e-08, -5.7429e-08, -7.7546e-08, -6.0199e-09,
        -3.3587e-08, -5.7429e-08, -8.4996e-08, -1.7196e-08, -4.1038e-08,
        -6.4880e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4880e-08,
         6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4880e-08,  6.6461e-09,
        -1.7196e-08, -4.1038e-08, -6.4880e-08,  6.6461e-09, -1.7196e-08,
        -4.1038e-08, -6.4880e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08,
        -6.4880e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4880e-08,
         6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4880e-08,  6.6461e-09,
        -1.7196e-08, -4.1038e-08, -6.4880e-08,  6.6461e-09, -1.7196e-08,
        -4.1038e-08, -6.4880e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08,
        -6.4880e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4880e-08,
         6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09,
        -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08,
        -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08,
        -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,
         6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09,
        -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08,
        -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08,
        -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,
         6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09,
        -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08,
        -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08,
        -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,
         6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09,
        -1.7196e-08, -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08,
        -4.1038e-08, -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08,
        -6.4879e-08,  6.6461e-09, -1.7196e-08, -4.1038e-08, -6.4879e-08,
         6.6461e-09, -1.7196e-08, -4.1038e-08,  1.4500e-01,  2.8727e-01,
         4.2682e-01,  5.6364e-01,  6.9774e-01,  8.2912e-01,  9.5776e-01,
         1.0837e+00,  1.2069e+00,  1.3274e+00,  1.4451e+00,  1.5601e+00,
         1.6724e+00,  1.7820e+00,  1.8889e+00,  1.9930e+00,  2.0944e+00,
         2.1930e+00,  2.2890e+00,  2.3822e+00,  2.4727e+00,  2.5605e+00,
         2.6455e+00,  2.7279e+00,  2.8075e+00,  2.8843e+00,  2.9585e+00,
         3.0299e+00,  3.0986e+00,  3.1646e+00,  3.2278e+00,  3.2884e+00,
         3.3462e+00,  3.4012e+00,  3.4536e+00,  3.5032e+00,  3.5501e+00,
         3.5943e+00,  3.6357e+00,  3.6744e+00,  3.7104e+00,  3.7437e+00,
         3.7743e+00,  3.8021e+00,  3.8272e+00,  3.8496e+00,  3.8692e+00,
         3.8861e+00,  3.9003e+00,  3.9118e+00,  3.9206e+00,  3.9266e+00,
         3.9299e+00,  3.9304e+00,  3.9283e+00,  3.9234e+00,  3.9158e+00,
         3.9055e+00,  3.8924e+00,  3.8767e+00,  3.8582e+00,  3.8369e+00,
         3.8130e+00,  3.7863e+00,  3.7569e+00,  3.7248e+00,  3.6899e+00,
         3.6524e+00,  3.6121e+00,  3.5690e+00,  3.5233e+00,  3.4748e+00,
         3.4236e+00,  3.3697e+00,  3.3130e+00,  3.2536e+00,  3.1915e+00,
         3.1267e+00,  3.0592e+00,  2.9889e+00,  2.9159e+00,  2.8402e+00,
         2.7617e+00,  2.6805e+00,  2.5966e+00,  2.5100e+00,  2.4207e+00,
         2.3286e+00,  2.2338e+00,  2.1362e+00,  2.0360e+00,  1.9330e+00,
         1.8273e+00,  1.7189e+00,  1.6077e+00,  1.4939e+00,  1.3773e+00,
         1.2579e+00,  1.1359e+00,  1.0111e+00,  8.8361e-01,  7.5338e-01,
         6.2043e-01,  4.8475e-01,  3.4635e-01,  2.0523e-01,  6.1376e-02,
        -8.5201e-02,  1.1845e-02,  1.0617e-01,  1.9776e-01,  2.8663e-01,
         3.7278e-01,  4.5620e-01,  5.3690e-01,  6.1487e-01,  6.9011e-01,
         7.6263e-01,  8.3243e-01,  8.9950e-01,  9.6385e-01,  1.0255e+00,
         1.0844e+00,  1.1405e+00,  1.1940e+00,  1.2447e+00,  1.2927e+00,
         1.3380e+00,  1.3805e+00,  1.4203e+00,  1.4574e+00,  1.4918e+00,
         1.5234e+00,  1.5524e+00,  1.5786e+00,  1.6020e+00,  1.6228e+00,
         1.6408e+00,  1.6561e+00,  1.6687e+00,  1.6785e+00,  1.6856e+00,
         1.6900e+00,  1.6917e+00,  1.6906e+00,  1.6869e+00,  1.6804e+00,
         1.6711e+00,  1.6592e+00,  1.6445e+00,  1.6271e+00,  1.6070e+00,
         1.5841e+00,  1.5585e+00,  1.5302e+00,  1.4992e+00,  1.4654e+00,
         1.4290e+00,  1.3898e+00,  1.3478e+00,  1.3032e+00,  1.2558e+00,
         1.2057e+00,  1.1529e+00,  1.0973e+00,  1.0390e+00,  9.7803e-01,
         9.1430e-01,  8.4785e-01,  7.7867e-01,  7.0676e-01,  6.3214e-01,
         5.5478e-01,  4.7470e-01,  3.9190e-01,  3.0637e-01,  2.1811e-01,
         1.2714e-01,  3.3432e-02, -6.2998e-02,  1.4525e-03,  6.3178e-02,
         1.2218e-01,  1.7845e-01,  2.3200e-01,  2.8283e-01,  3.3093e-01,
         3.7630e-01,  4.1895e-01,  4.5888e-01,  4.9608e-01,  5.3055e-01,
         5.6230e-01,  5.9133e-01,  6.1763e-01,  6.4120e-01,  6.6205e-01,
         6.8018e-01,  6.9558e-01,  7.0826e-01,  7.1821e-01,  7.2543e-01,
         7.2993e-01,  7.3171e-01,  7.3076e-01,  7.2708e-01,  7.2068e-01,
         7.1156e-01,  6.9971e-01,  6.8513e-01,  6.6783e-01,  6.4781e-01,
         6.2506e-01,  5.9958e-01,  5.7138e-01,  5.4046e-01,  5.0681e-01,
         4.7043e-01,  4.3133e-01,  3.8951e-01,  3.4496e-01,  2.9768e-01,
         2.4768e-01,  1.9496e-01,  1.3951e-01,  8.1333e-02,  2.0433e-02,
        -4.3192e-02, -6.4267e-05,  4.0338e-02,  7.8016e-02,  1.1297e-01,
         1.4520e-01,  1.7470e-01,  2.0148e-01,  2.2553e-01,  2.4686e-01,
         2.6546e-01,  2.8134e-01,  2.9449e-01,  3.0491e-01,  3.1262e-01,
         3.1759e-01,  3.1985e-01,  3.1937e-01,  3.1618e-01,  3.1025e-01,
         3.0161e-01,  2.9023e-01,  2.7614e-01,  2.5931e-01,  2.3977e-01,
         2.1749e-01,  1.9250e-01,  1.6477e-01,  1.3433e-01,  1.0115e-01,
         6.5256e-02,  2.6633e-02, -1.4715e-02,  1.3933e-02,  3.9855e-02,
         6.3052e-02,  8.3524e-02,  1.0127e-01,  1.1629e-01,  1.2859e-01,
         1.3816e-01,  1.4501e-01,  1.4913e-01,  1.5053e-01,  1.4920e-01,
         1.4515e-01,  1.3837e-01,  1.2887e-01,  1.1664e-01,  1.0169e-01,
         8.4010e-02,  6.3607e-02,  4.0479e-02,  1.4626e-02,  3.3202e-02,
         4.9052e-02,  6.2178e-02,  7.2579e-02,  8.0254e-02,  8.5205e-02,
         8.7430e-02,  8.6931e-02,  8.3706e-02,  7.7757e-02,  6.9083e-02,
         5.7683e-02,  4.3559e-02,  2.6709e-02,  7.1349e-03,  2.1630e-02,
         3.3399e-02,  4.2444e-02,  4.8763e-02,  5.2358e-02,  5.3228e-02,
         5.1372e-02,  4.6792e-02,  3.9487e-02,  2.9456e-02,  1.6701e-02,
         3.3402e-03,  1.3796e-02,  2.1526e-02,  2.6532e-02,  2.8813e-02,
         2.8368e-02,  2.5199e-02,  1.9305e-02,  1.0685e-02,  2.1371e-03],
       device='cuda:0')
torch.Size([1, 1, 487])
