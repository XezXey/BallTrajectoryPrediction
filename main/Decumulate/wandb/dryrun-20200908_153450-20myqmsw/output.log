[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise/val_set
Mixed:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.42it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.39it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
Mixed:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.42it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.39it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 1836, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1836, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 1837, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1837, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 2364, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2364, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 2365, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2365, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 2109, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2109, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 2110, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2110, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 1679, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1679, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 1680, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1680, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 2248, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2248, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 2249, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2249, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 1910, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1910, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 1911, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1911, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 2187, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2187, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 2188, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2188, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 1913, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1913, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 1914, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1914, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 1917, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1917, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 1918, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1918, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 2042, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2042, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 2043, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2043, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 2255, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2255, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 2256, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2256, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 1985, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1985, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 1986, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1986, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 2112, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2112, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 2113, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2113, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 2097, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2097, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 2098, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2098, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 1747, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1747, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 1748, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1748, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.001
===> [Minibatch 1/15].........torch.Size([128, 1953, 1]) torch.Size([128, 1953, 2]) torch.Size([128, 1, 4]) torch.Size([128, 1953, 1])
tensor([[  0],
        [221],
        [387]], device='cuda:0')
[tensor([[61.3173],
        [-0.3194],
        [-0.3190],
        [-0.3206],
        [-0.3225],
        [-0.3205],
        [-0.3228],
        [-0.3221],
        [-0.3211],
        [-0.3203],
        [-0.3217],
        [-0.3236],
        [-0.3199],
        [-0.3220],
        [-0.3208],
        [-0.3201],
        [-0.3226],
        [-0.3198],
        [-0.3206],
        [-0.3209],
        [-0.3192],
        [-0.3212],
        [-0.3191],
        [-0.3226],
        [-0.3202],
        [-0.3194],
        [-0.3198],
        [-0.3217],
        [-0.3188],
        [-0.3176],
        [-0.3197],
        [-0.3200],
        [-0.3184],
        [-0.3191],
        [-0.3194],
        [-0.3214],
        [-0.3176],
        [-0.3185],
        [-0.3194],
        [-0.3184],
        [-0.3191],
        [-0.3197],
        [-0.3170],
        [-0.3187],
        [-0.3207],
        [-0.3186],
        [-0.3183],
        [-0.3174],
        [-0.3197],
        [-0.3188],
        [-0.3176],
        [-0.3192],
        [-0.3186],
        [-0.3184],
        [-0.3175],
        [-0.3180],
        [-0.3193],
        [-0.3190],
        [-0.3200],
        [-0.3178],
        [-0.3192],
        [-0.3187],
        [-0.3189],
        [-0.3198],
        [-0.3183],
        [-0.3195],
        [-0.3201],
        [-0.3202],
        [-0.3202],
        [-0.3203],
        [-0.3225],
        [-0.3213],
        [-0.3205],
        [-0.3214],
        [-0.3207],
        [-0.3213],
        [-0.3200],
        [-0.3209],
        [-0.3214],
        [-0.3213],
        [-0.3211],
        [-0.3191],
        [-0.3219],
        [-0.3203],
        [-0.3222],
        [-0.3200],
        [-0.3204],
        [-0.3188],
        [-0.3203],
        [-0.3196],
        [-0.3206],
        [-0.3194],
        [-0.3202],
        [-0.3203],
        [-0.3192],
        [-0.3178],
        [-0.3216],
        [-0.3189],
        [-0.3191],
        [-0.3192],
        [-0.3173],
        [-0.3209],
        [-0.3192],
        [-0.3200],
        [-0.3189],
        [-0.3181],
        [-0.3189],
        [-0.3198],
        [-0.3192],
        [-0.3194],
        [-0.3198],
        [-0.3203],
        [-0.3198],
        [-0.3206],
        [-0.3203],
        [-0.3204],
        [-0.3224],
        [-0.3210],
        [-0.3202],
        [-0.3208],
        [-0.3216],
        [-0.3222],
        [-0.3200],
        [-0.3199],
        [-0.3214],
        [-0.3211],
        [-0.3220],
        [-0.3197],
        [-0.3198],
        [-0.3224],
        [-0.3205],
        [-0.3200],
        [-0.3210],
        [-0.3210],
        [-0.3216],
        [-0.3185],
        [-0.3190],
        [-0.3224],
        [-0.3193],
        [-0.3191],
        [-0.3211],
        [-0.3193],
        [-0.3193],
        [-0.3207],
        [-0.3214],
        [-0.3209],
        [-0.3219],
        [-0.3203],
        [-0.3215],
        [-0.3212],
        [-0.3204],
        [-0.3197],
        [-0.3211],
        [-0.3207],
        [-0.3211],
        [-0.3215],
        [-0.3213],
        [-0.3207],
        [-0.3206],
        [-0.3183],
        [-0.3227],
        [-0.3224],
        [-0.3182],
        [-0.3213],
        [-0.3215],
        [-0.3204],
        [-0.3207],
        [-0.3217],
        [-0.3205],
        [-0.3218],
        [-0.3218],
        [-0.3205],
        [-0.3220],
        [-0.3208],
        [-0.3206],
        [-0.3213],
        [-0.3209],
        [-0.3196],
        [-0.3231],
        [-0.3209],
        [-0.3210],
        [-0.3206],
        [-0.3227],
        [-0.3221],
        [-0.3204],
        [-0.3214],
        [-0.3203],
        [-0.3197],
        [-0.3230],
        [-0.3209],
        [-0.3215],
        [-0.3221],
        [-0.3204],
        [-0.3218],
        [-0.3206],
        [-0.3222],
        [-0.3211],
        [-0.3215],
        [-0.3228],
        [-0.3205],
        [-0.3212],
        [-0.3209],
        [-0.3221],
        [-0.3221],
        [-0.3200],
        [-0.3214],
        [-0.3227],
        [-0.3208],
        [-0.3224],
        [-0.3207],
        [-0.3216],
        [-0.3213],
        [-0.3206],
        [-0.3221],
        [-0.3207],
        [-0.3212],
        [-0.3204],
        [-0.3209],
        [-0.3181],
        [-0.3183],
        [-0.3171],
        [-0.3162]], device='cuda:0', grad_fn=<CatBackward>), tensor([[54.2293],
        [-0.3179],
        [-0.3160],
        [-0.3154],
        [-0.3156],
        [-0.3156],
        [-0.3160],
        [-0.3153],
        [-0.3176],
        [-0.3153],
        [-0.3162],
        [-0.3159],
        [-0.3150],
        [-0.3162],
        [-0.3174],
        [-0.3159],
        [-0.3162],
        [-0.3166],
        [-0.3169],
        [-0.3153],
        [-0.3177],
        [-0.3177],
        [-0.3155],
        [-0.3172],
        [-0.3163],
        [-0.3166],
        [-0.3154],
        [-0.3158],
        [-0.3161],
        [-0.3157],
        [-0.3166],
        [-0.3164],
        [-0.3178],
        [-0.3161],
        [-0.3165],
        [-0.3166],
        [-0.3155],
        [-0.3184],
        [-0.3164],
        [-0.3170],
        [-0.3166],
        [-0.3157],
        [-0.3173],
        [-0.3166],
        [-0.3160],
        [-0.3155],
        [-0.3163],
        [-0.3182],
        [-0.3166],
        [-0.3170],
        [-0.3168],
        [-0.3165],
        [-0.3168],
        [-0.3186],
        [-0.3156],
        [-0.3181],
        [-0.3171],
        [-0.3162],
        [-0.3184],
        [-0.3174],
        [-0.3168],
        [-0.3177],
        [-0.3163],
        [-0.3169],
        [-0.3170],
        [-0.3178],
        [-0.3179],
        [-0.3183],
        [-0.3187],
        [-0.3171],
        [-0.3172],
        [-0.3163],
        [-0.3187],
        [-0.3193],
        [-0.3183],
        [-0.3173],
        [-0.3195],
        [-0.3181],
        [-0.3173],
        [-0.3190],
        [-0.3189],
        [-0.3181],
        [-0.3198],
        [-0.3192],
        [-0.3182],
        [-0.3202],
        [-0.3174],
        [-0.3204],
        [-0.3192],
        [-0.3179],
        [-0.3204],
        [-0.3196],
        [-0.3179],
        [-0.3211],
        [-0.3190],
        [-0.3193],
        [-0.3209],
        [-0.3211],
        [-0.3185],
        [-0.3190],
        [-0.3195],
        [-0.3201],
        [-0.3202],
        [-0.3207],
        [-0.3195],
        [-0.3196],
        [-0.3212],
        [-0.3191],
        [-0.3209],
        [-0.3192],
        [-0.3198],
        [-0.3204],
        [-0.3213],
        [-0.3185],
        [-0.3201],
        [-0.3203],
        [-0.3212],
        [-0.3189],
        [-0.3212],
        [-0.3217],
        [-0.3197],
        [-0.3217],
        [-0.3208],
        [-0.3205],
        [-0.3188],
        [-0.3208],
        [-0.3221],
        [-0.3186],
        [-0.3203],
        [-0.3211],
        [-0.3204],
        [-0.3227],
        [-0.3215],
        [-0.3192],
        [-0.3206],
        [-0.3204],
        [-0.3226],
        [-0.3224],
        [-0.3195],
        [-0.3215],
        [-0.3206],
        [-0.3224],
        [-0.3200],
        [-0.3229],
        [-0.3197],
        [-0.3220],
        [-0.3220],
        [-0.3217],
        [-0.3196],
        [-0.3217],
        [-0.3220],
        [-0.3222],
        [-0.3212],
        [-0.3205],
        [-0.3206],
        [-0.3238],
        [-0.3204],
        [-0.3216],
        [-0.3209],
        [-0.3217],
        [-0.3218],
        [-0.3208],
        [-0.3211],
        [-0.3221],
        [-0.3215],
        [-0.3221],
        [-0.3196]], device='cuda:0', grad_fn=<CatBackward>)]
[tensor([[61.3173],
        [60.9979],
        [60.6789],
        [60.3583],
        [60.0358],
        [59.7153],
        [59.3925],
        [59.0704],
        [58.7493],
        [58.4290],
        [58.1073],
        [57.7836],
        [57.4637],
        [57.1417],
        [56.8209],
        [56.5008],
        [56.1782],
        [55.8584],
        [55.5378],
        [55.2169],
        [54.8977],
        [54.5765],
        [54.2575],
        [53.9349],
        [53.6146],
        [53.2953],
        [52.9755],
        [52.6537],
        [52.3350],
        [52.0173],
        [51.6976],
        [51.3776],
        [51.0592],
        [50.7400],
        [50.4206],
        [50.0992],
        [49.7816],
        [49.4631],
        [49.1437],
        [48.8253],
        [48.5062],
        [48.1865],
        [47.8695],
        [47.5508],
        [47.2301],
        [46.9115],
        [46.5932],
        [46.2758],
        [45.9561],
        [45.6374],
        [45.3198],
        [45.0006],
        [44.6820],
        [44.3636],
        [44.0461],
        [43.7281],
        [43.4089],
        [43.0898],
        [42.7699],
        [42.4521],
        [42.1329],
        [41.8141],
        [41.4952],
        [41.1754],
        [40.8571],
        [40.5376],
        [40.2175],
        [39.8973],
        [39.5771],
        [39.2568],
        [38.9343],
        [38.6131],
        [38.2925],
        [37.9711],
        [37.6504],
        [37.3292],
        [37.0091],
        [36.6882],
        [36.3668],
        [36.0455],
        [35.7244],
        [35.4053],
        [35.0833],
        [34.7630],
        [34.4408],
        [34.1209],
        [33.8005],
        [33.4816],
        [33.1614],
        [32.8417],
        [32.5211],
        [32.2017],
        [31.8815],
        [31.5611],
        [31.2419],
        [30.9241],
        [30.6025],
        [30.2837],
        [29.9646],
        [29.6454],
        [29.3281],
        [29.0071],
        [28.6879],
        [28.3679],
        [28.0491],
        [27.7310],
        [27.4121],
        [27.0923],
        [26.7730],
        [26.4536],
        [26.1338],
        [25.8135],
        [25.4937],
        [25.1731],
        [24.8528],
        [24.5324],
        [24.2100],
        [23.8890],
        [23.5688],
        [23.2479],
        [22.9263],
        [22.6041],
        [22.2841],
        [21.9642],
        [21.6428],
        [21.3217],
        [20.9997],
        [20.6800],
        [20.3602],
        [20.0377],
        [19.7173],
        [19.3972],
        [19.0763],
        [18.7552],
        [18.4336],
        [18.1151],
        [17.7961],
        [17.4737],
        [17.1544],
        [16.8353],
        [16.5142],
        [16.1949],
        [15.8757],
        [15.5550],
        [15.2336],
        [14.9127],
        [14.5908],
        [14.2705],
        [13.9490],
        [13.6278],
        [13.3074],
        [12.9877],
        [12.6666],
        [12.3459],
        [12.0249],
        [11.7033],
        [11.3820],
        [11.0613],
        [10.7407],
        [10.4223],
        [10.0996],
        [ 9.7772],
        [ 9.4590],
        [ 9.1376],
        [ 8.8161],
        [ 8.4957],
        [ 8.1750],
        [ 7.8533],
        [ 7.5328],
        [ 7.2110],
        [ 6.8892],
        [ 6.5687],
        [ 6.2467],
        [ 5.9258],
        [ 5.6052],
        [ 5.2839],
        [ 4.9630],
        [ 4.6433],
        [ 4.3203],
        [ 3.9993],
        [ 3.6784],
        [ 3.3578],
        [ 3.0351],
        [ 2.7130],
        [ 2.3926],
        [ 2.0712],
        [ 1.7510],
        [ 1.4313],
        [ 1.1083],
        [ 0.7874],
        [ 0.4659],
        [ 0.1438],
        [-0.1766],
        [-0.4985],
        [-0.8190],
        [-1.1412],
        [-1.4623],
        [-1.7839],
        [-2.1066],
        [-2.4271],
        [-2.7483],
        [-3.0692],
        [-3.3913],
        [-3.7134],
        [-4.0334],
        [-4.3547],
        [-4.6775],
        [-4.9982],
        [-5.3206],
        [-5.6413],
        [-5.9628],
        [-6.2842],
        [-6.6048],
        [-6.9269],
        [-7.2476],
        [-7.5687],
        [-7.8891],
        [-8.2100],
        [-8.5281],
        [-8.8465],
        [-9.1635],
        [-9.4798]], device='cuda:0', grad_fn=<CumsumBackward>), tensor([[54.2293],
        [53.9114],
        [53.5954],
        [53.2800],
        [52.9645],
        [52.6489],
        [52.3328],
        [52.0175],
        [51.6999],
        [51.3846],
        [51.0684],
        [50.7526],
        [50.4376],
        [50.1214],
        [49.8039],
        [49.4880],
        [49.1718],
        [48.8552],
        [48.5384],
        [48.2231],
        [47.9053],
        [47.5876],
        [47.2721],
        [46.9550],
        [46.6386],
        [46.3220],
        [46.0067],
        [45.6908],
        [45.3747],
        [45.0590],
        [44.7424],
        [44.4261],
        [44.1082],
        [43.7921],
        [43.4756],
        [43.1590],
        [42.8434],
        [42.5250],
        [42.2086],
        [41.8916],
        [41.5750],
        [41.2593],
        [40.9420],
        [40.6254],
        [40.3094],
        [39.9939],
        [39.6777],
        [39.3595],
        [39.0429],
        [38.7259],
        [38.4091],
        [38.0926],
        [37.7757],
        [37.4572],
        [37.1416],
        [36.8235],
        [36.5064],
        [36.1902],
        [35.8718],
        [35.5544],
        [35.2376],
        [34.9198],
        [34.6035],
        [34.2866],
        [33.9695],
        [33.6517],
        [33.3338],
        [33.0155],
        [32.6968],
        [32.3797],
        [32.0625],
        [31.7462],
        [31.4275],
        [31.1082],
        [30.7899],
        [30.4726],
        [30.1531],
        [29.8350],
        [29.5176],
        [29.1987],
        [28.8798],
        [28.5617],
        [28.2419],
        [27.9227],
        [27.6045],
        [27.2843],
        [26.9669],
        [26.6465],
        [26.3273],
        [26.0094],
        [25.6889],
        [25.3694],
        [25.0515],
        [24.7304],
        [24.4114],
        [24.0921],
        [23.7712],
        [23.4501],
        [23.1316],
        [22.8126],
        [22.4930],
        [22.1729],
        [21.8527],
        [21.5320],
        [21.2125],
        [20.8929],
        [20.5716],
        [20.2525],
        [19.9316],
        [19.6124],
        [19.2926],
        [18.9722],
        [18.6509],
        [18.3324],
        [18.0123],
        [17.6919],
        [17.3708],
        [17.0518],
        [16.7307],
        [16.4090],
        [16.0893],
        [15.7676],
        [15.4468],
        [15.1263],
        [14.8075],
        [14.4867],
        [14.1646],
        [13.8460],
        [13.5257],
        [13.2045],
        [12.8842],
        [12.5615],
        [12.2400],
        [11.9208],
        [11.6002],
        [11.2798],
        [10.9572],
        [10.6348],
        [10.3153],
        [ 9.9938],
        [ 9.6733],
        [ 9.3509],
        [ 9.0309],
        [ 8.7081],
        [ 8.3884],
        [ 8.0664],
        [ 7.7444],
        [ 7.4228],
        [ 7.1031],
        [ 6.7815],
        [ 6.4595],
        [ 6.1373],
        [ 5.8160],
        [ 5.4955],
        [ 5.1749],
        [ 4.8511],
        [ 4.5308],
        [ 4.2092],
        [ 3.8883],
        [ 3.5666],
        [ 3.2448],
        [ 2.9240],
        [ 2.6029],
        [ 2.2808],
        [ 1.9593],
        [ 1.6372],
        [ 1.3176]], device='cuda:0', grad_fn=<CumsumBackward>)]
torch.Size([389, 1])
