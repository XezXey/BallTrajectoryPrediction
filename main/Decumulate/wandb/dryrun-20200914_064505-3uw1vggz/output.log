[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                                 | 0/1 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.52it/s]
===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.60it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.55it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 839, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 839, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 840, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 918, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 918, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 918, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 919, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 908, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 908, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 909, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 820, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 820, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 820, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 821, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 941, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 941, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 941, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 951, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 879, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 879, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 879, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 880, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 904, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 904, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 904, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 905, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 907, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 928, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 895, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 895, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 896, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 860, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 860, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 860, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 861, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 839, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 839, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 840, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 911, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 911, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 911, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 912, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 796, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 796, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 796, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 797, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........torch.Size([530])
tensor([-6.4655e-08,  6.8704e-09, -1.6971e-08, -4.0813e-08, -6.4655e-08,
         6.8704e-09, -1.6971e-08, -4.6277e-08, -7.0119e-08,  1.4066e-09,
        -1.4985e-08, -3.8827e-08, -6.2668e-08, -8.3505e-11, -2.3925e-08,
        -4.7767e-08, -6.1427e-08,  1.0099e-08, -1.3743e-08, -4.8761e-08,
        -7.2603e-08, -1.0769e-09, -1.3246e-08, -2.5167e-08, -6.1178e-08,
         1.0347e-08, -1.3495e-08, -3.7336e-08, -7.3844e-08, -1.5233e-08,
        -2.6161e-08, -5.0002e-08, -8.0301e-08, -8.7759e-09, -1.9455e-08,
        -2.9886e-08, -6.0433e-08,  1.7798e-08,  6.6154e-10, -2.3180e-08,
        -4.7022e-08, -7.0864e-08,  7.3671e-09, -1.6475e-08, -4.0317e-08,
        -6.9374e-08, -1.0018e-08, -1.8462e-08, -3.0134e-08, -4.7271e-08,
        -6.2420e-08,  2.1517e-09,  2.6484e-09,  6.6220e-09, -5.0506e-09,
        -1.1259e-08, -2.6409e-08, -7.2106e-08, -1.9703e-08, -4.0813e-08,
        -5.5963e-08, -8.6759e-08, -1.8220e-09, -5.2989e-09, -3.5350e-08,
        -5.6956e-08, -9.2222e-08, -2.2684e-08, -5.5218e-08, -6.4904e-08,
         2.1275e-08, -3.5605e-09, -1.6226e-08, -3.9820e-08, -5.0996e-08,
        -5.6956e-08, -5.6956e-08, -6.3662e-08,  1.7053e-08, -1.9952e-08,
        -4.9754e-08, -8.6262e-08, -8.2792e-09, -9.2726e-09, -4.5532e-08,
        -5.5963e-08, -5.8695e-08, -8.7504e-08,  1.6549e-09, -2.8147e-08,
        -5.7950e-08, -7.1112e-08,  4.3868e-09, -1.3246e-08, -2.7154e-08,
        -5.0996e-08, -6.7635e-08, -1.2004e-08, -3.5846e-08, -5.2734e-08,
        -7.6576e-08,  6.1253e-09, -1.0018e-08, -4.6029e-08, -7.5831e-08,
        -8.5275e-09, -2.5167e-08, -3.7833e-08, -6.5649e-08,  1.6556e-08,
        -2.0703e-09, -3.0879e-08, -5.9688e-08,  2.4000e-09, -1.0763e-08,
        -2.4174e-08, -4.2303e-08, -7.3596e-08, -2.0704e-09, -2.1690e-08,
        -5.3728e-08, -7.8563e-08, -7.0374e-09, -3.5598e-08, -5.5715e-08,
        -7.2354e-08, -8.2860e-10, -2.5664e-08, -5.2983e-08, -7.0119e-08,
         1.4066e-09, -2.5664e-08, -5.5466e-08, -8.2288e-08, -1.6475e-08,
        -3.7336e-08, -5.5715e-08, -8.2288e-08, -1.3495e-08, -3.4605e-08,
        -5.8446e-08, -7.9805e-08, -5.7956e-09, -3.2121e-08, -5.8446e-08,
        -7.7818e-08, -8.5275e-09, -2.8147e-08, -5.6211e-08, -8.0053e-08,
        -6.7891e-09, -3.2369e-08,  1.6740e-01,  3.3208e-01,  4.9404e-01,
         6.5327e-01,  8.0977e-01,  9.6355e-01,  1.1146e+00,  1.2629e+00,
         1.4085e+00,  1.5514e+00,  1.6916e+00,  1.8290e+00,  1.9637e+00,
         2.0957e+00,  2.2249e+00,  2.3515e+00,  2.4753e+00,  2.5963e+00,
         2.7147e+00,  2.8303e+00,  2.9432e+00,  3.0534e+00,  3.1609e+00,
         3.2656e+00,  3.3676e+00,  3.4669e+00,  3.5634e+00,  3.6573e+00,
         3.7484e+00,  3.8368e+00,  3.9224e+00,  4.0053e+00,  4.0855e+00,
         4.1630e+00,  4.2378e+00,  4.3098e+00,  4.3791e+00,  4.4457e+00,
         4.5095e+00,  4.5707e+00,  4.6291e+00,  4.6848e+00,  4.7377e+00,
         4.7879e+00,  4.8354e+00,  4.8802e+00,  4.9223e+00,  4.9616e+00,
         4.9982e+00,  5.0321e+00,  5.0632e+00,  5.0917e+00,  5.1174e+00,
         5.1404e+00,  5.1606e+00,  5.1781e+00,  5.1929e+00,  5.2050e+00,
         5.2144e+00,  5.2210e+00,  5.2249e+00,  5.2261e+00,  5.2245e+00,
         5.2203e+00,  5.2133e+00,  5.2036e+00,  5.1911e+00,  5.1759e+00,
         5.1580e+00,  5.1374e+00,  5.1141e+00,  5.0880e+00,  5.0592e+00,
         5.0277e+00,  4.9934e+00,  4.9565e+00,  4.9168e+00,  4.8744e+00,
         4.8292e+00,  4.7813e+00,  4.7307e+00,  4.6774e+00,  4.6214e+00,
         4.5626e+00,  4.5011e+00,  4.4369e+00,  4.3699e+00,  4.3003e+00,
         4.2279e+00,  4.1528e+00,  4.0749e+00,  3.9943e+00,  3.9110e+00,
         3.8250e+00,  3.7363e+00,  3.6448e+00,  3.5506e+00,  3.4537e+00,
         3.3540e+00,  3.2517e+00,  3.1466e+00,  3.0388e+00,  2.9282e+00,
         2.8149e+00,  2.6989e+00,  2.5802e+00,  2.4588e+00,  2.3346e+00,
         2.2077e+00,  2.0781e+00,  1.9457e+00,  1.8107e+00,  1.6729e+00,
         1.5324e+00,  1.3891e+00,  1.2431e+00,  1.0944e+00,  9.4302e-01,
         7.8887e-01,  6.3200e-01,  4.7241e-01,  3.1009e-01,  1.4504e-01,
        -2.2731e-02,  8.8091e-02,  1.9619e-01,  3.0156e-01,  4.0421e-01,
         5.0413e-01,  6.0133e-01,  6.9580e-01,  7.8755e-01,  8.7657e-01,
         9.6287e-01,  1.0464e+00,  1.1273e+00,  1.2054e+00,  1.2808e+00,
         1.3535e+00,  1.4234e+00,  1.4906e+00,  1.5551e+00,  1.6169e+00,
         1.6760e+00,  1.7323e+00,  1.7859e+00,  1.8368e+00,  1.8849e+00,
         1.9303e+00,  1.9730e+00,  2.0130e+00,  2.0502e+00,  2.0848e+00,
         2.1166e+00,  2.1456e+00,  2.1720e+00,  2.1956e+00,  2.2165e+00,
         2.2347e+00,  2.2501e+00,  2.2628e+00,  2.2728e+00,  2.2801e+00,
         2.2847e+00,  2.2865e+00,  2.2856e+00,  2.2820e+00,  2.2756e+00,
         2.2665e+00,  2.2547e+00,  2.2402e+00,  2.2229e+00,  2.2030e+00,
         2.1803e+00,  2.1548e+00,  2.1267e+00,  2.0958e+00,  2.0622e+00,
         2.0259e+00,  1.9868e+00,  1.9450e+00,  1.9005e+00,  1.8533e+00,
         1.8034e+00,  1.7507e+00,  1.6953e+00,  1.6371e+00,  1.5763e+00,
         1.5127e+00,  1.4464e+00,  1.3774e+00,  1.3056e+00,  1.2312e+00,
         1.1540e+00,  1.0740e+00,  9.9137e-01,  9.0599e-01,  8.1789e-01,
         7.2706e-01,  6.3351e-01,  5.3723e-01,  4.3823e-01,  3.3650e-01,
         2.3205e-01,  1.2487e-01,  1.4968e-02,  8.8176e-02,  1.5866e-01,
         2.2642e-01,  2.9145e-01,  3.5376e-01,  4.1334e-01,  4.7020e-01,
         5.2433e-01,  5.7574e-01,  6.2442e-01,  6.7038e-01,  7.1361e-01,
         7.5412e-01,  7.9191e-01,  8.2696e-01,  8.5930e-01,  8.8890e-01,
         9.1579e-01,  9.3995e-01,  9.6138e-01,  9.8009e-01,  9.9607e-01,
         1.0093e+00,  1.0199e+00,  1.0277e+00,  1.0328e+00,  1.0351e+00,
         1.0347e+00,  1.0317e+00,  1.0258e+00,  1.0173e+00,  1.0060e+00,
         9.9203e-01,  9.7532e-01,  9.5587e-01,  9.3371e-01,  9.0881e-01,
         8.8120e-01,  8.5086e-01,  8.1779e-01,  7.8200e-01,  7.4348e-01,
         7.0224e-01,  6.5827e-01,  6.1158e-01,  5.6216e-01,  5.1002e-01,
         4.5515e-01,  3.9756e-01,  3.3724e-01,  2.7420e-01,  2.0843e-01,
         1.3994e-01,  6.8725e-02, -5.2174e-03,  4.4616e-02,  9.1725e-02,
         1.3611e-01,  1.7777e-01,  2.1670e-01,  2.5291e-01,  2.8639e-01,
         3.1715e-01,  3.4518e-01,  3.7049e-01,  3.9308e-01,  4.1294e-01,
         4.3007e-01,  4.4448e-01,  4.5616e-01,  4.6512e-01,  4.7135e-01,
         4.7486e-01,  4.7565e-01,  4.7370e-01,  4.6904e-01,  4.6165e-01,
         4.5153e-01,  4.3869e-01,  4.2312e-01,  4.0483e-01,  3.8381e-01,
         3.6007e-01,  3.3361e-01,  3.0441e-01,  2.7250e-01,  2.3786e-01,
         2.0049e-01,  1.6040e-01,  1.1758e-01,  7.2040e-02,  2.3774e-02,
        -2.7218e-02,  7.6981e-03,  3.9889e-02,  6.9355e-02,  9.6095e-02,
         1.2011e-01,  1.4140e-01,  1.5997e-01,  1.7581e-01,  1.8892e-01,
         1.9931e-01,  2.0698e-01,  2.1192e-01,  2.1414e-01,  2.1363e-01,
         2.1039e-01,  2.0443e-01,  1.9575e-01,  1.8434e-01,  1.7021e-01,
         1.5335e-01,  1.3376e-01,  1.1145e-01,  8.6418e-02,  5.8659e-02,
         2.8175e-02, -5.0346e-03,  1.8323e-02,  3.8955e-02,  5.6862e-02,
         7.2045e-02,  8.4502e-02,  9.4234e-02,  1.0124e-01,  1.0552e-01,
         1.0708e-01,  1.0591e-01,  1.0202e-01,  9.5403e-02,  8.6060e-02,
         7.3993e-02,  5.9200e-02,  4.1682e-02,  2.1440e-02, -1.5282e-03,
         1.5172e-02,  2.9147e-02,  4.0398e-02,  4.8923e-02,  5.4723e-02,
         5.7798e-02,  5.8149e-02,  5.5774e-02,  5.0674e-02,  4.2849e-02,
         3.2300e-02,  1.9025e-02,  3.8050e-03,  1.5469e-02,  2.4408e-02,
         3.0623e-02,  3.4112e-02,  3.4876e-02,  3.2915e-02,  2.8229e-02,
         2.0818e-02,  1.0683e-02,  1.9042e-02,  2.4677e-02,  2.7586e-02,
         2.7771e-02,  2.5230e-02,  1.9965e-02,  1.1974e-02,  2.3949e-03,
         1.0393e-02,  1.5666e-02,  1.8214e-02,  1.8036e-02,  1.5134e-02],
       device='cuda:0')
tensor([[[ 2.2134e-08, -2.6915e-08, -2.5084e-08,  2.7878e-08,  3.1106e-08,
          -2.0365e-08, -2.8623e-08,  1.4622e-08,  1.9356e-08, -1.8332e-08,
          -1.3085e-08,  2.9119e-08,  1.7726e-08, -3.5856e-08, -2.9787e-08,
           3.1091e-08,  3.9659e-08, -1.3799e-08, -2.9911e-08,  9.7478e-09,
           1.6500e-08, -2.6217e-08, -3.1137e-08,  1.7416e-08,  3.0625e-08,
          -1.0664e-08, -1.9465e-08,  2.4339e-08,  3.3807e-08, -1.1036e-08,
          -2.3004e-08,  1.6422e-08,  2.0148e-08, -3.1044e-08, -5.1875e-08,
          -3.3528e-09,  4.8941e-08,  3.1153e-08, -2.8328e-08, -3.3388e-08,
           1.8844e-08,  3.4133e-08, -8.7700e-09, -3.4164e-08, -2.0023e-09,
           4.2841e-08,  3.9519e-08, -7.2178e-09, -3.8883e-08, -3.5483e-08,
          -2.4245e-08, -8.1491e-09,  1.9278e-08,  2.3314e-08, -4.9826e-09,
          -3.3838e-09,  3.8277e-08,  3.5313e-08, -3.0113e-08, -6.4339e-08,
          -2.0163e-08,  3.6756e-08,  3.8572e-08,  1.6500e-08,  2.0318e-08,
           9.3287e-09, -3.8619e-08, -5.6749e-08, -2.3919e-08,  9.6082e-09,
           2.3361e-08,  3.5452e-08,  3.8200e-08, -2.5301e-09, -5.5941e-08,
          -3.7284e-08,  3.5204e-08,  4.6520e-08, -1.6345e-08, -5.0136e-08,
          -1.8425e-08,  2.8033e-08,  4.8087e-08,  1.7028e-08, -3.5623e-08,
          -2.5891e-08,  3.1991e-08,  3.2457e-08, -2.6838e-08, -5.0462e-08,
          -9.4219e-09,  3.3652e-08,  2.1265e-08, -1.8937e-08, -1.1005e-08,
           3.4117e-08,  2.4571e-08, -3.6073e-08, -4.0994e-08,  1.8533e-08,
           3.5421e-08, -7.8852e-09, -1.6174e-08,  2.4075e-08,  2.1219e-08,
          -3.2193e-08, -3.7563e-08,  1.6500e-08,  3.2565e-08, -1.2309e-08,
          -3.9705e-08, -8.1491e-09,  3.5763e-08,  2.4742e-08, -2.5689e-08,
          -2.6605e-08,  2.2942e-08,  2.6403e-08, -1.8114e-08, -1.6391e-08,
           2.7117e-08,  1.9744e-08, -2.8933e-08, -2.3531e-08,  2.5751e-08,
           2.0148e-08, -3.2115e-08, -2.9849e-08,  2.4230e-08,  3.0563e-08,
          -1.5336e-08, -2.0769e-08,  2.0893e-08,  2.1886e-08, -2.1560e-08,
          -2.0629e-08,  2.4571e-08,  2.1265e-08, -2.7101e-08, -2.3935e-08,
           2.6279e-08,  2.4820e-08, -2.5673e-08, -2.6030e-08,  1.0463e-02,
           6.2606e-02,  1.5575e-01,  2.0551e-01,  1.4979e-01,  5.3069e-02,
          -2.6685e-04, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0899e-02, -1.0900e-02, -1.0900e-02, -1.0901e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0899e-02, -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0901e-02,
          -1.0900e-02, -1.0899e-02, -1.0900e-02, -1.0901e-02, -1.0901e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0901e-02,
          -1.0900e-02, -1.0899e-02, -1.0899e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02, -1.0901e-02,
          -1.0901e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02, -1.0899e-02,
          -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0900e-02,
          -1.0899e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02,  6.6824e-03,  9.4594e-02,
           2.5284e-01,  3.4075e-01,  2.5284e-01,  9.4594e-02,  6.6824e-03,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0901e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0901e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0901e-02, -1.0901e-02, -1.0900e-02, -1.0899e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0899e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
           7.1469e-04,  5.8788e-02,  1.6332e-01,  2.2139e-01,  1.6332e-01,
           5.8788e-02,  7.1470e-04, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0899e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -2.9937e-03,  3.6538e-02,  1.0769e-01,  1.4723e-01,  1.0769e-01,
           3.6538e-02, -2.9937e-03, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -5.3605e-03,  2.2337e-02,
           7.2193e-02,  9.9890e-02,  7.2193e-02,  2.2337e-02, -5.3605e-03,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -7.1943e-03,
           1.1334e-02,  4.4686e-02,  6.3215e-02,  4.4686e-02,  1.1334e-02,
          -7.1943e-03, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -8.2504e-03,  4.9974e-03,  2.8843e-02,
           4.2091e-02,  2.8843e-02,  4.9974e-03, -8.2504e-03, -1.0900e-02,
          -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0900e-02, -1.0851e-02,
          -8.7570e-03,  9.3451e-04,  1.7833e-02,  2.6842e-02,  1.7151e-02,
           2.5215e-04, -9.0494e-03, -1.0900e-02, -1.0900e-02, -9.5737e-03,
          -2.9424e-03,  8.9941e-03,  1.5625e-02,  8.9941e-03, -2.9424e-03,
          -9.5737e-03, -1.0829e-02, -9.2051e-03, -2.2216e-03,  9.5535e-03,
           1.5543e-02,  8.5595e-03]]], device='cuda:0')
Traceback (most recent call last):
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 732, in <module>
    optimizer=optimizer, epoch=epoch, n_epochs=n_epochs, vis_signal=vis_signal, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 418, in train
    train_gravity_loss = GravityLoss(output=output_train_xyz, trajectory_gt=output_trajectory_train_xyz[..., :-1], mask=output_trajectory_train_mask[..., :-1], lengths=output_trajectory_train_lengths)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 167, in GravityLoss
    print(trajectory_gt_yaxis_2nd_finite_difference.shape[i][:, :lengths[i]+1])
TypeError: 'int' object is not subscriptable
