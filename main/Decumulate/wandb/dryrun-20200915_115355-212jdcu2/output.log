[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise/train_set
Mixed:   0%|                                                                                                                                                                                                                                    | 0/3 [00:00<?, ?it/s]Mixed:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                         | 2/3 [00:00<00:00, 13.89it/s]===============================Dataset shape===============================
Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 12.61it/s]
Mixed : (7434,)
===========================================================================
Mixed:   0%|                                                                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.11it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.06it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 907, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 851, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 851, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 851, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 852, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 905, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 905, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 905, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 906, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 807, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 807, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 807, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 808, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 937, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 937, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 937, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 938, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 854, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 854, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 854, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 855, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 919, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 919, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 919, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 920, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 882, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 882, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 882, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 883, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 883, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 883, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 883, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 884, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 925, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 925, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 925, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 926, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 906, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 906, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 906, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 870, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 870, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 871, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 918, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 918, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 918, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 919, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 916, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 916, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 916, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 917, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 904, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 904, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 904, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 905, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [15] : batch=torch.Size([128, 901, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 901, 3]), initial position=torch.Size([128, 1, 4])
Output batch [15] : batch=torch.Size([128, 901, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 902, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [16] : batch=torch.Size([128, 849, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 849, 3]), initial position=torch.Size([128, 1, 4])
Output batch [16] : batch=torch.Size([128, 849, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 850, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [17] : batch=torch.Size([128, 953, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 953, 3]), initial position=torch.Size([128, 1, 4])
Output batch [17] : batch=torch.Size([128, 953, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 954, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [18] : batch=torch.Size([128, 819, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 819, 3]), initial position=torch.Size([128, 1, 4])
Output batch [18] : batch=torch.Size([128, 819, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 820, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [19] : batch=torch.Size([128, 863, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 863, 3]), initial position=torch.Size([128, 1, 4])
Output batch [19] : batch=torch.Size([128, 863, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 864, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [20] : batch=torch.Size([128, 883, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 883, 3]), initial position=torch.Size([128, 1, 4])
Output batch [20] : batch=torch.Size([128, 883, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 884, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [21] : batch=torch.Size([128, 808, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 808, 3]), initial position=torch.Size([128, 1, 4])
Output batch [21] : batch=torch.Size([128, 808, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 809, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [22] : batch=torch.Size([128, 932, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 932, 3]), initial position=torch.Size([128, 1, 4])
Output batch [22] : batch=torch.Size([128, 932, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 933, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [23] : batch=torch.Size([128, 792, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 792, 3]), initial position=torch.Size([128, 1, 4])
Output batch [23] : batch=torch.Size([128, 792, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 793, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [24] : batch=torch.Size([128, 865, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 865, 3]), initial position=torch.Size([128, 1, 4])
Output batch [24] : batch=torch.Size([128, 865, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 866, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [25] : batch=torch.Size([128, 914, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 914, 3]), initial position=torch.Size([128, 1, 4])
Output batch [25] : batch=torch.Size([128, 914, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 915, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [26] : batch=torch.Size([128, 759, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 759, 3]), initial position=torch.Size([128, 1, 4])
Output batch [26] : batch=torch.Size([128, 759, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 760, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [27] : batch=torch.Size([128, 849, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 849, 3]), initial position=torch.Size([128, 1, 4])
Output batch [27] : batch=torch.Size([128, 849, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 850, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [28] : batch=torch.Size([128, 848, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 848, 3]), initial position=torch.Size([128, 1, 4])
Output batch [28] : batch=torch.Size([128, 848, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 849, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [29] : batch=torch.Size([128, 776, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 776, 3]), initial position=torch.Size([128, 1, 4])
Output batch [29] : batch=torch.Size([128, 776, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 777, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [30] : batch=torch.Size([128, 912, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 912, 3]), initial position=torch.Size([128, 1, 4])
Output batch [30] : batch=torch.Size([128, 912, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 913, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [31] : batch=torch.Size([128, 832, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 832, 3]), initial position=torch.Size([128, 1, 4])
Output batch [31] : batch=torch.Size([128, 832, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 833, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [32] : batch=torch.Size([128, 871, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 871, 3]), initial position=torch.Size([128, 1, 4])
Output batch [32] : batch=torch.Size([128, 871, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 872, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [33] : batch=torch.Size([128, 817, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 817, 3]), initial position=torch.Size([128, 1, 4])
Output batch [33] : batch=torch.Size([128, 817, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 818, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [34] : batch=torch.Size([128, 873, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 873, 3]), initial position=torch.Size([128, 1, 4])
Output batch [34] : batch=torch.Size([128, 873, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 874, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [35] : batch=torch.Size([128, 792, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 792, 3]), initial position=torch.Size([128, 1, 4])
Output batch [35] : batch=torch.Size([128, 792, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 793, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [36] : batch=torch.Size([128, 855, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 855, 3]), initial position=torch.Size([128, 1, 4])
Output batch [36] : batch=torch.Size([128, 855, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 856, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [37] : batch=torch.Size([128, 942, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 3]), initial position=torch.Size([128, 1, 4])
Output batch [37] : batch=torch.Size([128, 942, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 943, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [38] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [38] : batch=torch.Size([128, 869, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [39] : batch=torch.Size([128, 855, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 855, 3]), initial position=torch.Size([128, 1, 4])
Output batch [39] : batch=torch.Size([128, 855, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 856, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [40] : batch=torch.Size([128, 847, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 847, 3]), initial position=torch.Size([128, 1, 4])
Output batch [40] : batch=torch.Size([128, 847, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 848, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [41] : batch=torch.Size([128, 859, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 859, 3]), initial position=torch.Size([128, 1, 4])
Output batch [41] : batch=torch.Size([128, 859, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 860, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [42] : batch=torch.Size([128, 817, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 817, 3]), initial position=torch.Size([128, 1, 4])
Output batch [42] : batch=torch.Size([128, 817, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 818, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [43] : batch=torch.Size([128, 857, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 857, 3]), initial position=torch.Size([128, 1, 4])
Output batch [43] : batch=torch.Size([128, 857, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 858, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [44] : batch=torch.Size([128, 795, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 795, 3]), initial position=torch.Size([128, 1, 4])
Output batch [44] : batch=torch.Size([128, 795, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 796, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [45] : batch=torch.Size([128, 859, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 859, 3]), initial position=torch.Size([128, 1, 4])
Output batch [45] : batch=torch.Size([128, 859, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 860, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [46] : batch=torch.Size([128, 879, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 879, 3]), initial position=torch.Size([128, 1, 4])
Output batch [46] : batch=torch.Size([128, 879, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 880, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [47] : batch=torch.Size([128, 967, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 967, 3]), initial position=torch.Size([128, 1, 4])
Output batch [47] : batch=torch.Size([128, 967, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 968, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [48] : batch=torch.Size([128, 920, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 920, 3]), initial position=torch.Size([128, 1, 4])
Output batch [48] : batch=torch.Size([128, 920, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 921, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [49] : batch=torch.Size([128, 866, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 866, 3]), initial position=torch.Size([128, 1, 4])
Output batch [49] : batch=torch.Size([128, 866, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 867, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [50] : batch=torch.Size([128, 874, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 874, 3]), initial position=torch.Size([128, 1, 4])
Output batch [50] : batch=torch.Size([128, 874, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 875, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [51] : batch=torch.Size([128, 939, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 939, 3]), initial position=torch.Size([128, 1, 4])
Output batch [51] : batch=torch.Size([128, 939, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 940, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [52] : batch=torch.Size([128, 857, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 857, 3]), initial position=torch.Size([128, 1, 4])
Output batch [52] : batch=torch.Size([128, 857, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 858, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [53] : batch=torch.Size([128, 799, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 799, 3]), initial position=torch.Size([128, 1, 4])
Output batch [53] : batch=torch.Size([128, 799, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 800, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [54] : batch=torch.Size([128, 853, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 853, 3]), initial position=torch.Size([128, 1, 4])
Output batch [54] : batch=torch.Size([128, 853, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 854, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [55] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [55] : batch=torch.Size([128, 928, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [56] : batch=torch.Size([128, 840, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 840, 3]), initial position=torch.Size([128, 1, 4])
Output batch [56] : batch=torch.Size([128, 840, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 841, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [57] : batch=torch.Size([128, 892, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 892, 3]), initial position=torch.Size([128, 1, 4])
Output batch [57] : batch=torch.Size([128, 892, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 893, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
train_ball_trajectory_depth_jointly_decumulate.py:265: RuntimeWarning:

invalid value encountered in long_scalars

===> [Minibatch 1/58].........tensor([[-0.1792, -0.1792, -0.1792],
        [-0.1800, -0.1800, -0.1800],
        [-0.1803, -0.1803, -0.1803],
        ...,
        [-0.0000, -0.0000, -0.0000],
        [-0.0000, -0.0000, -0.0000],
        [-0.0000, -0.0000, -0.0000]], device='cuda:0', grad_fn=<MulBackward0>)
tensor([[0.1151, 0.1151, 0.1151],
        [0.1141, 0.1141, 0.1141],
        [0.1131, 0.1131, 0.1131],
        ...,
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000]], device='cuda:0')
tensor([[-0.1792, -0.1792, -0.1792],
        [-0.1800, -0.1800, -0.1800],
        [-0.1803, -0.1803, -0.1803],
        [-0.1806, -0.1806, -0.1806],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1804, -0.1804, -0.1804],
        [-0.1804, -0.1804, -0.1804],
        [-0.1803, -0.1803, -0.1803],
        [-0.1803, -0.1803, -0.1803],
        [-0.1803, -0.1803, -0.1803],
        [-0.1802, -0.1802, -0.1802],
        [-0.1800, -0.1800, -0.1800],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1799, -0.1799, -0.1799],
        [-0.1799, -0.1799, -0.1799],
        [-0.1798, -0.1798, -0.1798],
        [-0.1798, -0.1798, -0.1798],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1796, -0.1796, -0.1796],
        [-0.1796, -0.1796, -0.1796],
        [-0.1796, -0.1796, -0.1796],
        [-0.1795, -0.1795, -0.1795],
        [-0.1795, -0.1795, -0.1795],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1790, -0.1790, -0.1790],
        [-0.1790, -0.1790, -0.1790],
        [-0.1789, -0.1789, -0.1789],
        [-0.1789, -0.1789, -0.1789],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1787, -0.1787, -0.1787],
        [-0.1787, -0.1787, -0.1787],
        [-0.1787, -0.1787, -0.1787],
        [-0.1787, -0.1787, -0.1787],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1788, -0.1788, -0.1788],
        [-0.1790, -0.1790, -0.1790],
        [-0.1788, -0.1788, -0.1788],
        [-0.1789, -0.1789, -0.1789],
        [-0.1789, -0.1789, -0.1789],
        [-0.1789, -0.1789, -0.1789],
        [-0.1789, -0.1789, -0.1789],
        [-0.1789, -0.1789, -0.1789],
        [-0.1790, -0.1790, -0.1790],
        [-0.1791, -0.1791, -0.1791],
        [-0.1790, -0.1790, -0.1790],
        [-0.1789, -0.1789, -0.1789],
        [-0.1790, -0.1790, -0.1790],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1791, -0.1791, -0.1791],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1792, -0.1792, -0.1792],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1793, -0.1793, -0.1793],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1795, -0.1795, -0.1795],
        [-0.1795, -0.1795, -0.1795],
        [-0.1795, -0.1795, -0.1795],
        [-0.1795, -0.1795, -0.1795],
        [-0.1794, -0.1794, -0.1794],
        [-0.1795, -0.1795, -0.1795],
        [-0.1796, -0.1796, -0.1796],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1794, -0.1794, -0.1794],
        [-0.1795, -0.1795, -0.1795],
        [-0.1797, -0.1797, -0.1797],
        [-0.1799, -0.1799, -0.1799],
        [-0.1802, -0.1802, -0.1802],
        [-0.1805, -0.1805, -0.1805],
        [-0.1803, -0.1803, -0.1803],
        [-0.1797, -0.1797, -0.1797],
        [-0.1796, -0.1796, -0.1796],
        [-0.1796, -0.1796, -0.1796],
        [-0.1796, -0.1796, -0.1796],
        [-0.1796, -0.1796, -0.1796],
        [-0.1796, -0.1796, -0.1796],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1798, -0.1798, -0.1798],
        [-0.1798, -0.1798, -0.1798],
        [-0.1798, -0.1798, -0.1798],
        [-0.1798, -0.1798, -0.1798],
        [-0.1799, -0.1799, -0.1799],
        [-0.1799, -0.1799, -0.1799],
        [-0.1799, -0.1799, -0.1799],
        [-0.1799, -0.1799, -0.1799],
        [-0.1799, -0.1799, -0.1799],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1803, -0.1803, -0.1803],
        [-0.1803, -0.1803, -0.1803],
        [-0.1803, -0.1803, -0.1803],
        [-0.1803, -0.1803, -0.1803],
        [-0.1804, -0.1804, -0.1804],
        [-0.1804, -0.1804, -0.1804],
        [-0.1804, -0.1804, -0.1804],
        [-0.1804, -0.1804, -0.1804],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1806, -0.1806, -0.1806],
        [-0.1806, -0.1806, -0.1806],
        [-0.1806, -0.1806, -0.1806],
        [-0.1806, -0.1806, -0.1806],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1810, -0.1810, -0.1810],
        [-0.1810, -0.1810, -0.1810],
        [-0.1810, -0.1810, -0.1810],
        [-0.1811, -0.1811, -0.1811],
        [-0.1811, -0.1811, -0.1811],
        [-0.1811, -0.1811, -0.1811],
        [-0.1811, -0.1811, -0.1811],
        [-0.1812, -0.1812, -0.1812],
        [-0.1812, -0.1812, -0.1812],
        [-0.1812, -0.1812, -0.1812],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1813, -0.1813, -0.1813],
        [-0.1812, -0.1812, -0.1812],
        [-0.1812, -0.1812, -0.1812],
        [-0.1811, -0.1811, -0.1811],
        [-0.1811, -0.1811, -0.1811],
        [-0.1810, -0.1810, -0.1810],
        [-0.1810, -0.1810, -0.1810],
        [-0.1810, -0.1810, -0.1810],
        [-0.1809, -0.1809, -0.1809],
        [-0.1809, -0.1809, -0.1809],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1808, -0.1808, -0.1808],
        [-0.1807, -0.1807, -0.1807],
        [-0.1807, -0.1807, -0.1807],
        [-0.1806, -0.1806, -0.1806],
        [-0.1807, -0.1807, -0.1807],
        [-0.1806, -0.1806, -0.1806],
        [-0.1806, -0.1806, -0.1806],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1805, -0.1805, -0.1805],
        [-0.1804, -0.1804, -0.1804],
        [-0.1804, -0.1804, -0.1804],
        [-0.1804, -0.1804, -0.1804],
        [-0.1803, -0.1803, -0.1803],
        [-0.1803, -0.1803, -0.1803],
        [-0.1803, -0.1803, -0.1803],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1802, -0.1802, -0.1802],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1801, -0.1801, -0.1801],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1800, -0.1800, -0.1800],
        [-0.1799, -0.1799, -0.1799],
        [-0.1799, -0.1799, -0.1799],
        [-0.1799, -0.1799, -0.1799],
        [-0.1798, -0.1798, -0.1798],
        [-0.1798, -0.1798, -0.1798],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1796, -0.1796, -0.1796],
        [-0.1797, -0.1797, -0.1797],
        [-0.1797, -0.1797, -0.1797],
        [-0.1799, -0.1799, -0.1799]], device='cuda:0', grad_fn=<SliceBackward>)
tensor([[ 0.1151,  0.1151,  0.1151],
        [ 0.1141,  0.1141,  0.1141],
        [ 0.1131,  0.1131,  0.1131],
        [ 0.1121,  0.1121,  0.1121],
        [ 0.1111,  0.1111,  0.1111],
        [ 0.1101,  0.1101,  0.1101],
        [ 0.1091,  0.1091,  0.1091],
        [ 0.1081,  0.1081,  0.1081],
        [ 0.1071,  0.1071,  0.1071],
        [ 0.1061,  0.1061,  0.1061],
        [ 0.1051,  0.1051,  0.1051],
        [ 0.1041,  0.1041,  0.1041],
        [ 0.1031,  0.1031,  0.1031],
        [ 0.1022,  0.1022,  0.1022],
        [ 0.1012,  0.1012,  0.1012],
        [ 0.1002,  0.1002,  0.1002],
        [ 0.0992,  0.0992,  0.0992],
        [ 0.0982,  0.0982,  0.0982],
        [ 0.0972,  0.0972,  0.0972],
        [ 0.0962,  0.0962,  0.0962],
        [ 0.0952,  0.0952,  0.0952],
        [ 0.0942,  0.0942,  0.0942],
        [ 0.0932,  0.0932,  0.0932],
        [ 0.0922,  0.0922,  0.0922],
        [ 0.0912,  0.0912,  0.0912],
        [ 0.0903,  0.0903,  0.0903],
        [ 0.0893,  0.0893,  0.0893],
        [ 0.0883,  0.0883,  0.0883],
        [ 0.0873,  0.0873,  0.0873],
        [ 0.0863,  0.0863,  0.0863],
        [ 0.0853,  0.0853,  0.0853],
        [ 0.0843,  0.0843,  0.0843],
        [ 0.0833,  0.0833,  0.0833],
        [ 0.0823,  0.0823,  0.0823],
        [ 0.0813,  0.0813,  0.0813],
        [ 0.0803,  0.0803,  0.0803],
        [ 0.0794,  0.0794,  0.0794],
        [ 0.0784,  0.0784,  0.0784],
        [ 0.0774,  0.0774,  0.0774],
        [ 0.0764,  0.0764,  0.0764],
        [ 0.0754,  0.0754,  0.0754],
        [ 0.0744,  0.0744,  0.0744],
        [ 0.0734,  0.0734,  0.0734],
        [ 0.0724,  0.0724,  0.0724],
        [ 0.0714,  0.0714,  0.0714],
        [ 0.0704,  0.0704,  0.0704],
        [ 0.0694,  0.0694,  0.0694],
        [ 0.0684,  0.0684,  0.0684],
        [ 0.0674,  0.0674,  0.0674],
        [ 0.0665,  0.0665,  0.0665],
        [ 0.0655,  0.0655,  0.0655],
        [ 0.0645,  0.0645,  0.0645],
        [ 0.0635,  0.0635,  0.0635],
        [ 0.0625,  0.0625,  0.0625],
        [ 0.0615,  0.0615,  0.0615],
        [ 0.0605,  0.0605,  0.0605],
        [ 0.0595,  0.0595,  0.0595],
        [ 0.0585,  0.0585,  0.0585],
        [ 0.0575,  0.0575,  0.0575],
        [ 0.0565,  0.0565,  0.0565],
        [ 0.0556,  0.0556,  0.0556],
        [ 0.0546,  0.0546,  0.0546],
        [ 0.0536,  0.0536,  0.0536],
        [ 0.0526,  0.0526,  0.0526],
        [ 0.0516,  0.0516,  0.0516],
        [ 0.0506,  0.0506,  0.0506],
        [ 0.0496,  0.0496,  0.0496],
        [ 0.0486,  0.0486,  0.0486],
        [ 0.0476,  0.0476,  0.0476],
        [ 0.0466,  0.0466,  0.0466],
        [ 0.0456,  0.0456,  0.0456],
        [ 0.0446,  0.0446,  0.0446],
        [ 0.0437,  0.0437,  0.0437],
        [ 0.0427,  0.0427,  0.0427],
        [ 0.0417,  0.0417,  0.0417],
        [ 0.0407,  0.0407,  0.0407],
        [ 0.0397,  0.0397,  0.0397],
        [ 0.0387,  0.0387,  0.0387],
        [ 0.0377,  0.0377,  0.0377],
        [ 0.0367,  0.0367,  0.0367],
        [ 0.0357,  0.0357,  0.0357],
        [ 0.0347,  0.0347,  0.0347],
        [ 0.0337,  0.0337,  0.0337],
        [ 0.0327,  0.0327,  0.0327],
        [ 0.0318,  0.0318,  0.0318],
        [ 0.0308,  0.0308,  0.0308],
        [ 0.0298,  0.0298,  0.0298],
        [ 0.0291,  0.0291,  0.0291],
        [ 0.0284,  0.0284,  0.0284],
        [ 0.0277,  0.0277,  0.0277],
        [ 0.0270,  0.0270,  0.0270],
        [ 0.0264,  0.0264,  0.0264],
        [ 0.0258,  0.0258,  0.0258],
        [ 0.0251,  0.0251,  0.0251],
        [ 0.0245,  0.0245,  0.0245],
        [ 0.0240,  0.0240,  0.0240],
        [ 0.0234,  0.0234,  0.0234],
        [ 0.0228,  0.0228,  0.0228],
        [ 0.0223,  0.0223,  0.0223],
        [ 0.0217,  0.0217,  0.0217],
        [ 0.0213,  0.0213,  0.0213],
        [ 0.0207,  0.0207,  0.0207],
        [ 0.0202,  0.0202,  0.0202],
        [ 0.0198,  0.0198,  0.0198],
        [ 0.0193,  0.0193,  0.0193],
        [ 0.0188,  0.0188,  0.0188],
        [ 0.0184,  0.0184,  0.0184],
        [ 0.0180,  0.0180,  0.0180],
        [ 0.0175,  0.0175,  0.0175],
        [ 0.0171,  0.0171,  0.0171],
        [ 0.0167,  0.0167,  0.0167],
        [ 0.0163,  0.0163,  0.0163],
        [ 0.0159,  0.0159,  0.0159],
        [ 0.0155,  0.0155,  0.0155],
        [ 0.0152,  0.0152,  0.0152],
        [ 0.0148,  0.0148,  0.0148],
        [ 0.0144,  0.0144,  0.0144],
        [ 0.0141,  0.0141,  0.0141],
        [ 0.0138,  0.0138,  0.0138],
        [ 0.0134,  0.0134,  0.0134],
        [ 0.0131,  0.0131,  0.0131],
        [ 0.0128,  0.0128,  0.0128],
        [ 0.0125,  0.0125,  0.0125],
        [ 0.0122,  0.0122,  0.0122],
        [ 0.0119,  0.0119,  0.0119],
        [ 0.0116,  0.0116,  0.0116],
        [ 0.0114,  0.0114,  0.0114],
        [ 0.0111,  0.0111,  0.0111],
        [ 0.0108,  0.0108,  0.0108],
        [ 0.0106,  0.0106,  0.0106],
        [ 0.0103,  0.0103,  0.0103],
        [ 0.0101,  0.0101,  0.0101],
        [ 0.0098,  0.0098,  0.0098],
        [ 0.0096,  0.0096,  0.0096],
        [ 0.0093,  0.0093,  0.0093],
        [ 0.0091,  0.0091,  0.0091],
        [ 0.0089,  0.0089,  0.0089],
        [ 0.0087,  0.0087,  0.0087],
        [ 0.0085,  0.0085,  0.0085],
        [ 0.0083,  0.0083,  0.0083],
        [ 0.0081,  0.0081,  0.0081],
        [ 0.0079,  0.0079,  0.0079],
        [ 0.0077,  0.0077,  0.0077],
        [ 0.0075,  0.0075,  0.0075],
        [ 0.0074,  0.0074,  0.0074],
        [ 0.0072,  0.0072,  0.0072],
        [ 0.0070,  0.0070,  0.0070],
        [ 0.0068,  0.0068,  0.0068],
        [ 0.0067,  0.0067,  0.0067],
        [ 0.0065,  0.0065,  0.0065],
        [ 0.0064,  0.0064,  0.0064],
        [ 0.0062,  0.0062,  0.0062],
        [ 0.0061,  0.0061,  0.0061],
        [ 0.0059,  0.0059,  0.0059],
        [ 0.0058,  0.0058,  0.0058],
        [ 0.0057,  0.0057,  0.0057],
        [ 0.0055,  0.0055,  0.0055],
        [ 0.0054,  0.0054,  0.0054],
        [ 0.0052,  0.0052,  0.0052],
        [ 0.0051,  0.0051,  0.0051],
        [ 0.0050,  0.0050,  0.0050],
        [ 0.0049,  0.0049,  0.0049],
        [ 0.0048,  0.0048,  0.0048],
        [ 0.0047,  0.0047,  0.0047],
        [-0.1059, -0.1059, -0.1059],
        [-0.1049, -0.1049, -0.1049],
        [-0.1040, -0.1040, -0.1040],
        [-0.1030, -0.1030, -0.1030],
        [-0.1021, -0.1021, -0.1021],
        [-0.1011, -0.1011, -0.1011],
        [-0.1002, -0.1002, -0.1002],
        [-0.0993, -0.0993, -0.0993],
        [-0.0983, -0.0983, -0.0983],
        [-0.0974, -0.0974, -0.0974],
        [-0.0964, -0.0964, -0.0964],
        [-0.0955, -0.0955, -0.0955],
        [-0.0946, -0.0946, -0.0946],
        [-0.0936, -0.0936, -0.0936],
        [-0.0927, -0.0927, -0.0927],
        [-0.0917, -0.0917, -0.0917],
        [-0.0908, -0.0908, -0.0908],
        [-0.0899, -0.0899, -0.0899],
        [-0.0889, -0.0889, -0.0889],
        [-0.0880, -0.0880, -0.0880],
        [-0.0870, -0.0870, -0.0870],
        [-0.0861, -0.0861, -0.0861],
        [-0.0852, -0.0852, -0.0852],
        [-0.0842, -0.0842, -0.0842],
        [-0.0833, -0.0833, -0.0833],
        [-0.0824, -0.0824, -0.0824],
        [-0.0814, -0.0814, -0.0814],
        [-0.0805, -0.0805, -0.0805],
        [-0.0795, -0.0795, -0.0795],
        [-0.0786, -0.0786, -0.0786],
        [-0.0777, -0.0777, -0.0777],
        [-0.0767, -0.0767, -0.0767],
        [-0.0758, -0.0758, -0.0758],
        [-0.0748, -0.0748, -0.0748],
        [-0.0739, -0.0739, -0.0739],
        [-0.0730, -0.0730, -0.0730],
        [-0.0720, -0.0720, -0.0720],
        [-0.0711, -0.0711, -0.0711],
        [-0.0701, -0.0701, -0.0701],
        [-0.0692, -0.0692, -0.0692],
        [-0.0683, -0.0683, -0.0683],
        [-0.0673, -0.0673, -0.0673],
        [-0.0664, -0.0664, -0.0664],
        [-0.0654, -0.0654, -0.0654],
        [-0.0645, -0.0645, -0.0645],
        [-0.0636, -0.0636, -0.0636],
        [-0.0626, -0.0626, -0.0626],
        [-0.0617, -0.0617, -0.0617],
        [-0.0608, -0.0608, -0.0608],
        [-0.0598, -0.0598, -0.0598],
        [-0.0589, -0.0589, -0.0589],
        [-0.0579, -0.0579, -0.0579],
        [-0.0570, -0.0570, -0.0570],
        [-0.0560, -0.0560, -0.0560],
        [-0.0551, -0.0551, -0.0551],
        [-0.0542, -0.0542, -0.0542],
        [-0.0532, -0.0532, -0.0532],
        [-0.0523, -0.0523, -0.0523],
        [-0.0514, -0.0514, -0.0514],
        [-0.0504, -0.0504, -0.0504],
        [-0.0495, -0.0495, -0.0495],
        [-0.0485, -0.0485, -0.0485],
        [-0.0476, -0.0476, -0.0476],
        [-0.0467, -0.0467, -0.0467],
        [-0.0457, -0.0457, -0.0457],
        [-0.0448, -0.0448, -0.0448],
        [-0.0438, -0.0438, -0.0438],
        [-0.0429, -0.0429, -0.0429],
        [-0.0420, -0.0420, -0.0420],
        [-0.0410, -0.0410, -0.0410],
        [-0.0401, -0.0401, -0.0401],
        [-0.0391, -0.0391, -0.0391],
        [-0.0382, -0.0382, -0.0382],
        [-0.0373, -0.0373, -0.0373],
        [-0.0363, -0.0363, -0.0363],
        [-0.0354, -0.0354, -0.0354],
        [-0.0344, -0.0344, -0.0344],
        [-0.0335, -0.0335, -0.0335],
        [-0.0326, -0.0326, -0.0326],
        [-0.0316, -0.0316, -0.0316],
        [-0.0307, -0.0307, -0.0307],
        [-0.0298, -0.0298, -0.0298],
        [-0.0288, -0.0288, -0.0288],
        [-0.0280, -0.0280, -0.0280],
        [-0.0273, -0.0273, -0.0273],
        [-0.0266, -0.0266, -0.0266],
        [-0.0260, -0.0260, -0.0260],
        [-0.0254, -0.0254, -0.0254],
        [-0.0248, -0.0248, -0.0248],
        [-0.0242, -0.0242, -0.0242],
        [-0.0236, -0.0236, -0.0236],
        [-0.0231, -0.0231, -0.0231],
        [-0.0225, -0.0225, -0.0225],
        [-0.0220, -0.0220, -0.0220],
        [-0.0214, -0.0214, -0.0214],
        [-0.0209, -0.0209, -0.0209],
        [-0.0204, -0.0204, -0.0204],
        [-0.0200, -0.0200, -0.0200],
        [-0.0195, -0.0195, -0.0195],
        [-0.0190, -0.0190, -0.0190],
        [-0.0186, -0.0186, -0.0186],
        [-0.0181, -0.0181, -0.0181],
        [-0.0177, -0.0177, -0.0177],
        [-0.0173, -0.0173, -0.0173],
        [-0.0169, -0.0169, -0.0169],
        [-0.0165, -0.0165, -0.0165],
        [-0.0161, -0.0161, -0.0161],
        [-0.0157, -0.0157, -0.0157],
        [-0.0153, -0.0153, -0.0153],
        [-0.0149, -0.0149, -0.0149],
        [-0.0146, -0.0146, -0.0146],
        [-0.0142, -0.0142, -0.0142],
        [-0.0139, -0.0139, -0.0139],
        [-0.0136, -0.0136, -0.0136],
        [-0.0132, -0.0132, -0.0132],
        [-0.0129, -0.0129, -0.0129],
        [-0.0126, -0.0126, -0.0126],
        [-0.0123, -0.0123, -0.0123],
        [-0.0120, -0.0120, -0.0120],
        [-0.0117, -0.0117, -0.0117],
        [-0.0115, -0.0115, -0.0115],
        [-0.0112, -0.0112, -0.0112],
        [-0.0109, -0.0109, -0.0109],
        [-0.0107, -0.0107, -0.0107],
        [-0.0104, -0.0104, -0.0104],
        [-0.0102, -0.0102, -0.0102],
        [-0.0099, -0.0099, -0.0099],
        [-0.0097, -0.0097, -0.0097],
        [-0.0095, -0.0095, -0.0095],
        [-0.0092, -0.0092, -0.0092],
        [-0.0090, -0.0090, -0.0090],
        [-0.0088, -0.0088, -0.0088],
        [-0.0086, -0.0086, -0.0086],
        [-0.0084, -0.0084, -0.0084],
        [-0.0082, -0.0082, -0.0082],
        [-0.0080, -0.0080, -0.0080],
        [-0.0078, -0.0078, -0.0078],
        [-0.0076, -0.0076, -0.0076],
        [-0.0074, -0.0074, -0.0074],
        [-0.0072, -0.0072, -0.0072],
        [-0.0071, -0.0071, -0.0071],
        [-0.0069, -0.0069, -0.0069],
        [-0.0068, -0.0068, -0.0068],
        [-0.0066, -0.0066, -0.0066],
        [-0.0064, -0.0064, -0.0064],
        [-0.0063, -0.0063, -0.0063],
        [-0.0061, -0.0061, -0.0061],
        [-0.0060, -0.0060, -0.0060],
        [-0.0058, -0.0058, -0.0058],
        [-0.0057, -0.0057, -0.0057],
        [-0.0056, -0.0056, -0.0056],
        [-0.0054, -0.0054, -0.0054],
        [-0.0053, -0.0053, -0.0053],
        [-0.0052, -0.0052, -0.0052],
        [-0.0051, -0.0051, -0.0051],
        [-0.0049, -0.0049, -0.0049],
        [-0.0048, -0.0048, -0.0048],
        [-0.0047, -0.0047, -0.0047],
        [-0.0046, -0.0046, -0.0046]], device='cuda:0')
