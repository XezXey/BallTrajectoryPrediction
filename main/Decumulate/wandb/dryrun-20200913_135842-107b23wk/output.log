[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                | 0/1 [00:00<?, ?it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.25it/s]
===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed:   0%|                                                                                                | 0/2 [00:00<?, ?it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.36it/s]Mixed: 100%|████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.32it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 870, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 853, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 853, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 854, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 854, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 792, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 792, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 793, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 793, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 904, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 904, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 905, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 905, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 911, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 911, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 912, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 912, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 908, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 854, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 854, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 855, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 855, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 894, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 894, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 895, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 824, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 824, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 825, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 825, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 960, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 960, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 961, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 961, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 918, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 918, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 919, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 919, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 839, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 840, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 840, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 813, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 813, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 814, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 814, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 941, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 941, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 942, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 952, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........tensor([147, 448], device='cuda:0')
tensor([[-0.3236],
        [-0.3220],
        [-0.3228],
        [-0.3227],
        [-0.3229],
        [-0.3221],
        [-0.3235],
        [-0.3221],
        [-0.3220],
        [-0.3222],
        [-0.3226],
        [-0.3220],
        [-0.3226],
        [-0.3224],
        [-0.3223],
        [-0.3216],
        [-0.3231],
        [-0.3221],
        [-0.3227],
        [-0.3218],
        [-0.3224],
        [-0.3217],
        [-0.3224],
        [-0.3229],
        [-0.3223],
        [-0.3222],
        [-0.3222],
        [-0.3212],
        [-0.3221],
        [-0.3221],
        [-0.3228],
        [-0.3222],
        [-0.3209],
        [-0.3203],
        [-0.3229],
        [-0.3216],
        [-0.3221],
        [-0.3227],
        [-0.3214],
        [-0.3209],
        [-0.3214],
        [-0.3215],
        [-0.3206],
        [-0.3211],
        [-0.3210],
        [-0.3219],
        [-0.3221],
        [-0.3197],
        [-0.3203],
        [-0.3213],
        [-0.3204],
        [-0.3226],
        [-0.3203],
        [-0.3205],
        [-0.3211],
        [-0.3207],
        [-0.3202],
        [-0.3203],
        [-0.3201],
        [-0.3218],
        [-0.3208],
        [-0.3208],
        [-0.3202],
        [-0.3190],
        [-0.3197],
        [-0.3202],
        [-0.3201],
        [-0.3199],
        [-0.3199],
        [-0.3205],
        [-0.3194],
        [-0.3194],
        [-0.3191],
        [-0.3202],
        [-0.3191],
        [-0.3197],
        [-0.3204],
        [-0.3186],
        [-0.3186],
        [-0.3197],
        [-0.3189],
        [-0.3198],
        [-0.3194],
        [-0.3193],
        [-0.3199],
        [-0.3188],
        [-0.3197],
        [-0.3194],
        [-0.3184],
        [-0.3186],
        [-0.3192],
        [-0.3186],
        [-0.3185],
        [-0.3190],
        [-0.3195],
        [-0.3189],
        [-0.3187],
        [-0.3184],
        [-0.3189],
        [-0.3197],
        [-0.3182],
        [-0.3194],
        [-0.3187],
        [-0.3189],
        [-0.3188],
        [-0.3192],
        [-0.3200],
        [-0.3180],
        [-0.3191],
        [-0.3181],
        [-0.3181],
        [-0.3181],
        [-0.3200],
        [-0.3187],
        [-0.3181],
        [-0.3190],
        [-0.3185],
        [-0.3187],
        [-0.3187],
        [-0.3190],
        [-0.3186],
        [-0.3195],
        [-0.3181],
        [-0.3187],
        [-0.3187],
        [-0.3183],
        [-0.3180],
        [-0.3188],
        [-0.3188],
        [-0.3183],
        [-0.3200],
        [-0.3187],
        [-0.3180],
        [-0.3185],
        [-0.3181],
        [-0.3188],
        [-0.3188],
        [-0.3190],
        [-0.3189],
        [-0.3187],
        [-0.3186],
        [-0.3186],
        [-0.3180],
        [-0.3180],
        [-0.3183],
        [-0.3187],
        [-0.3178],
        [-0.3186],
        [-0.3181],
        [-0.3189],
        [-0.3186],
        [-0.3201],
        [-0.3191],
        [-0.3200],
        [-0.3192],
        [-0.3206],
        [-0.3198],
        [-0.3195],
        [-0.3195],
        [-0.3205],
        [-0.3191],
        [-0.3202],
        [-0.3195],
        [-0.3192],
        [-0.3202],
        [-0.3203],
        [-0.3207],
        [-0.3196],
        [-0.3205],
        [-0.3185],
        [-0.3222],
        [-0.3201],
        [-0.3202],
        [-0.3208],
        [-0.3207],
        [-0.3213],
        [-0.3204],
        [-0.3204],
        [-0.3222],
        [-0.3208],
        [-0.3202],
        [-0.3219],
        [-0.3212],
        [-0.3222],
        [-0.3214],
        [-0.3203],
        [-0.3218],
        [-0.3212],
        [-0.3224],
        [-0.3216],
        [-0.3208],
        [-0.3225],
        [-0.3220],
        [-0.3230],
        [-0.3210],
        [-0.3222],
        [-0.3231],
        [-0.3224],
        [-0.3227],
        [-0.3221],
        [-0.3230],
        [-0.3218],
        [-0.3225],
        [-0.3228],
        [-0.3225],
        [-0.3230],
        [-0.3226],
        [-0.3229],
        [-0.3225],
        [-0.3230],
        [-0.3232],
        [-0.3233],
        [-0.3229],
        [-0.3229],
        [-0.3242],
        [-0.3221],
        [-0.3239],
        [-0.3234],
        [-0.3230],
        [-0.3241],
        [-0.3234],
        [-0.3236],
        [-0.3240],
        [-0.3225],
        [-0.3238],
        [-0.3233],
        [-0.3242],
        [-0.3239],
        [-0.3241],
        [-0.3237],
        [-0.3240],
        [-0.3220],
        [-0.3241],
        [-0.3243],
        [-0.3231],
        [-0.3235],
        [-0.3242],
        [-0.3244],
        [-0.3236],
        [-0.3237],
        [-0.3241],
        [-0.3225],
        [-0.3226],
        [-0.3229],
        [-0.3177],
        [-0.3185],
        [-0.3184],
        [-0.3192],
        [-0.3197],
        [-0.3191],
        [-0.3195],
        [-0.3192],
        [-0.3196],
        [-0.3193],
        [-0.3184],
        [-0.3206],
        [-0.3197],
        [-0.3195],
        [-0.3198],
        [-0.3196],
        [-0.3190],
        [-0.3206],
        [-0.3196],
        [-0.3206],
        [-0.3197],
        [-0.3218],
        [-0.3191],
        [-0.3205],
        [-0.3198],
        [-0.3198],
        [-0.3211],
        [-0.3208],
        [-0.3206],
        [-0.3212],
        [-0.3220],
        [-0.3203],
        [-0.3204],
        [-0.3209],
        [-0.3221],
        [-0.3216],
        [-0.3213],
        [-0.3220],
        [-0.3217],
        [-0.3211],
        [-0.3194],
        [-0.3212],
        [-0.3218],
        [-0.3212],
        [-0.3214],
        [-0.3221],
        [-0.3213],
        [-0.3220],
        [-0.3210],
        [-0.3211],
        [-0.3224],
        [-0.3225],
        [-0.3212],
        [-0.3225],
        [-0.3215],
        [-0.3220],
        [-0.3215],
        [-0.3230],
        [-0.3221],
        [-0.3231],
        [-0.3214],
        [-0.3200],
        [-0.3207],
        [-0.3201],
        [-0.3196],
        [-0.3186],
        [-0.3193],
        [-0.3193],
        [-0.3201],
        [-0.3188],
        [-0.3194],
        [-0.3190],
        [-0.3196],
        [-0.3189],
        [-0.3190],
        [-0.3194],
        [-0.3200],
        [-0.3188],
        [-0.3201],
        [-0.3209],
        [-0.3204],
        [-0.3196],
        [-0.3202],
        [-0.3200],
        [-0.3198],
        [-0.3202],
        [-0.3206],
        [-0.3205],
        [-0.3217],
        [-0.3205],
        [-0.3198],
        [-0.3204],
        [-0.3207],
        [-0.3202],
        [-0.3201],
        [-0.3202],
        [-0.3218],
        [-0.3210],
        [-0.3201],
        [-0.3211],
        [-0.3200],
        [-0.3209],
        [-0.3216],
        [-0.3219],
        [-0.3206],
        [-0.3201],
        [-0.3215],
        [-0.3195],
        [-0.3182],
        [-0.3198],
        [-0.3194],
        [-0.3193],
        [-0.3193],
        [-0.3197],
        [-0.3197],
        [-0.3193],
        [-0.3187],
        [-0.3190],
        [-0.3197],
        [-0.3195],
        [-0.3191],
        [-0.3203],
        [-0.3203],
        [-0.3202],
        [-0.3194],
        [-0.3206],
        [-0.3195],
        [-0.3202],
        [-0.3208],
        [-0.3195],
        [-0.3188],
        [-0.3192],
        [-0.3200],
        [-0.3214],
        [-0.3191],
        [-0.3201],
        [-0.3188],
        [-0.3190],
        [-0.3181],
        [-0.3190],
        [-0.3190],
        [-0.3196],
        [-0.3194],
        [-0.3199],
        [-0.3191],
        [-0.3203],
        [-0.3187],
        [-0.3193],
        [-0.3187],
        [-0.3194],
        [-0.3195],
        [-0.3193],
        [-0.3193],
        [-0.3198],
        [-0.3187],
        [-0.3193],
        [-0.3181],
        [-0.3188],
        [-0.3195],
        [-0.3187],
        [-0.3192],
        [-0.3187],
        [-0.3184],
        [-0.3188],
        [-0.3189],
        [-0.3188],
        [-0.3192],
        [-0.3194],
        [-0.3190],
        [-0.3190],
        [-0.3189],
        [-0.3190],
        [-0.3187],
        [-0.3185],
        [-0.3184],
        [-0.3182],
        [-0.3178],
        [-0.3190],
        [-0.3185],
        [-0.3191],
        [-0.3190],
        [-0.3183],
        [-0.3184],
        [-0.3181],
        [-0.3194],
        [-0.3180],
        [-0.3186],
        [-0.3191],
        [-0.3189],
        [-0.3179],
        [-0.3186],
        [-0.3180],
        [-0.3190],
        [-0.3189],
        [-0.3186],
        [-0.3189],
        [-0.3184],
        [-0.3192],
        [-0.3180],
        [-0.3200],
        [-0.3197],
        [-0.3202],
        [-0.3199],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296],
        [-0.3296]], device='cuda:0', grad_fn=<SelectBackward>)
