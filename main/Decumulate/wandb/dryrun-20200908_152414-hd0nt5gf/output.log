[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise/val_set
Mixed:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.00it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 16.96it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
Mixed:   0%|                                                                                                     | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.16it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.12it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 1915, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1915, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 1916, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1916, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 1815, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1815, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 1816, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1816, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 1917, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1917, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 1918, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1918, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 2364, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2364, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 2365, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2365, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 1836, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1836, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 1837, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1837, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 2248, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2248, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 2249, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2249, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 1817, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1817, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 1818, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1818, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 2024, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2024, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 2025, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2025, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 2042, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2042, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 2043, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2043, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 2112, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2112, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 2113, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2113, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 2028, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2028, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 2029, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2029, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 2187, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2187, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 2188, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2188, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 2255, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2255, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 2256, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2256, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 1775, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 1775, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 1776, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 1776, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 2171, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 2171, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 2172, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 2172, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.001
===> [Minibatch 1/15].........torch.Size([128, 1938, 1]) torch.Size([128, 1938, 2]) torch.Size([128, 1, 4]) torch.Size([128, 1938, 1])
tensor([[  0],
        [136],
        [449],
        [581]], device='cuda:0')
[tensor([[57.5615],
        [-0.1821],
        [-0.1823],
        [-0.1802],
        [-0.1774],
        [-0.1733],
        [-0.1744],
        [-0.1735],
        [-0.1698],
        [-0.1697],
        [-0.1684],
        [-0.1683],
        [-0.1681],
        [-0.1679],
        [-0.1677],
        [-0.1676],
        [-0.1675],
        [-0.1674],
        [-0.1674],
        [-0.1675],
        [-0.1673],
        [-0.1674],
        [-0.1674],
        [-0.1675],
        [-0.1675],
        [-0.1676],
        [-0.1676],
        [-0.1677],
        [-0.1677],
        [-0.1678],
        [-0.1677],
        [-0.1678],
        [-0.1680],
        [-0.1681],
        [-0.1681],
        [-0.1681],
        [-0.1680],
        [-0.1681],
        [-0.1682],
        [-0.1682],
        [-0.1682],
        [-0.1682],
        [-0.1684],
        [-0.1684],
        [-0.1684],
        [-0.1685],
        [-0.1686],
        [-0.1686],
        [-0.1686],
        [-0.1687],
        [-0.1687],
        [-0.1688],
        [-0.1689],
        [-0.1689],
        [-0.1689],
        [-0.1692],
        [-0.1692],
        [-0.1692],
        [-0.1693],
        [-0.1694],
        [-0.1694],
        [-0.1695],
        [-0.1695],
        [-0.1696],
        [-0.1698],
        [-0.1698],
        [-0.1699],
        [-0.1698],
        [-0.1700],
        [-0.1700],
        [-0.1701],
        [-0.1702],
        [-0.1703],
        [-0.1703],
        [-0.1704],
        [-0.1705],
        [-0.1705],
        [-0.1706],
        [-0.1706],
        [-0.1707],
        [-0.1707],
        [-0.1708],
        [-0.1708],
        [-0.1708],
        [-0.1709],
        [-0.1710],
        [-0.1712],
        [-0.1710],
        [-0.1710],
        [-0.1711],
        [-0.1711],
        [-0.1713],
        [-0.1713],
        [-0.1713],
        [-0.1722],
        [-0.1731],
        [-0.1722],
        [-0.1721],
        [-0.1722],
        [-0.1724],
        [-0.1728],
        [-0.1735],
        [-0.1730],
        [-0.1731],
        [-0.1728],
        [-0.1728],
        [-0.1729],
        [-0.1730],
        [-0.1731],
        [-0.1733],
        [-0.1742],
        [-0.1741],
        [-0.1742],
        [-0.1743],
        [-0.1744],
        [-0.1744],
        [-0.1744],
        [-0.1742],
        [-0.1745],
        [-0.1746],
        [-0.1747],
        [-0.1746],
        [-0.1746],
        [-0.1747],
        [-0.1750],
        [-0.1751],
        [-0.1749],
        [-0.1752],
        [-0.1748],
        [-0.1756],
        [-0.1760],
        [-0.1747],
        [-0.1727],
        [-0.1718],
        [-0.1718],
        [-0.1710],
        [-0.1703]], device='cuda:0', grad_fn=<CatBackward>), tensor([[68.9434],
        [-0.1704],
        [-0.1701],
        [-0.1690],
        [-0.1678],
        [-0.1671],
        [-0.1665],
        [-0.1664],
        [-0.1660],
        [-0.1657],
        [-0.1655],
        [-0.1654],
        [-0.1654],
        [-0.1654],
        [-0.1654],
        [-0.1655],
        [-0.1654],
        [-0.1654],
        [-0.1655],
        [-0.1655],
        [-0.1655],
        [-0.1655],
        [-0.1654],
        [-0.1656],
        [-0.1656],
        [-0.1657],
        [-0.1656],
        [-0.1660],
        [-0.1659],
        [-0.1660],
        [-0.1660],
        [-0.1660],
        [-0.1661],
        [-0.1666],
        [-0.1665],
        [-0.1660],
        [-0.1665],
        [-0.1665],
        [-0.1666],
        [-0.1666],
        [-0.1667],
        [-0.1668],
        [-0.1669],
        [-0.1670],
        [-0.1671],
        [-0.1672],
        [-0.1673],
        [-0.1675],
        [-0.1677],
        [-0.1677],
        [-0.1678],
        [-0.1679],
        [-0.1680],
        [-0.1681],
        [-0.1682],
        [-0.1683],
        [-0.1684],
        [-0.1687],
        [-0.1687],
        [-0.1688],
        [-0.1688],
        [-0.1689],
        [-0.1690],
        [-0.1691],
        [-0.1692],
        [-0.1693],
        [-0.1695],
        [-0.1696],
        [-0.1696],
        [-0.1698],
        [-0.1699],
        [-0.1700],
        [-0.1701],
        [-0.1701],
        [-0.1703],
        [-0.1704],
        [-0.1705],
        [-0.1711],
        [-0.1719],
        [-0.1719],
        [-0.1721],
        [-0.1725],
        [-0.1729],
        [-0.1733],
        [-0.1743],
        [-0.1744],
        [-0.1749],
        [-0.1754],
        [-0.1761],
        [-0.1767],
        [-0.1764],
        [-0.1789],
        [-0.1794],
        [-0.1802],
        [-0.1809],
        [-0.1817],
        [-0.1823],
        [-0.1827],
        [-0.1826],
        [-0.1812],
        [-0.1786],
        [-0.1765],
        [-0.1725],
        [-0.1702],
        [-0.1694],
        [-0.1688],
        [-0.1682],
        [-0.1680],
        [-0.1678],
        [-0.1677],
        [-0.1676],
        [-0.1676],
        [-0.1676],
        [-0.1677],
        [-0.1678],
        [-0.1678],
        [-0.1678],
        [-0.1678],
        [-0.1680],
        [-0.1680],
        [-0.1680],
        [-0.1681],
        [-0.1681],
        [-0.1682],
        [-0.1683],
        [-0.1684],
        [-0.1685],
        [-0.1686],
        [-0.1686],
        [-0.1687],
        [-0.1694],
        [-0.1691],
        [-0.1692],
        [-0.1692],
        [-0.1694],
        [-0.1694],
        [-0.1695],
        [-0.1697],
        [-0.1698],
        [-0.1697],
        [-0.1699],
        [-0.1700],
        [-0.1700],
        [-0.1702],
        [-0.1703],
        [-0.1704],
        [-0.1705],
        [-0.1707],
        [-0.1706],
        [-0.1711],
        [-0.1714],
        [-0.1711],
        [-0.1716],
        [-0.1721],
        [-0.1720],
        [-0.1723],
        [-0.1727],
        [-0.1726],
        [-0.1729],
        [-0.1730],
        [-0.1740],
        [-0.1744],
        [-0.1750],
        [-0.1753],
        [-0.1756],
        [-0.1759],
        [-0.1757],
        [-0.1746],
        [-0.1727],
        [-0.1716],
        [-0.1704],
        [-0.1700],
        [-0.1695],
        [-0.1695],
        [-0.1694],
        [-0.1694],
        [-0.1694],
        [-0.1694],
        [-0.1695],
        [-0.1695],
        [-0.1696],
        [-0.1696],
        [-0.1697],
        [-0.1699],
        [-0.1699],
        [-0.1700],
        [-0.1701],
        [-0.1701],
        [-0.1702],
        [-0.1703],
        [-0.1705],
        [-0.1706],
        [-0.1707],
        [-0.1708],
        [-0.1707],
        [-0.1710],
        [-0.1712],
        [-0.1714],
        [-0.1718],
        [-0.1721],
        [-0.1723],
        [-0.1726],
        [-0.1728],
        [-0.1730],
        [-0.1731],
        [-0.1732],
        [-0.1737],
        [-0.1737],
        [-0.1742],
        [-0.1748],
        [-0.1751],
        [-0.1756],
        [-0.1761],
        [-0.1740],
        [-0.1723],
        [-0.1711],
        [-0.1709],
        [-0.1708],
        [-0.1709],
        [-0.1707],
        [-0.1707],
        [-0.1707],
        [-0.1708],
        [-0.1709],
        [-0.1709],
        [-0.1709],
        [-0.1711],
        [-0.1712],
        [-0.1715],
        [-0.1721],
        [-0.1730],
        [-0.1728],
        [-0.1735],
        [-0.1730],
        [-0.1730],
        [-0.1732],
        [-0.1734],
        [-0.1737],
        [-0.1738],
        [-0.1745],
        [-0.1756],
        [-0.1757],
        [-0.1759],
        [-0.1748],
        [-0.1734],
        [-0.1732],
        [-0.1730],
        [-0.1728],
        [-0.1727],
        [-0.1727],
        [-0.1728],
        [-0.1729],
        [-0.1729],
        [-0.1729],
        [-0.1734],
        [-0.1736],
        [-0.1740],
        [-0.1746],
        [-0.1756],
        [-0.1752],
        [-0.1749],
        [-0.1751],
        [-0.1759],
        [-0.1760],
        [-0.1753],
        [-0.1747],
        [-0.1743],
        [-0.1740],
        [-0.1740],
        [-0.1740],
        [-0.1742],
        [-0.1744],
        [-0.1746],
        [-0.1748],
        [-0.1751],
        [-0.1754],
        [-0.1757],
        [-0.1758],
        [-0.1758],
        [-0.1753],
        [-0.1753],
        [-0.1751],
        [-0.1748],
        [-0.1750],
        [-0.1752],
        [-0.1753],
        [-0.1757],
        [-0.1759],
        [-0.1760],
        [-0.1759],
        [-0.1756],
        [-0.1754],
        [-0.1753],
        [-0.1752],
        [-0.1753],
        [-0.1753],
        [-0.1759],
        [-0.1766],
        [-0.1767],
        [-0.1758],
        [-0.1755],
        [-0.1754],
        [-0.1754],
        [-0.1757],
        [-0.1761],
        [-0.1765],
        [-0.1768],
        [-0.1770],
        [-0.1774],
        [-0.1782],
        [-0.1793],
        [-0.1806],
        [-0.1817],
        [-0.1850]], device='cuda:0', grad_fn=<CatBackward>), tensor([[65.7225],
        [-0.1867],
        [-0.1853],
        [-0.1843],
        [-0.1836],
        [-0.1827],
        [-0.1820],
        [-0.1816],
        [-0.1812],
        [-0.1805],
        [-0.1803],
        [-0.1801],
        [-0.1799],
        [-0.1797],
        [-0.1796],
        [-0.1795],
        [-0.1794],
        [-0.1793],
        [-0.1793],
        [-0.1792],
        [-0.1792],
        [-0.1792],
        [-0.1790],
        [-0.1789],
        [-0.1791],
        [-0.1792],
        [-0.1791],
        [-0.1793],
        [-0.1793],
        [-0.1792],
        [-0.1793],
        [-0.1794],
        [-0.1794],
        [-0.1795],
        [-0.1796],
        [-0.1797],
        [-0.1793],
        [-0.1802],
        [-0.1804],
        [-0.1806],
        [-0.1810],
        [-0.1809],
        [-0.1810],
        [-0.1812],
        [-0.1811],
        [-0.1815],
        [-0.1811],
        [-0.1816],
        [-0.1818],
        [-0.1832],
        [-0.1821],
        [-0.1820],
        [-0.1834],
        [-0.1824],
        [-0.1821],
        [-0.1821],
        [-0.1822],
        [-0.1822],
        [-0.1822],
        [-0.1823],
        [-0.1823],
        [-0.1823],
        [-0.1823],
        [-0.1823],
        [-0.1821],
        [-0.1819],
        [-0.1817],
        [-0.1817],
        [-0.1816],
        [-0.1812],
        [-0.1803],
        [-0.1805],
        [-0.1805],
        [-0.1802],
        [-0.1801],
        [-0.1799],
        [-0.1798],
        [-0.1796],
        [-0.1796],
        [-0.1795],
        [-0.1794],
        [-0.1794],
        [-0.1794],
        [-0.1793],
        [-0.1793],
        [-0.1792],
        [-0.1791],
        [-0.1789],
        [-0.1787],
        [-0.1787],
        [-0.1787],
        [-0.1793],
        [-0.1797],
        [-0.1787],
        [-0.1790],
        [-0.1789],
        [-0.1772],
        [-0.1792],
        [-0.1785],
        [-0.1778],
        [-0.1782],
        [-0.1791],
        [-0.1787],
        [-0.1785],
        [-0.1784],
        [-0.1783],
        [-0.1782],
        [-0.1779],
        [-0.1778],
        [-0.1785],
        [-0.1784],
        [-0.1782],
        [-0.1779],
        [-0.1783],
        [-0.1783],
        [-0.1778],
        [-0.1780],
        [-0.1780],
        [-0.1779],
        [-0.1778],
        [-0.1776],
        [-0.1772],
        [-0.1781],
        [-0.1777],
        [-0.1781],
        [-0.1769],
        [-0.1768],
        [-0.1765],
        [-0.1767],
        [-0.1762],
        [-0.1761],
        [-0.1762],
        [-0.1775]], device='cuda:0', grad_fn=<CatBackward>)]
Traceback (most recent call last):
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 689, in <module>
    optimizer=optimizer, epoch=epoch, n_epochs=n_epochs, vis_signal=vis_signal, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 380, in train
    output_train_depth, input_trajectory_train_uv = cumsum_decumulate_trajectory(depth=output_train_depth, uv=input_trajectory_train_gt[..., :-1], trajectory_startpos=input_trajectory_train_startpos, lengths=input_trajectory_train_lengths, eot=input_trajectory_train_gt[..., -1].unsqueeze(dim=-1), projection_matrix=projection_matrix, camera_to_world_matrix=camera_to_world_matrix, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 342, in cumsum_decumulate_trajectory
    depth_cumsum = [split_cumsum(reset_idx=reset_idx[i][0], lengths=lengths, reset_depth=reset_depth[i], depth=depth[i]) for i in range(trajectory_startpos.shape[0])]
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 342, in <listcomp>
    depth_cumsum = [split_cumsum(reset_idx=reset_idx[i][0], lengths=lengths, reset_depth=reset_depth[i], depth=depth[i]) for i in range(trajectory_startpos.shape[0])]
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 313, in split_cumsum
    depth = pt.stack([pt.cat([trajectory_startpos[i][:, -1].view(-1, 1), depth[i]]) for i in range(trajectory_startpos.shape[0])])
NameError: name 'trajectory_startpos' is not defined
