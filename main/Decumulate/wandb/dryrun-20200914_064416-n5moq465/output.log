[%]GPU Enabled
/home/puntawat/Mint/Work/Vision/BallTrajectory/UnityDataset//RealWorld/Unity/Mixed/NormalScaled/No_noise_old/train_set
Mixed:   0%|                                                                                                                 | 0/1 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.51it/s]
===============================Dataset shape===============================
Mixed : (2035,)
===========================================================================
Mixed:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.68it/s]Mixed: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.63it/s]
===============================Dataset shape===============================
Mixed : (2000,)
===========================================================================
======================================================Summary Batch (batch_size = 128)=========================================================================
Input batch [0] : batch=torch.Size([128, 826, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 826, 3]), initial position=torch.Size([128, 1, 4])
Output batch [0] : batch=torch.Size([128, 826, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 827, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [1] : batch=torch.Size([128, 907, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 907, 3]), initial position=torch.Size([128, 1, 4])
Output batch [1] : batch=torch.Size([128, 907, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 908, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [2] : batch=torch.Size([128, 911, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 911, 3]), initial position=torch.Size([128, 1, 4])
Output batch [2] : batch=torch.Size([128, 911, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 912, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [3] : batch=torch.Size([128, 869, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 869, 3]), initial position=torch.Size([128, 1, 4])
Output batch [3] : batch=torch.Size([128, 869, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 870, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [4] : batch=torch.Size([128, 941, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 941, 3]), initial position=torch.Size([128, 1, 4])
Output batch [4] : batch=torch.Size([128, 941, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 942, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [5] : batch=torch.Size([128, 895, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 895, 3]), initial position=torch.Size([128, 1, 4])
Output batch [5] : batch=torch.Size([128, 895, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 896, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [6] : batch=torch.Size([128, 834, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 834, 3]), initial position=torch.Size([128, 1, 4])
Output batch [6] : batch=torch.Size([128, 834, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 835, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [7] : batch=torch.Size([128, 888, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 888, 3]), initial position=torch.Size([128, 1, 4])
Output batch [7] : batch=torch.Size([128, 888, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 889, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [8] : batch=torch.Size([128, 864, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 864, 3]), initial position=torch.Size([128, 1, 4])
Output batch [8] : batch=torch.Size([128, 864, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 865, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [9] : batch=torch.Size([128, 951, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 951, 3]), initial position=torch.Size([128, 1, 4])
Output batch [9] : batch=torch.Size([128, 951, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 952, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [10] : batch=torch.Size([128, 904, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 904, 3]), initial position=torch.Size([128, 1, 4])
Output batch [10] : batch=torch.Size([128, 904, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 905, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [11] : batch=torch.Size([128, 829, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 829, 3]), initial position=torch.Size([128, 1, 4])
Output batch [11] : batch=torch.Size([128, 829, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 830, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [12] : batch=torch.Size([128, 838, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 838, 3]), initial position=torch.Size([128, 1, 4])
Output batch [12] : batch=torch.Size([128, 838, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 839, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [13] : batch=torch.Size([128, 928, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 928, 3]), initial position=torch.Size([128, 1, 4])
Output batch [13] : batch=torch.Size([128, 928, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 929, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
Input batch [14] : batch=torch.Size([128, 960, 3]), lengths=torch.Size([128]), mask=torch.Size([128, 960, 3]), initial position=torch.Size([128, 1, 4])
Output batch [14] : batch=torch.Size([128, 960, 2]), lengths=torch.Size([128]), mask=torch.Size([128, 961, 4]), initial position=torch.Size([128, 1, 4])
Unpacked equality :  tensor(True)
===============================================================================================================================================================
===>No model checkpoint
[#] Define the Learning rate, Optimizer, Decay rate and Scheduler...
[#]Model Architecture
####### Model - EOT #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(2, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
####### Model - Depth #######
BiGRUResidualAdd(
  (recurrent_blocks): ModuleList(
    (0): GRU(3, 32, batch_first=True, bidirectional=True)
    (1): GRU(64, 32, batch_first=True, bidirectional=True)
    (2): GRU(64, 32, batch_first=True, bidirectional=True)
    (3): GRU(64, 32, batch_first=True, bidirectional=True)
  )
  (fc_blocks): Sequential(
    (0): Sequential(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): ReLU()
    )
    (1): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
    )
    (2): Sequential(
      (0): Linear(in_features=16, out_features=8, bias=True)
      (1): ReLU()
    )
    (3): Sequential(
      (0): Linear(in_features=8, out_features=4, bias=True)
      (1): ReLU()
    )
    (4): Sequential(
      (0): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>[Epoch : 1/100000]<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[#]Learning rate (Depth & EOT) :  0.01
===> [Minibatch 1/15].........torch.Size([297])
tensor([ 4.4384e-04,  8.8760e-05,  1.7711e-05,  3.5152e-06,  7.0183e-07,
         1.0604e-07, -2.2859e-08, -5.7131e-08, -8.0973e-08,  2.4732e-09,
        -2.1369e-08, -7.2281e-08, -7.5537e-10,  4.4600e-09, -2.6832e-08,
        -7.3523e-08, -1.8140e-08, -1.7892e-08, -4.9929e-08, -9.0162e-08,
        -1.0441e-08, -2.9813e-08, -6.2098e-08, -4.2323e-09, -1.0193e-08,
        -5.0178e-08, -9.4384e-08, -3.1303e-08, -6.7562e-08,  1.0421e-08,
        -2.2859e-08, -4.5707e-08, -6.2098e-08,  1.6878e-08,  3.9633e-09,
        -3.9002e-08, -5.8373e-08, -9.7116e-08, -3.0061e-08, -6.2595e-08,
         3.9633e-09, -2.2455e-09, -1.8885e-08, -6.2844e-08, -1.2521e-09,
        -3.0061e-08, -7.4268e-08,  2.2249e-09, -2.6584e-08, -4.0492e-08,
        -8.4699e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,
         7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09,
        -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08,
        -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08,
        -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,
         7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09,
        -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08,
        -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08,
        -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,
         7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09,
        -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08,
        -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08,
        -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,
         7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09,
        -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08,
        -4.0492e-08, -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08,
        -6.4334e-08,  7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,
         7.1919e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1919e-09,
        -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1920e-09, -1.6650e-08,
        -4.0492e-08, -6.4334e-08,  7.1920e-09, -1.6650e-08, -4.0492e-08,
        -6.4334e-08,  7.1920e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,
         7.1920e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1920e-09,
        -1.6650e-08, -4.0492e-08, -6.4334e-08,  7.1920e-09, -1.6650e-08,
        -4.0492e-08, -6.4334e-08,  7.1920e-09, -1.6650e-08, -4.0492e-08,
        -6.4334e-08,  7.1920e-09, -1.6650e-08, -4.0492e-08, -6.4334e-08,
         7.1920e-09, -1.6650e-08, -4.0492e-08, -5.9863e-08,  1.6133e-08,
        -3.2389e-09, -3.1799e-08, -5.5641e-08, -7.7248e-08, -5.7224e-09,
        -3.1799e-08, -4.9929e-08, -8.1470e-08, -1.6898e-08, -4.7694e-08,
        -6.8059e-08, -3.4872e-09, -2.7329e-08, -5.1171e-08, -7.9980e-08,
        -3.4872e-09, -3.2296e-08, -6.1105e-08,  1.9858e-08,  9.8314e-10,
        -3.2296e-08, -6.5575e-08, -3.7356e-09, -3.7263e-08, -6.1105e-08,
         2.0106e-08, -3.7356e-09, -1.7892e-08, -4.1734e-08, -7.5261e-08,
        -1.3421e-08, -4.6949e-08, -9.0411e-08, -9.1993e-09, -3.3041e-08,
        -7.6503e-08, -1.4663e-08, -3.8505e-08, -6.2347e-08,  9.1788e-09,
        -1.4663e-08, -3.8505e-08, -6.2347e-08, -9.1993e-09, -3.3041e-08,
        -7.4516e-08, -2.0127e-08, -4.3969e-08, -6.7811e-08,  3.7150e-09,
        -2.0127e-08, -4.3969e-08, -5.2909e-08, -6.2347e-08, -4.9773e-09,
        -4.2479e-08, -6.6320e-08,  5.2051e-09, -3.1551e-08, -5.5393e-08,
        -7.9235e-08, -7.7092e-09, -1.9879e-08, -3.2296e-08, -6.7314e-08,
         4.2117e-09, -1.9630e-08, -5.3655e-08, -7.7496e-08, -5.9708e-09,
        -2.0375e-08, -5.3655e-08, -7.7496e-08, -5.9708e-09, -2.1120e-08,
        -4.4962e-08, -7.7248e-08, -1.3918e-08, -3.7760e-08, -6.1602e-08,
         1.7623e-08, -6.2191e-09, -3.0061e-08, -5.3903e-08, -7.7745e-08,
         4.8638e-10, -2.3355e-08, -4.7197e-08, -6.4830e-08,  6.6952e-09,
        -2.3107e-08, -4.6949e-08, -7.0791e-08,  7.3473e-10, -1.7892e-08,
        -4.6949e-08, -7.0791e-08,  7.3473e-10, -2.3107e-08, -5.1668e-08,
        -7.5510e-08,  4.8637e-10, -2.3355e-08, -5.1419e-08, -7.1039e-08,
         4.7084e-09, -2.3107e-08, -5.0923e-08, -7.8738e-08, -1.0938e-08,
        -3.4780e-08, -5.8622e-08, -8.2463e-08, -1.0938e-08, -3.8257e-08,
        -6.2099e-08,  9.4270e-09], device='cuda:0')
tensor([[[ 5.2989e-05,  1.0584e-05,  2.1423e-06,  4.7409e-07,  1.0547e-07,
          -7.3109e-09, -5.8984e-09,  1.7323e-08, -1.8580e-08, -4.3182e-08,
           2.3128e-09,  3.1013e-08, -7.1246e-09, -2.3625e-08,  1.9682e-08,
           3.4599e-08,  1.2107e-09, -6.1157e-09, -1.9403e-09, -3.3077e-08,
          -3.9302e-08,  1.4994e-08,  5.6159e-08,  3.8728e-08, -1.0353e-08,
          -4.4362e-08, -1.8564e-08,  3.6337e-08,  2.6279e-08, -4.2918e-08,
          -6.6077e-08, -1.1890e-08,  4.8926e-08,  6.0303e-08,  3.3869e-08,
          -4.8739e-09, -4.2220e-08, -4.2003e-08, -9.3132e-10,  1.4886e-08,
          -6.9850e-10,  8.4750e-09,  1.4110e-08, -1.8782e-08, -2.0396e-08,
           2.8343e-08,  3.2224e-08, -2.1545e-08, -2.9554e-08,  1.8751e-08,
           2.2569e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08,
          -2.3842e-08, -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08,
          -2.3842e-08,  2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,
           2.3842e-08,  2.3842e-08, -2.3842e-08, -2.3842e-08,  2.3842e-08,
           2.4121e-08, -2.2165e-08, -1.9651e-08,  2.8855e-08,  2.4882e-08,
          -3.4831e-08, -5.3768e-08, -1.5988e-09,  4.9019e-08,  3.0346e-08,
          -2.5130e-08, -2.8995e-08,  1.4901e-08,  2.4261e-08,  2.4680e-09,
           5.3085e-09,  2.9958e-09, -2.8219e-08, -2.2134e-08,  2.5177e-08,
           2.9585e-08,  1.5522e-10,  5.0291e-09,  5.3085e-09, -3.6027e-08,
          -4.0730e-08,  9.9341e-09,  2.8048e-08,  1.0974e-08,  1.8068e-08,
           1.5118e-08, -3.0315e-08, -5.3132e-08, -1.5134e-08,  2.8902e-08,
           2.1762e-08, -3.1665e-09,  9.5926e-09,  1.7462e-08, -8.3819e-09,
          -1.0291e-08,  7.2643e-09,  7.4971e-09,  1.1579e-08,  4.2530e-09,
          -3.0346e-08, -2.9585e-08,  1.2402e-08,  1.7478e-08, -9.5305e-09,
          -4.8118e-09,  1.1859e-08,  1.0462e-08,  1.5150e-08,  7.3885e-09,
          -2.9135e-08, -3.5685e-08,  7.4816e-09,  3.5297e-08,  1.0772e-08,
          -1.3411e-08,  2.7629e-09,  3.9426e-09, -2.8685e-08, -1.8223e-08,
           3.5468e-08,  3.7315e-08, -1.6453e-08, -3.0206e-08,  9.5771e-09,
           1.4078e-08, -2.7583e-08, -2.3656e-08,  3.0082e-08,  3.1929e-08,
          -2.3019e-08, -2.8561e-08,  2.4199e-08,  3.0563e-08, -2.1917e-08,
          -3.3279e-08,  1.1704e-08,  2.7645e-08,  9.1735e-09,  1.1579e-08,
           5.4948e-09, -3.7656e-08, -5.1595e-08, -3.1045e-10,  5.0974e-08,
           3.2286e-08, -2.3997e-08, -2.4494e-08,  2.1824e-08,  1.8130e-08,
          -2.9197e-08, -2.3594e-08,  2.8110e-08,  2.7008e-08, -2.4773e-08,
          -2.7101e-08,  2.2243e-08,  2.3671e-08, -2.4913e-08, -2.3594e-08,
           2.7567e-08,  2.6838e-08, -2.4556e-08, -2.3935e-08,  2.5968e-08,
           2.2212e-08, -3.1168e-08, -3.0672e-08,  2.2398e-08,  2.7133e-08,
          -1.8952e-08, -2.0474e-08,  2.4168e-08,  2.8079e-08]]],
       device='cuda:0')
Traceback (most recent call last):
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 732, in <module>
    optimizer=optimizer, epoch=epoch, n_epochs=n_epochs, vis_signal=vis_signal, width=width, height=height)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 418, in train
    train_gravity_loss = GravityLoss(output=output_train_xyz, trajectory_gt=output_trajectory_train_xyz[..., :-1], mask=output_trajectory_train_mask[..., :-1], lengths=output_trajectory_train_lengths)
  File "train_ball_trajectory_depth_jointly_decumulate.py", line 167, in GravityLoss
    print(trajectory_gt_yaxis_2nd_finite_difference.shape[i][:, :lengths[i]+1])
TypeError: 'int' object is not subscriptable
